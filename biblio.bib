% Encoding: UTF-8

 
@Book{Minsky1967,
  author     = {Minsky, Marvin L.},
  date       = {1967},
  title      = {Computation: finite and infinite machines},
  isbn       = {9780131655638},
  location   = {{USA}},
  pagetotal  = {334},
  publisher  = {Prentice-Hall, Inc.},
  abstract   = {From the Preface (See Front Matter for full Preface) Man has within a single generation found himself sharing the world with a strange new species: the computers and computer-like machines. Neither history, nor philosophy, nor common sense will tell us how these machines will affect us, for they do not do "work" as did machines of the Industrial Revolution. Instead of dealing with materials or energy, we are told that they handle "control" and "information" and even "intellectual processes." There are very few individuals today who doubt that the computer and its relatives are developing rapidly in capability and complexity, and that these machines are destined to play important (though not as yet fully understood) roles in society's future. Though only some of us deal directly with computers, all of us are falling under the shadow of their ever-growing sphere of influence, and thus we all need to understand their capabilities and their limitations. It would indeed be reassuring to have a book that categorically and systematically described what all these machines can do and what they cannot do, giving sound theoretical or practical grounds for each judgment. However, although some books have purported to do this, it cannot be done for the following reasons: a) Computer-like devices are utterly unlike anything which science has ever considered---we still lack the tools necessary to fully analyze, synthesize, or even think about them; and b) The methods discovered so far are effective in certain areas, but are developing much too rapidly to allow a useful interpretation and interpolation of results. The abstract theory---as described in this book---tells us in no uncertain terms that the machines' potential range is enormous, and that its theoretical limitations are of the subtlest and most elusive sort. There is no reason to suppose machines have any limitations not shared by man.},
  shorttitle = {Computation},
}

 
@Article{Lifschitz2015,
  author       = {Lifschitz, Vladimir},
  date         = {2015},
  journaltitle = {Journal of Philosophical Logic},
  title        = {The Dramatic True Story of the Frame Default},
  issn         = {0022-3611},
  number       = {2},
  pages        = {163--176},
  url          = {https://www.jstor.org/stable/24564010},
  urldate      = {2025-09-01},
  volume       = {44},
  abstract     = {This is an expository article about the solution to the frame problem proposed in 1980 by Raymond Reiter. For years, his "frame default" remained untested and suspect. But developments in some seemingly unrelated areas of computer science—logic programming and satisfiability solvers—eventually exonerated the frame default and turned it into a basis for important applications.},
  file         = {JSTOR Full Text PDF:https\://www.jstor.org/stable/pdfplus/10.2307/24564010.pdf?acceptTC=true:application/pdf},
  publisher    = {Springer},
}

 
@Article{Harnad1990,
  author       = {Harnad, Stevan},
  date         = {1990-06-01},
  journaltitle = {Physica D: Nonlinear Phenomena},
  title        = {The symbol grounding problem},
  doi          = {10.1016/0167-2789(90)90087-6},
  issn         = {0167-2789},
  number       = {1},
  pages        = {335--346},
  url          = {https://www.sciencedirect.com/science/article/pii/0167278990900876},
  urldate      = {2025-08-31},
  volume       = {42},
  abstract     = {There has been much discussion recently about the scope and limits of purely symbolic models of the mind and about the proper role of connectionism in cognitive modeling. This paper describes the “symbol grounding problem”: How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) iconic representations, which are analogs of the proximal sensory projections of distal objects and events, and (2) categorical representations, which are learned and innate feature detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) symbolic representations, grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g. “An X is a Y that is Z”). Connectionism is one natural candidate for the mechanism that learns the invariant features underlying categorical representations, thereby connecting names to the proximal projections of the distal objects they stand for. In this way connectionism can be seen as a complementary component in a hybrid nonsymbolic/symbolic model of the mind, rather than a rival to purely symbolic modeling. Such a hybrid model would not have an autonomous symbolic “module,” however; the symbolic functions would emerge as an intrinsically “dedicated” symbol system as a consequence of the bottom-up grounding of categories' names in their sensory representations. Symbol manipulation would be governed not just by the arbitrary shapes of the symbol tokens, but by the nonarbitrary shapes of the icons and category invariants in which they are grounded.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/0167278990900876/pdfft?download=true:application/pdf},
  shortjournal = {Physica D: Nonlinear Phenomena},
}

 
@Misc{Balestriero2023,
  author     = {Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and {LeCun}, Yann and Goldblum, Micah},
  date       = {2023-06-28},
  title      = {A Cookbook of Self-Supervised Learning},
  doi        = {10.48550/arXiv.2304.12210},
  eprint     = {2304.12210 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2304.12210},
  urldate    = {2025-08-28},
  abstract   = {Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training {SSL} methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a {SSL} method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into {SSL} research by laying the foundations and latest {SSL} recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious {SSL} can be.},
  file       = {Preprint PDF:Balestriero2023 - A Cookbook of Self Supervised Learning.pdf:PDF:http\://arxiv.org/pdf/2304.12210v2},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  number     = {{arXiv}:2304.12210},
  publisher  = {{arXiv}},
}

 
@Misc{Cobbe2021,
  author     = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  date       = {2021-11-18},
  title      = {Training Verifiers to Solve Math Word Problems},
  doi        = {10.48550/arXiv.2110.14168},
  eprint     = {2110.14168 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2110.14168},
  urldate    = {2025-08-27},
  abstract   = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce {GSM}8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on {GSM}8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
  file       = {Preprint PDF:Cobbe2021 - Training Verifiers to Solve Math Word Problems.pdf:PDF:http\://arxiv.org/pdf/2110.14168v2},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  number     = {{arXiv}:2110.14168},
  publisher  = {{arXiv}},
}

 
@TechReport{Radford2019,
  author      = {Radford, Alec and Wu, Jeff and Child, R. and Luan, D. and Amodei, Dario and Sutskever, I.},
  date        = {2019},
  institution = {OpenAI},
  title       = {Language Models are Unsupervised Multitask Learners},
  url         = {https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe},
  urldate     = {2025-08-23},
  abstract    = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called {WebText}. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the {CoQA} dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, {GPT}-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits {WebText}. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  file        = {Full Text PDF:https\://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf:application/pdf},
  priority    = {prio1},
}

 
@InProceedings{Moore2025,
  author     = {Moore, Jared and Grabb, Declan and Agnew, William and Klyman, Kevin and Chancellor, Stevie and Ong, Desmond C. and Haber, Nick},
  booktitle  = {Proceedings of the 2025 {ACM} Conference on Fairness, Accountability, and Transparency},
  date       = {2025-06-23},
  title      = {Expressing stigma and inappropriate responses prevents {LLMs} from safely replacing mental health providers},
  doi        = {10.1145/3715275.3732039},
  eprint     = {2504.18412 [cs]},
  eprinttype = {arxiv},
  pages      = {599--627},
  url        = {http://arxiv.org/abs/2504.18412},
  urldate    = {2025-08-19},
  abstract   = {Should a large language model ({LLM}) be used as a therapist? In this paper, we investigate the use of {LLMs} to *replace* mental health providers, a use case promoted in the tech startup and research space. We conduct a mapping review of therapy guides used by major medical institutions to identify crucial aspects of therapeutic relationships, such as the importance of a therapeutic alliance between therapist and client. We then assess the ability of {LLMs} to reproduce and adhere to these aspects of therapeutic relationships by conducting several experiments investigating the responses of current {LLMs}, such as `gpt-4o`. Contrary to best practices in the medical community, {LLMs} 1) express stigma toward those with mental health conditions and 2) respond inappropriately to certain common (and critical) conditions in naturalistic therapy settings -- e.g., {LLMs} encourage clients' delusional thinking, likely due to their sycophancy. This occurs even with larger and newer {LLMs}, indicating that current safety practices may not address these gaps. Furthermore, we note foundational and practical barriers to the adoption of {LLMs} as therapists, such as that a therapeutic alliance requires human characteristics (e.g., identity and stakes). For these reasons, we conclude that {LLMs} should not replace therapists, and we discuss alternative roles for {LLMs} in clinical therapy.},
  file       = {Preprint PDF:Moore2025 - Expressing Stigma and Inappropriate Responses Prevents LLMs from Safely Replacing Mental Health Providers.pdf:PDF:http\://arxiv.org/pdf/2504.18412v1},
  keywords   = {Computer Science - Computation and Language},
  priority   = {prio1},
}

 
@Article{Amari1967,
  author       = {Amari, Shunichi},
  date         = {1967-06},
  journaltitle = {{IEEE} Transactions on Electronic Computers},
  title        = {A Theory of Adaptive Pattern Classifiers},
  doi          = {10.1109/PGEC.1967.264666},
  issn         = {0367-7508},
  number       = {3},
  pages        = {299--307},
  url          = {https://ieeexplore.ieee.org/document/4039068},
  urldate      = {2025-07-28},
  volume       = {{EC}-16},
  abstract     = {This paper describes error-correction adjustment procedures for determining the weight vector of linear pattern classifiers under general pattern distribution. It is mainly aimed at clarifying theoretically the performance of adaptive pattern classifiers. In the case where the loss depends on the distance between a pattern vector and a decision boundary and where the average risk function is unimodal, it is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions. The speed and the accuracy of convergence are analyzed, and it is shown that there is an important tradeoff between speed and accuracy of convergence. Dynamical behaviors, when the probability distributions of patterns are changing, are also shown. The theory is generalized and made applicable to the case with general discriminant functions, including piecewise-linear discriminant functions.},
  keywords     = {Probability distribution, Convergence, Vectors, Piecewise linear techniques, Computer errors, Adaptive systems, Logic, Accuracy of learning, adaptive pattern classifier, convergence of learning, learning under nonseparable pattern distribution, linear decision function, piecewise-linear decision function, rapidity of learning},
}

@article{LeCun85,
  author = {Le Cun, Y.},
  journal = {Proceedings of Cognitiva 85, Paris},
  pages = {599--604},
  title = {Une procédure d'apprentissage pour réseau à seuil asymétrique},
  year = 1985
}

 
@Article{Linnainmaa1976,
  author       = {Linnainmaa, Seppo},
  date         = {1976-06-01},
  journaltitle = {{BIT} Numerical Mathematics},
  title        = {Taylor expansion of the accumulated rounding error},
  doi          = {10.1007/BF01931367},
  issn         = {1572-9125},
  number       = {2},
  pages        = {146--160},
  url          = {https://doi.org/10.1007/BF01931367},
  urldate      = {2025-07-28},
  volume       = {16},
  abstract     = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
  keywords     = {Approximations and Expansions, Computational Complexity, Computational Mathematics and Numerical Analysis, Computational Number Theory, Mathematics and Computing, Numerical Analysis, Computational Mathematic, Rounding Error, Taylor Expansion, Local Error, Storage Requirement},
  langid       = {english},
  shortjournal = {{BIT}},
}

 
@Misc{Becker2025,
  author     = {Becker, Joel and Rush, Nate and Barnes, Elizabeth and Rein, David},
  date       = {2025-07-12},
  title      = {Measuring the Impact of Early-2025 {AI} on Experienced Open-Source Developer Productivity},
  doi        = {10.48550/arXiv.2507.09089},
  eprint     = {2507.09089 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2507.09089},
  urldate    = {2025-07-20},
  abstract   = {Despite widespread adoption, the impact of {AI} tools on software development in the wild remains understudied. We conduct a randomized controlled trial ({RCT}) to understand how {AI} tools at the February-June 2025 frontier affect the productivity of experienced open-source developers. 16 developers with moderate {AI} experience complete 246 tasks in mature projects on which they have an average of 5 years of prior experience. Each task is randomly assigned to allow or disallow usage of early 2025 {AI} tools. When {AI} tools are allowed, developers primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet. Before starting tasks, developers forecast that allowing {AI} will reduce completion time by 24\%. After completing the study, developers estimate that allowing {AI} reduced completion time by 20\%. Surprisingly, we find that allowing {AI} actually increases completion time by 19\%--{AI} tooling slowed developers down. This slowdown also contradicts predictions from experts in economics (39\% shorter) and {ML} (38\% shorter). To understand this result, we collect and evaluate evidence for 20 properties of our setting that a priori could contribute to the observed slowdown effect--for example, the size and quality standards of projects, or prior developer experience with {AI} tooling. Although the influence of experimental artifacts cannot be entirely ruled out, the robustness of the slowdown effect across our analyses suggests it is unlikely to primarily be a function of our experimental design.},
  file       = {Preprint PDF:Becker2025 - Measuring the Impact of Early 2025 AI on Experienced Open Source Developer Productivity.pdf:PDF:http\://arxiv.org/pdf/2507.09089v1},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Software Engineering},
  number     = {{arXiv}:2507.09089},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

 
@Misc{Vafa2025,
  author     = {Vafa, Keyon and Chang, Peter G. and Rambachan, Ashesh and Mullainathan, Sendhil},
  date       = {2025-07-10},
  title      = {What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models},
  doi        = {10.48550/arXiv.2507.06952},
  eprint     = {2507.06952 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2507.06952},
  urldate    = {2025-07-18},
  abstract   = {Foundation models are premised on the idea that sequence prediction can uncover deeper domain understanding, much like how Kepler's predictions of planetary motion later led to the discovery of Newtonian mechanics. However, evaluating whether these models truly capture deeper structure remains a challenge. We develop a technique for evaluating foundation models that examines how they adapt to synthetic datasets generated from some postulated world model. Our technique measures whether the foundation model's inductive bias aligns with the world model, and so we refer to it as an inductive bias probe. Across multiple domains, we find that foundation models can excel at their training tasks yet fail to develop inductive biases towards the underlying world model when adapted to new tasks. We particularly find that foundation models trained on orbital trajectories consistently fail to apply Newtonian mechanics when adapted to new physics tasks. Further analysis reveals that these models behave as if they develop task-specific heuristics that fail to generalize.},
  file       = {Preprint PDF:Vafa2025 - What Has a Foundation Model Found_ Using Inductive Bias to Probe for World Models.pdf:PDF:http\://arxiv.org/pdf/2507.06952v2},
  keywords   = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  number     = {{arXiv}:2507.06952},
  publisher  = {{arXiv}},
}

 
@Misc{Xu2025a,
  author     = {Xu, Frank F. and Song, Yufan and Li, Boxuan and Tang, Yuxuan and Jain, Kritanjali and Bao, Mengxue and Wang, Zora Z. and Zhou, Xuhui and Guo, Zhitong and Cao, Murong and Yang, Mingyang and Lu, Hao Yang and Martin, Amaad and Su, Zhe and Maben, Leander and Mehta, Raj and Chi, Wayne and Jang, Lawrence and Xie, Yiqing and Zhou, Shuyan and Neubig, Graham},
  date       = {2025-05-19},
  title      = {{TheAgentCompany}: Benchmarking {LLM} Agents on Consequential Real World Tasks},
  doi        = {10.48550/arXiv.2412.14161},
  eprint     = {2412.14161 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2412.14161},
  urldate    = {2025-07-06},
  abstract   = {We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models ({LLMs}), there has also been a rapid development in {AI} agents that interact with and affect change in their surrounding environments. But how performant are {AI} agents at accelerating or even autonomously performing work-related tasks? The answer to this question has important implications both for industry looking to adopt {AI} into their workflows and for economic policy to understand the effects that adoption of {AI} may have on the labor market. To measure the progress of these {LLM} agents' performance on performing real-world professional tasks, in this paper we introduce {TheAgentCompany}, an extensible benchmark for evaluating {AI} agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed {API}-based and open-weights language models ({LMs}), and find that the most competitive agent can complete 30\% of tasks autonomously. This paints a nuanced picture on task automation with {LM} agents--in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems. We release code, data, environment, and experiments on https://the-agent-company.com.},
  file       = {Preprint PDF:Xu2025a - TheAgentCompany_ Benchmarking LLM Agents on Consequential Real World Tasks.pdf:PDF:http\://arxiv.org/pdf/2412.14161v2},
  keywords   = {Computer Science - Computation and Language},
  number     = {{arXiv}:2412.14161},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

 
@Article{Sondeck2025,
  author       = {Sondeck, Louis Philippe and Laurent, Maryline},
  date         = {2025-07-02},
  journaltitle = {Scientific Reports},
  title        = {Practical and ready-to-use methodology to assess the re-identification risk in anonymized datasets},
  doi          = {10.1038/s41598-025-04907-3},
  issn         = {2045-2322},
  number       = {1},
  pages        = {23223},
  url          = {https://www.nature.com/articles/s41598-025-04907-3},
  urldate      = {2025-07-03},
  volume       = {15},
  abstract     = {To prove that a dataset is sufficiently anonymized, many privacy policies suggest that a re-identification risk assessment be performed, but do not provide a precise methodology for doing so, leaving the industry alone with the problem. This paper proposes a practical and ready-to-use methodology for re-identification risk assessment, the originality of which is manifold: (1) it is the first to follow well-known risk analysis methods (e.g. {EBIOS}) that have been used in the cybersecurity field for years, which consider not only the ability to perform an attack, but also the severity such an attack can have on an individual; (2) it is the first to qualify attributes and values of attributes with e.g. degree of exposure, as known real-world attacks mainly target certain types of attributes and not others; (3) it is the first to provide clear, comprehensible criteria and interpretable, explainable assessment results. In addition, the fine granularity of the methodology makes it possible to score the risk as accurately as possible, and thus maintain good data quality at an acceptable risk, which is very promising for the {AI} industrial sector. Finally, the implementation of the methodology is illustrated using the publicly available Adult dataset, which was assessed as having a critical risk of re-identification, with 14 concrete cases of individualization.},
  file         = {Full Text PDF:Sondeck2025 - Practical and Ready to Use Methodology to Assess the Re Identification Risk in Anonymized Datasets.pdf:PDF:https\://www.nature.com/articles/s41598-025-04907-3.pdf},
  howpublished = {{OriginalPaper}},
  keywords     = {Mathematics and computing, Medical research},
  langid       = {english},
  priority     = {prio1},
  publisher    = {Nature Publishing Group},
  rights       = {2025 The Author(s)},
  shortjournal = {Sci Rep},
  type         = {{OriginalPaper}},
}

@Misc{illusion25,
  author   = {Parshin Shojaee and Iman Mirzadeh and Keivan Alizadeh and Maxwell Horton and Samy Bengio and Mehrdad Farajtabar},
  title    = {The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity},
  url      = {https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf},
  priority = {prio1},
  year     = {2025},
}

 
@Article{Liu2018,
  author       = {Liu, Yun and Kohlberger, Timo and Norouzi, Mohammad and Dahl, George E. and Smith, Jenny L. and Mohtashamian, Arash and Olson, Niels and Peng, Lily H. and Hipp, Jason D. and Stumpe, Martin C.},
  date         = {2018-10-08},
  journaltitle = {Archives of Pathology \& Laboratory Medicine},
  title        = {Artificial Intelligence–Based Breast Cancer Nodal Metastasis Detection: Insights Into the Black Box for Pathologists},
  doi          = {10.5858/arpa.2018-0147-OA},
  issn         = {0003-9985},
  number       = {7},
  pages        = {859--868},
  url          = {https://doi.org/10.5858/arpa.2018-0147-OA},
  urldate      = {2025-06-07},
  volume       = {143},
  abstract     = {Nodal metastasis of a primary tumor influences therapy decisions for a variety of cancers. Histologic identification of tumor cells in lymph nodes can be laborious and error-prone, especially for small tumor foci.To evaluate the application and clinical implementation of a state-of-the-art deep learning–based artificial intelligence algorithm ({LYmph} Node Assistant or {LYNA}) for detection of metastatic breast cancer in sentinel lymph node biopsies.Whole slide images were obtained from hematoxylin-eosin–stained lymph nodes from 399 patients (publicly available Camelyon16 challenge dataset). {LYNA} was developed by using 270 slides and evaluated on the remaining 129 slides. We compared the findings to those obtained from an independent laboratory (108 slides from 20 patients/86 blocks) using a different scanner to measure reproducibility.{LYNA} achieved a slide-level area under the receiver operating characteristic ({AUC}) of 99\% and a tumor-level sensitivity of 91\% at 1 false positive per patient on the Camelyon16 evaluation dataset. We also identified 2 “normal” slides that contained micrometastases. When applied to our second dataset, {LYNA} achieved an {AUC} of 99.6\%. {LYNA} was not affected by common histology artifacts such as overfixation, poor staining, and air bubbles.Artificial intelligence algorithms can exhaustively evaluate every tissue patch on a slide, achieving higher tumor-level sensitivity than, and comparable slide-level performance to, pathologists. These techniques may improve the pathologist's productivity and reduce the number of false negatives associated with morphologic detection of tumor cells. We provide a framework to aid practicing pathologists in assessing such algorithms for adoption into their workflow (akin to how a pathologist assesses immunohistochemistry results).},
  file         = {Full Text PDF:https\://meridian.allenpress.com/aplm/article-pdf/143/7/859/2501371/arpa_2018-0147-oa.pdf:application/pdf},
  priority     = {prio1},
  shortjournal = {Archives of Pathology \& Laboratory Medicine},
  shorttitle   = {Artificial Intelligence–Based Breast Cancer Nodal Metastasis Detection},
}

 
@Misc{Jha2025,
  author     = {Jha, Rishi and Zhang, Collin and Shmatikov, Vitaly and Morris, John X.},
  date       = {2025-05-20},
  title      = {Harnessing the Universal Geometry of Embeddings},
  doi        = {10.48550/arXiv.2505.12540},
  eprint     = {2505.12540 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2505.12540},
  urldate    = {2025-05-23},
  abstract   = {We introduce the first method for translating text embeddings from one vector space to another without any paired data, encoders, or predefined sets of matches. Our unsupervised approach translates any embedding to and from a universal latent representation (i.e., a universal semantic structure conjectured by the Platonic Representation Hypothesis). Our translations achieve high cosine similarity across model pairs with different architectures, parameter counts, and training datasets. The ability to translate unknown embeddings into a different space while preserving their geometry has serious implications for the security of vector databases. An adversary with access only to embedding vectors can extract sensitive information about the underlying documents, sufficient for classification and attribute inference.},
  file       = {Preprint PDF:Jha2025 - Harnessing the Universal Geometry of Embeddings.pdf:PDF:http\://arxiv.org/pdf/2505.12540v2},
  keywords   = {Computer Science - Machine Learning},
  number     = {{arXiv}:2505.12540},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

 
@Article{Olatunji2024,
  author       = {Olatunji, Iyiola E. and Rauch, Jens and Katzensteiner, Matthias and Khosla, Megha},
  date         = {2024-12},
  journaltitle = {Big Data},
  title        = {A Review of Anonymization for Healthcare Data},
  doi          = {10.1089/big.2021.0169},
  issn         = {2167-6461},
  number       = {6},
  pages        = {538--555},
  url          = {https://www.liebertpub.com/doi/10.1089/big.2021.0169},
  urldate      = {2025-05-21},
  volume       = {12},
  abstract     = {Mining health data can lead to faster medical decisions, improvement in the quality of treatment, disease prevention, and reduced cost, and it drives innovative solutions within the healthcare sector. However, health data are highly sensitive and subject to regulations such as the General Data Protection Regulation, which aims to ensure patient's privacy. Anonymization or removal of patient identifiable information, although the most conventional way, is the first important step to adhere to the regulations and incorporate privacy concerns. In this article, we review the existing anonymization techniques and their applicability to various types (relational and graph based) of health data. Besides, we provide an overview of possible attacks on anonymized data. We illustrate via a reconstruction attack that anonymization, although necessary, is not sufficient to address patient privacy and discuss methods for protecting against such attacks. Finally, we discuss tools that can be used to achieve anonymization.},
  file         = {Full Text PDF:https\://www.liebertpub.com/doi/pdf/10.1089/big.2021.0169?url_ver=Z39.88-2003&rfr_id=ori\:rid\:crossref.org&rfr_dat=cr_pub%20%200pubmed:application/pdf},
  priority     = {prio1},
  publisher    = {Mary Ann Liebert, Inc., publishers},
}

@Misc{Coulter2023,
  author     =  {Coulter, Martin and Bensinger, Greg},
  date       = {2023-02-09},
  title      = {Alphabet shares dive after Google AI chatbot Bard flubs answer in ad},
  url        = {https://www.reuters.com/technology/google-ai-chatbot-bard-offers-inaccurate-information-company-ad-2023-02-08/},
  urldate    = {2025-03-14}
  }

@Misc{Song2025,
  author     = {Julien Song},
  date       = {2025},
  title      = {ChatGPT affronte Le Chat aux échecs},
  url        = {https://www.youtube.com/watch?v=MtucJ2MnUh4},
  urldate    = {2025-03-14}
  }



@Misc{Dumais1988,
  author     = {  Scott C. Deerwester and
    Susan T. Dumais and
    George W. Furnas and
    Richard A. Harshman and
    Thomas K. Landauer and
    Karen E. Lochbaum and
    Lynn A. Streeter},
  date       = {1988},
  title      = {Computer information retrieval using latent semantic structure },
  url        = {https://patentimages.storage.googleapis.com/7e/32/86/61ff89667a8ad1/US4839853.pdf}
}


@article{Paccanaro2001,
author = {Paccanaro, Alberto and Hinton, Geoffrey E.},
year = {2001},
month = {04},
pages = {232 - 244},
title = {Learning distributed representations of concepts using Linear Relational Embedding},
volume = {13},
journal = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/69.917563}
}

@InProceedings{Hinton1986,
  author    = {Hinton, Geoffrey E.},
  booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  date      = {1986},
  title     = {Learning Distributed Representations of Concepts},
  url       = {https://escholarship.org/content/qt79w838g1/qt79w838g1.pdf},
  volume    = {8},
}


@article{Bengio2003,
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
title = {A neural probabilistic language model},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = {03},
pages = {1137–1155},
numpages = {19}
}

@InProceedings{Bengio2000,
  author    = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal},
  booktitle = {Advances in Neural Information Processing Systems},
  date      = {2000},
  title     = {A Neural Probabilistic Language Model},
  editor    = {T. Leen and T. Dietterich and V. Tresp},
  publisher = {MIT Press},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf},
  volume    = {13},
}

 
@Misc{Kanithi2024,
  author     = {Kanithi, Praveen K. and Christophe, Clément and Pimentel, Marco {AF} and Raha, Tathagata and Saadi, Nada and Javed, Hamza and Maslenkova, Svetlana and Hayat, Nasir and Rajan, Ronnie and Khan, Shadab},
  date       = {2024-09-11},
  title      = {{MEDIC}: Towards a Comprehensive Framework for Evaluating {LLMs} in Clinical Applications},
  doi        = {10.48550/arXiv.2409.07314},
  eprint     = {2409.07314 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2409.07314},
  urldate    = {2025-03-14},
  abstract   = {The rapid development of Large Language Models ({LLMs}) for healthcare applications has spurred calls for holistic evaluation beyond frequently-cited benchmarks like {USMLE}, to better reflect real-world performance. While real-world assessments are valuable indicators of utility, they often lag behind the pace of {LLM} evolution, likely rendering findings obsolete upon deployment. This temporal disconnect necessitates a comprehensive upfront evaluation that can guide model selection for specific clinical applications. We introduce {MEDIC}, a framework assessing {LLMs} across five critical dimensions of clinical competence: medical reasoning, ethics and bias, data and language understanding, in-context learning, and clinical safety. {MEDIC} features a novel cross-examination framework quantifying {LLM} performance across areas like coverage and hallucination detection, without requiring reference outputs. We apply {MEDIC} to evaluate {LLMs} on medical question-answering, safety, summarization, note generation, and other tasks. Our results show performance disparities across model sizes, baseline vs medically finetuned models, and have implications on model selection for applications requiring specific model strengths, such as low hallucination or lower cost of inference. {MEDIC}'s multifaceted evaluation reveals these performance trade-offs, bridging the gap between theoretical capabilities and practical implementation in healthcare settings, ensuring that the most promising models are identified and adapted for diverse healthcare applications.},
  file       = {Preprint PDF:Kanithi2024 - MEDIC_ Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications.pdf:PDF:http\://arxiv.org/pdf/2409.07314v1},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  number     = {{arXiv}:2409.07314},
  publisher  = {{arXiv}},
}

@InProceedings{Alex012,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  editor    = {Pereira, F. and Burges, C. J. and Bottou, L. and Weinberger, K. Q.},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012},
}

@online{valmeekam2024,
      title={LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench}, 
      author={Karthik Valmeekam and Kaya Stechly and Subbarao Kambhampati},
      year={2024},
      eprint={2409.13373},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.13373}, 
}

@online{mirzadeh2024,
      title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models}, 
      author={Iman Mirzadeh and Keivan Alizadeh and Hooman Shahrokhi and Oncel Tuzel and Samy Bengio and Mehrdad Farajtabar},
      year={2024},
      eprint={2410.05229},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.05229}, 
}

@online{valmeekam2023,
      title={PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change}, 
      author={Karthik Valmeekam and Matthew Marquez and Alberto Olmo and Sarath Sreedharan and Subbarao Kambhampati},
      year={2023},
      eprint={2206.10498},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.10498}, 
}

@article{Kamb2024,
author = {Kambhampati, Subbarao},
title = {Can large language models reason and plan?},
journal = {Annals of the New York Academy of Sciences},
volume = {1534},
number = {1},
pages = {15-18},
doi = {https://doi.org/10.1111/nyas.15125},
url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/nyas.15125},
eprint = {https://nyaspubs.onlinelibrary.wiley.com/doi/pdf/10.1111/nyas.15125},
abstract = {Abstract While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.},
year = {2024}
}


@online{Kohs20,
  author = {Greg Kohs},
  title = {{AlphaGo - The Movie}},
  url={https://www.youtube.com/watch?v=WXuK6gekU1Y},
  year = {2020}, 
  urldate = {2024-07-19}
}


@Article{Silver2016,
  author   = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  title    = {Mastering the game of Go with deep neural networks and tree search},
  doi      = {10.1038/nature16961},
  issn     = {1476-4687},
  number   = {7587},
  pages    = {484-489},
  url      = {https://doi.org/10.1038/nature16961},
  volume   = {529},
  journal  = {Nature},
  month    = {01},
  priority = {prio1},
  year     = {2016},
}


@InProceedings{Coulom07,
  author    = {Coulom, Rémi},
  booktitle = {Computers and Games},
  title     = {Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search},
  editor    = {van den Herik, Jaap and Ciancarini, Paolo and Donkers, Jeroen},
  isbn      = {978-3-540-75538-8},
  pages     = {72--83},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  year      = {2007},
}

@Book{Husserl0,
  author =      "Edmund Husserl",
  title =       "Logische Untersuchungen",
  publisher =   "Niemeyer",
  year =        "1913",
  OPTeditor =   "",
  OPTvolume =   "",
  OPTseries =   "",
  OPTaddress =  "",
  OPTedition =  "",
  OPTmonth =    "",
  note =        "Traduction française de 1959 (Presses Universitaires de France): Recherches logiques"
}

@InProceedings{1575717,
  author    = {Steinkraus, D. and Buck, I. and Simard, P. Y.},
  booktitle = {Eighth International Conference on Document Analysis and Recognition (ICDAR'05)},
  title     = {Using GPUs for machine learning algorithms},
  doi       = {10.1109/ICDAR.2005.251},
  pages     = {1115-1120},
  volume    = {2},
  keywords  = {Machine learning algorithms;Hardware;Computer graphics;Bandwidth;Machine learning;Testing;Central Processing Unit;Rendering (computer graphics);Neural networks;Optical character recognition software},
  year      = {2005},
}

@TechReport{Alliot932,
  author = 	 {Jean-Marc Alliot},
  title = 	 {Pourquoi les systèmes experts n'ont pas d'avenir},
  institution =  {Centre d'Etudes de la Navigation Aérienne},
  year = 	 {1993},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  note = 	 {N93-738},
  url =          {https://amisducena.org/output/N93-738.pdf},
  OPTannote = 	 {}
}

@TechReport{Leroux,
  author = 	 {Marcel Leroux and Jean-Marc Alliot},
  title = 	 {ERATO, un système expert d'aide au contrôle du trafic aérien},
  institution =  {Centre d'Etudes de la Navigation Aérienne},
  year = 	 {1991},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  note = 	 {N91-012},
  url =          {https://amisducena.org/output/N91-012.pdf},
  OPTannote = 	 {}
}

@article{Good14,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Y.},
year = {2014},
month = {06},
pages = {},
title = {Generative Adversarial Networks},
volume = {3},
journal = {Advances in Neural Information Processing Systems},
doi = {10.1145/3422622}
}

@InCollection{Alliot16,
  author    = {Alliot, Jean-Marc and Demolombe, Robert and Diéguez, Martín and del Cerro, Luis Fariñas and Favre, Gilles and Faye, Jean-Charles and Obeid, Naji and Sordet, Olivier},
  booktitle = {Towards Paraconsistent Engineering},
  title     = {Temporal Logic Modeling of Biological Systems},
  doi       = {10.1007/978-3-319-40418-9\_11},
  editor    = {Seiki Akama},
  pages     = {205--226},
  publisher = {Springer},
  series    = {Intelligent Systems Reference Library},
  url       = {https://doi.org/10.1007/978-3-319-40418-9\_11},
  volume    = {110},
  year      = {2016},
}

@InBook{Clark0,
  author = 	 {Arthur C. Clarke},
  ALTeditor = 	 {},
  title = 	 {Profiles of the future},
  chapter = 	 {Hazards of Prophecy: The Failure of Imagination},
  publisher = 	 {MacMillan},
  year = 	 {1973},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTtype = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTpages = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Book{Woolf0,
  author = 	 {Virginia Woolf},
  ALTeditor = 	 {},
  title = 	 {Orlando: A Biography},
  publisher = 	 {Hogarth Press},
  year = 	 {1928},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Book{Sokal0,
  author = 	 {Alan Sokal and Jean Bricmont},
  ALTeditor = 	 {},
  title = 	 {Impostures intellectuelles},
  publisher = 	 {Odile Jacob},
  year = 	 {1997},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Book{Bouveresse0,
  author = 	 {Jacques Bouveresse},
  ALTeditor = 	 {Raisons d'agir},
  title = 	 {Prodiges et vertiges de l'analogie},
  publisher = 	 {Raisons d'agir},
  year = 	 {1999},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Article{Gentzen0,
  author = 	 {Gerhardt Gentzen},
  title = 	 {Die Widerspruchsfreiheit der reinen Zahlentheorie},
  journal = 	 {Mathematische Annalen},
  year = 	 {1935},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@online{Altman0,
  author = {MacKenzie Sigalos},
  title = {OpenAI's Sam Altman says human-level AI is coming},
  url =  {https://www.cnbc.com/2024/01/16/openais-sam-altman-agi-coming-but-is-less-impactful-than-we-think.html},
  year = {2024}, 
  urldate = {2024-07-19}
}

@online{Hassabis0,
  author = {Demis Hassabis},
  title = {{Strachey Lecture: Artificial Intelligence and the Future}},
  url = {https://podcasts.ox.ac.uk/strachey-lecture-artificial-intelligence-and-future},
  year = {2016}, 
  urldate = {2024-07-19}
}


@Book{Minsky0,
  author    = {Marvin Minsky and Seymour Papert},
  date      = {1969},
  title     = {Perceptrons},
  publisher = {MIT Press},
  year      = {1969},
}

@Article{Finlayson2019-vz,
  author  = {Finlayson, Samuel G. and Bowers, John D. and Ito, Joichi and Zittrain, Jonathan L. and Beam, Andrew L. and Kohane, Isaac S.},
  title   = {Adversarial attacks on medical machine learning},
  number  = {6433},
  pages   = {1287--1289},
  volume  = {363},
  journal = {Science},
  month   = {03},
  year    = {2019},
}

@Online{Owkin,
  author  = {Olivier Moindrota and Sebastian Schwarz and Antonia Trower and Anna Huyghues-Despointes},
  title   = {{Developing virtual staining at Owkin to enrich digital pathology}},
  url     = {https://www.owkin.com/blogs-case-studies/developing-virtual-staining-at-owkin-to-enrich-digital-pathology},
  urldate = {2024-07-19},
  year    = {2020},
}

@Article{Kim2023,
  author   = {Kim, Cherry and Yang, Zepa and Park, Seong Ho and Hwang, Sung Ho and Oh, Yu-Whan and Kang, Eun-Young and Yong, Hwan Seok},
  title    = {Multicentre external validation of a commercial artificial intelligence software to analyse chest radiographs in health screening environments with low disease prevalence},
  number   = {5},
  pages    = {3501--3509},
  volume   = {33},
  address  = {Germany},
  journal  = {Eur Radiol},
  keywords = {Artificial intelligence; Multicentre study; Software; Thoracic radiography; Validation study},
  month    = {01},
  year     = {2023},
}

@Article{Finlayson0,
  author     = {Finlayson, Samuel G. and Kohane, Isaac S. and Beam, Andrew L.},
  title      = {Adversarial Attacks Against Medical Deep Learning Systems},
  eprint     = {1804.05296},
  eprinttype = {arXiv},
  url        = {http://arxiv.org/abs/1804.05296},
  volume     = {abs/1804.05296},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1804-05296.bib},
  journal    = {CoRR},
  timestamp  = {Mon, 13 Aug 2018 16:48:58 +0200},
  year       = {2018},
}

@Article{Merkle0,
  author = 	 {Ralph Merkle and Martin Hellman},
  title = 	 {Hiding information and signatures in trapdoor knapsacks},
  journal = 	 {IEEE Transactions on Information Theory},
  year = 	 {1978},
  OPTkey = 	 {},
  volume = 	 {24},
  number = 	 {5},
  pages = 	 {525--530},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Article{Shamir0,
  author = 	 {Adi Shamir},
  title = 	 {A polynomial-time algorithm for breaking the basic {Merkle-Hellman} cryptosystem},
  journal = 	 {IEEE Transactions on Information Theory},
  year = 	 {1984},
  OPTkey = 	 {},
  volume =	 {30},
  number =	 {5},
  pages =	 {699--704},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@online{Zhao23,
      title={TabuLa: Harnessing Language Models for Tabular Data Synthesis}, 
      author={Zilong Zhao and Robert Birke and Lydia Chen},
      year={2023},
      eprint={2310.12746},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.12746}, 
}

@online{carlini2023,
      title={Extracting Training Data from Diffusion Models}, 
      author={Nicholas Carlini and Jamie Hayes and Milad Nasr and Matthew Jagielski and Vikash Sehwag and Florian Tramèr and Borja Balle and Daphne Ippolito and Eric Wallace},
      year={2023},
      eprint={2301.13188},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2301.13188}, 
}


@InProceedings{Yates2024,
  author = 	 {Adam Yates and Misti Paudel and Fengming Hu},
  title = 	 {Generation of Synthetic Data for Clinical Trials in Base
{SAS} using a 2-Phase Discrete-Time {Markov} and {Poisson}
Rare Event Framework},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle =	 {PharmaSug proceedings},
  year =	 {2024},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTaddress = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@online{xu2018,
      title={Synthesizing Tabular Data using Generative Adversarial Networks}, 
      author={Lei Xu and Kalyan Veeramachaneni},
      year={2018},
      eprint={1811.11264},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1811.11264}, 
}

@article{Racha2024,
   title={Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming Generative Adversarial Networks},
   volume={79},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.1.15317},
   DOI={10.1613/jair.1.15317},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Ramachandranpillai, Resmi and Sikder, Md Fahim and Bergström, David and Heintz, Fredrik},
   year={2024},
   month=04,
   pages={1313–1341} }

@inproceedings{Adiga2018,
author = {Adiga, Sudarshan and Attia, Mohamed and Chang, Wei-Ting and Tandon, Ravi},
booktitle = {2018 IEEE Global Conference on Signal and Information Processing},
year = {2018},
month = {11},
pages = {1184-1188},
title = {On the tradeoff between mode collapse and sample quality in Generative Adversarial Networks},
doi = {10.1109/GlobalSIP.2018.8646478}
}

@article{FETTAH2024,
title = {Convolutional Autoencoder-Based medical image compression using a novel annotated medical X-ray imaging dataset},
journal = {Biomedical Signal Processing and Control},
volume = {94},
pages = {106238},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106238},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424002969},
author = {Amina Fettah and Rafik Menassel and Abdeljalil Gattal and Abdelhak Gattal},
keywords = {Medical x-ray imaging dataset (MXID), Medical image compression, Gender classification, Body parts classification, Deep convolutional autoencoder (DCAE)},
}


@Book{li2019,
  author    = {Ming Li and Paul Vitányi},
  title     = {An Introduction to Kolmogorov Complexity and Its Applications},
  isbn      = {978-3-030-11297-4},
  publisher = {Springer},
  year      = {2019},
}

@INPROCEEDINGS{7298640,
  author={Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  booktitle={2015 {IEEE} Conference on Computer Vision and Pattern Recognition {(CVPR)}}, 
  title={Deep neural networks are easily fooled: High confidence predictions for unrecognizable images}, 
  year={2015},
  volume={},
  number={},
  pages={427-436},
  keywords={Biomedical imaging},
  doi={10.1109/CVPR.2015.7298640}}

@ARTICLE{Chadebec23,
  author={Chadebec, Clément and Thibeau-Sutre, Elina and Burgos, Ninon and Allassonnière, Stéphanie},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Data Augmentation in High Dimensional Low Sample Size Setting Using a Geometry-Based Variational Autoencoder}, 
  year={2023},
  volume={45},
  number={3},
  pages={2879-2896},
  keywords={Data models;Measurement;Training;Magnetic resonance imaging;Databases;Three-dimensional displays;Task analysis;Variational autoencoders;data augmentation;latent space modeling},
  doi={10.1109/TPAMI.2022.3185773}
  }

@Article{Sandfort2019,
  author  = {Sandfort, Veitand Yan, Ke and Pickhardt, Perry J.and Summers, Ronald M.},
  title   = {Data augmentation using generative adversarial networks (CycleGAN) to improve generalizability in CT segmentation tasks},
  doi     = {10.1038/s41598-019-52737-x},
  issn    = {2045-2322},
  number  = {1},
  pages   = {16884},
  url     = {https://doi.org/10.1038/s41598-019-52737-x},
  volume  = {9},
  day     = {15},
  journal = {Scientific Reports},
  month   = {11},
  year    = {2019},
}

@Article{Saldanha2022,
  author  = {Saldanha, Jane and Chakraborty, Shaunak and Patil, Shruti and Kotecha, Ketan and Kumar, Satish and Nayyar, Anand},
  title   = {Data augmentation using Variational Autoencoders for improvement of respiratory disease classification},
  number  = {8},
  pages   = {e0266467},
  volume  = {17},
  journal = {PLoS One},
  month   = {08},
  year    = {2022},
}

@InProceedings{Dwork2006,
author="Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam",
editor="Halevi, Shai and Rabin, Tal",
title="Calibrating Noise to Sensitivity in Private Data Analysis",
booktitle="Theory of Cryptography",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="265--284",
isbn="978-3-540-32732-5"
}


@article{Dwork2017,
title={Calibrating Noise to Sensitivity in Private Data Analysis},
volume={7},
url={https://journalprivacyconfidentiality.org/index.php/jpc/article/view/405},
DOI={10.29012/jpc.v7i3.405},
number={3},
journal={Journal of Privacy and Confidentiality},
author={Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
year={2017},
month={05},
pages={17–51}
}

@PhdThesis{Gentry2009,
  author = {Craig Gentry},
  title  = {A fully homomorphic encryption scheme},
  school = {Stanford University},
  year   = {2009},
}

@Online{Bos2013,
  author       = {Bos, Joppe W. and Lauter, Kristin and Loftus, Jake and Naehrig, Michael},
  title        = {Improved Security for a Ring-Based Fully Homomorphic Encryption Scheme},
  url          = {https://eprint.iacr.org/2013/075},
  howpublished = {Cryptology {ePrint} Archive, Paper 2013/075},
  year         = {2013},
}

@online{Gama2018,
      author = {Ilaria Chillotti and Nicolas Gama and Mariya Georgieva and Malika Izabachène},
      title = {{TFHE}: Fast Fully Homomorphic Encryption over the Torus},
      howpublished = {Cryptology {ePrint} Archive, Paper 2018/421},
      year = {2018},
      url = {https://eprint.iacr.org/2018/421}
}

@InProceedings{Cheon2017,
  author    = {Cheon, Jung Hee and Kim, Andrey and Kim, Miran and Song, Yongsoo},
  booktitle = {Advances in Cryptology -- ASIACRYPT 2017},
  title     = {Homomorphic Encryption for Arithmetic of Approximate Numbers},
  editor    = {Takagi, Tsuyoshi and Peyrin, Thomas},
  isbn      = {978-3-319-70694-8},
  pages     = {409--437},
  publisher = {Springer International Publishing},
  address   = {Cham},
  year      = {2017},
}

@InProceedings{zhao21,
  author    = {Zhao, Zilong and Kunar, Aditya and Birke, Robert and Chen, Lydia Y.},
  booktitle = {Proceedings of The 13th Asian Conference on Machine Learning},
  title     = {{CTAB-GAN}: Effective Table Data Synthesizing},
  editor    = {Balasubramanian, Vineeth N. and Tsang, Ivor},
  pages     = {97--112},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  url       = {https://proceedings.mlr.press/v157/zhao21a.html},
  volume    = {157},
  file      = {zhao21a.pdf:zhao21 - CTAB GAN_ Effective Table Data Synthesizing.pdf:PDF:https\://proceedings.mlr.press/v157/zhao21a/zhao21a.pdf},
  month     = {11},
  year      = {2021},
}

@online{mirza2014,
      title={Conditional Generative Adversarial Nets}, 
      author={Mehdi Mirza and Simon Osindero},
      year={2014},
      eprint={1411.1784},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1411.1784}, 
}

@online{mescheder2018,
      title={Which Training Methods for GANs do actually Converge?}, 
      author={Lars Mescheder and Andreas Geiger and Sebastian Nowozin},
      year={2018},
      eprint={1801.04406},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1801.04406}, 
}

@Article{Wong2024,
  author  = {Wong, Pui Ching and Abdullah, Shahrum Shah and Shapiai, Mohd Ibrahim},
  title   = {Exceptional performance with minimal data using a generative adversarial network for alzheimer's disease classification},
  doi     = {10.1038/s41598-024-66874-5},
  issn    = {2045-2322},
  number  = {1},
  pages   = {17037},
  url     = {https://doi.org/10.1038/s41598-024-66874-5},
  volume  = {14},
  day     = {24},
  journal = {Scientific Reports},
  month   = {07},
  year    = {2024},
}

@online{cheng2023,
      title={Intel TDX Demystified: A Top-Down Approach}, 
      author={Pau-Chen Cheng and Wojciech Ozga and Enriquillo Valdez and Salman Ahmed and Zhongshu Gu and Hani Jamjoom and Hubertus Franke and James Bottomley},
      year={2023},
      eprint={2303.15540},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2303.15540}, 
}

@inproceedings{NIPS2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan and Kaiser, Lukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{Attention is All you Need}},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@online{rogers20,
      title={A Primer in BERTology: What we know about how BERT works}, 
      author={Anna Rogers and Olga Kovaleva and Anna Rumshisky},
      year={2020},
      eprint={2002.12327},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2002.12327}, 
}

@InProceedings{devlin19,
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  doi       = {10.18653/v1/N19-1423},
  editor    = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  address   = {Minneapolis, Minnesota},
  month     = jun,
  year      = {2019},
}

@Article{Rumelhart1986,
  author  = {Rumelhart, David E. and Hinton and Geoffrey, E. and Williams, Ronald J.},
  title   = {Learning representations by back-propagating errors},
  doi     = {10.1038/323533a0},
  issn    = {1476-4687},
  number  = {6088},
  pages   = {533-536},
  url     = {https://doi.org/10.1038/323533a0},
  volume  = {323},
  day     = {01},
  journal = {Nature},
  month   = {10},
  year    = {1986},
}

@inproceedings{serrurier0,
author = {Béthune, Louis and Boissin, Thibaut and Serrurier, Mathieu and Mamalet, Franck and Friedrich, Corentin and Gonzalez-Sanz, Alberto},
title = {Pay attention to your loss: understanding misconceptions about 1-Lipschitz neural networks},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1460},
numpages = {15},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@InProceedings{Qiyang19,
  author    = {Li, Qiyang and Haque, Saminul and Anil, Cem and Lucas, James and Grosse, Roger B. and Jacobsen, Joern-Henrik},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d'Alché-Buc and E. Fox and R. Garnett},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/file/1ce3e6e3f452828e23a0c94572bef9d9-Paper.pdf},
  volume    = {32},
  year      = {2019},
}

@InProceedings{Sato18,
  author    = {Tsuzuku, Yusuke and Sato, Issei and Sugiyama, Masashi},
  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  date      = {2018},
  title     = {Lipschitz-margin training: scalable certification of perturbation invariance for deep neural networks},
  location  = {Montréal, Canada},
  pages     = {6542--6551},
  publisher = {Curran Associates Inc.},
  series    = {NIPS'18},
  address   = {Red Hook, NY, USA},
  numpages  = {10},
}

@Article{Loubes23,
AUTHOR = {Jourdan, Fanny and Kaninku, Titon Tshiongo and Asher, Nicholas and Loubes, Jean-Michel and Risser, Laurent},
TITLE = {How Optimal Transport Can Tackle Gender Biases in Multi-Class Neural Network Classifiers for Job Recommendations},
JOURNAL = {Algorithms},
VOLUME = {16},
YEAR = {2023},
NUMBER = {3},
ARTICLE-NUMBER = {174},
URL = {https://www.mdpi.com/1999-4893/16/3/174},
ISSN = {1999-4893},
DOI = {10.3390/a16030174}
}

@Book{Goodfellow16,
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  title     = {Deep Learning},
  publisher = {MIT Press},
  url       = {https://www.deeplearningbook.org/},
  year      = {2016},
}

@online{simonyan15,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.1556}, 
}

@Article{Shannon49,
  author  = {Claude Shannon},
  title   = {Communication in the presence of noise},
  volume  = {37},
  journal = {Proceedings of the Institute of Radio Engineers},
  year    = {1949},
}


@InCollection{Lecun97,
  author    = {Le Cun, Yann and Jackel, L. D. and Bottou, Leon and Cortes, Corinna and Denker, J. S. and Drucker, Harris and Guyon, I. and Muller, U. A. and Sackinger, Eduard and Simard, Patrice and Vapnik, V.},
  booktitle = {Neural networks},
  title     = {Learning algorithms for classification: A comparison on handwritten digit recognition},
  editor    = {Oh, J. H. and Kwon, C. and Cho, S.},
  language  = {English (US)},
  pages     = {261--276},
  publisher = {World Scientific},
  year      = {1995},
}

@article{Chawal02,
title={SMOTE: Synthetic Minority Over-sampling Technique},
   volume={16},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.953},
   DOI={10.1613/jair.953},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
   year={2002},
   month=jun, pages={321–357}
   }

@ARTICLE{Aung21,
  author={Sahakyan, Maria and Aung, Zeyar and Rahwan, Talal},
  journal={IEEE Access}, 
  title={Explainable Artificial Intelligence for Tabular Data: A Survey}, 
  year={2021},
  volume={9},
  pages={135392-135422},
  doi={10.1109/ACCESS.2021.3116481}
  }

  
@InProceedings{Cohen18,
  author    = {Cohen, Joseph Paul and Luck, Margaux and Honari, Sina},
  booktitle = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2018},
  title     = {Distribution Matching Losses Can Hallucinate Features in Medical Image Translation},
  editor    = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and Alberola-Lopez, Carlos and Fichtinger, Gabor},
  isbn      = {978-3-030-00928-1},
  pages     = {529--536},
  publisher = {Springer International Publishing},
  address   = {Cham},
  year      = {2018},
}

@Article{Conte2021,
  author   = {Conte, Gian Marco and Weston, Alexander D. and Vogelsang, David C. and Philbrick, Kenneth A. and Cai, Jason C. and Barbera, Maurizio and Sanvito, Francesco and Lachance, Daniel H. and Jenkins, Robert B. and Tobin, W. Oliver and Eckel-Passow, Jeanette E. and Erickson, Bradley J.},
  title    = {Generative Adversarial Networks to Synthesize Missing {T1} and {FLAIR} {MRI} Sequences for Use in a Multisequence Brain Tumor Segmentation Model},
  language = {en},
  number   = {2},
  pages    = {313--323},
  volume   = {299},
  address  = {United States},
  journal  = {Radiology},
  month    = mar,
  year     = {2021},
}

@Book{Wolfram0,
  author = 	 {Stephen Wolfram},
  ALTeditor = 	 {},
  title = 	 {What Is ChatGPT Doing: ... And Why Does It Work?},
  publisher = 	 {Wolfram media},
  year = 	 {2023},
  url = {https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/},
  isbn = {978-1579550813},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Article{Montastruc23,
  author   = {Montastruc, François and Storck, Wilhelm and de Canecaude, Claire and Victor, Léa and Li, Julien and Cesbron, Candice and Zelmat, Yoann and Barus, Romain},
  title    = {Will artificial intelligence chatbots replace clinical pharmacologists? An exploratory study in clinical practice},
  number   = {10},
  pages    = {1375--1384},
  volume   = {79},
  journal  = {Eur. J. Clin. Pharmacol.},
  keywords = {Adverse drug reaction; ChatGPT; Drug information service; Pharmacovigilance; Safety},
  month    = oct,
  year     = {2023},
}

@Article{Singhal2023,
  author   = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Babiker, Abubakr and Schärli, Nathanael and Chowdhery, Aakanksha and Mansfield, Philip and Demner-Fushman, Dina and Agüera y Arcas, Blaise and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
  title    = {Large language models encode clinical knowledge},
  doi      = {10.1038/s41586-023-06291-2},
  issn     = {1476-4687},
  number   = {7972},
  pages    = {172-180},
  url      = {https://doi.org/10.1038/s41586-023-06291-2},
  volume   = {620},
  abstract = {Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6{\%} accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17{\%}. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.},
  day      = {01},
  journal  = {Nature},
  month    = {9},
  year     = {2023},
}

@online{MedLLM,
  author={Hongjian Zhou and Fenglin Liu and Boyang Gu and Xinyu Zou and Jinfa Huang and Jinge Wu and Yiru Li and Sam S. Chen and Peilin Zhou and Junling Liu and Yining Hua and Chengfeng Mao and Chenyu You and Xian Wu and Yefeng Zheng and Lei Clifton and Zheng Li and Jiebo Luo and David A. Clifton},
  title = {MedLLM Practical Guide},
  url={https://github.com/AI-in-Health/MedLLMsPracticalGuide},
  year = {2024}, 
  urldate = {2024-07-19}
}

@PhdThesis{Granger02,
  author = {Geraud Granger},
  title  = {Détection et résolution de conflits aériens},
  url    = {https://amisducena.org/output/NL02-011.pdf},
  school = {Ecole Polytechnique},
  year   = {2002},
}

@online{pal2023,
      title={Med-HALT: Medical Domain Hallucination Test for Large Language Models}, 
      author={Ankit Pal and Logesh Kumar Umapathi and Malaikannan Sankarasubbu},
      year={2023},
      eprint={2307.15343},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.15343}, 
}

@Article{Hicks2024,
  author  = {Hicks, Michael Townsen and Humphries, James and Slater, Joe},
  title   = {ChatGPT is bullshit},
  doi     = {10.1007/s10676-024-09775-5},
  issn    = {1572-8439},
  number  = {2},
  pages   = {38},
  url     = {https://doi.org/10.1007/s10676-024-09775-5},
  volume  = {26},
  day     = {08},
  journal = {Ethics and Information Technology},
  month   = {6},
  year    = {2024},
}

@Article{Daun2008,
  author   = {Daun, Silvia and Rubin, Jonathan and Vodovotz, Yoram and Clermont, Gilles},
  title    = {Equation-based models of dynamic biological systems},
  language = {en},
  number   = {4},
  pages    = {585--594},
  volume   = {23},
  address  = {United States},
  journal  = {J Crit Care},
  month    = may,
  year     = {2008},
}

@online{mikolov2013,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}

@InProceedings{Mikolov13,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  editor    = {Burges, C. J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
  volume    = {26},
  year      = {2013},
}

@InProceedings{pennington2014,
  author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  title     = {{G}lo{V}e: Global Vectors for Word Representation},
  doi       = {10.3115/v1/D14-1162},
  editor    = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
  pages     = {1532--1543},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D14-1162},
  address   = {Doha, Qatar},
  month     = oct,
  year      = {2014},
}

@online{Tao0,
  author = {Terence Tao},
  title = {{AI, Machine Learning and Proof Assistants}},
  url={https://www.youtube.com/watch?v=Gw7r6YtwD_8},
  year = {2024}, 
  urldate = {2024-07-19}
}

@Article{Hester2011,
  author   = {Hester, Robert L. and Brown, Alison J. and Husband, Leland and Iliescu, Radu and Pruett, Drew and Summers, Richard and Coleman, Thomas G.},
  title    = {{HumMod}: A Modeling Environment for the Simulation of Integrative Human Physiology},
  language = {en},
  pages    = {12},
  volume   = {2},
  address  = {Switzerland},
  journal  = {Front Physiol},
  keywords = {HumMod; integrative physiology; model; physiome},
  month    = apr,
  year     = {2011},
}

@InProceedings{Savy2019,
  author = 	 {Nicolas Savy and Philippe Saint-Pierre and Stéphanie Savy and Sylvia Julien},
  title = 	 {{In Silico Clinical Trials}: a way to improve drug development?},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle =	 {Proceedings of JSM},
  year =	 {2019},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTaddress = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Article{JHolland0,
  author =       "John Holland",
  title =        "Outline for a logical theory of adaptative systems",
  OPTcrossref =  "",
  OPTkey =       "",
  journal =      "Journal for the Association of Computing Machinery",
  year =         "1962",
  volume =       "3",
  OPTnumber =    "",
  OPTpages =     "",
  OPTmonth =     "",
  OPTnote =      "",
  OPTannote =    ""
}

@Book{JHolland1,
  author =       "John Holland",
  title =        "Adaptation in Natural and Artificial Systems",
  publisher =    "The University of Michigan Press",
  year =         "1975",
  OPTcrossref =  "",
  OPTkey =       "",
  OPTeditor =    "",
  OPTvolume =    "",
  OPTnumber =    "",
  OPTseries =    "",
  OPTaddress =   "",
  OPTedition =   "",
  OPTmonth =     "",
  OPTnote =      "",
  OPTannote =    ""
}

@Book{Talking0,
  ALTauthor = 	 {},
  editor = 	 {James A. Anderson and Edward Rosenfeld},
  title = 	 {{Talking Nets}: an oral history of neural networks},
  publisher = 	 {MIT PRESS},
  year = 	 {1998},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@ARTICLE{Decker0,
  author={Decker, Keith S.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
  title={Distributed problem-solving techniques: A survey}, 
  year={1987},
  volume={17},
  number={5},
  pages={729-740},
  keywords={Problem-solving;Organizations;Artificial intelligence;Task analysis;Taxonomy;Sensors;Distributed processing},
  doi={10.1109/TSMC.1987.6499280}}

@Article{Wiener0,
  author = 	 {Arturo Rosenblueth and Norbert Wiener and Julian Bigelow},
  title = 	 {{Behavior, Purpose and Teleology}},
  journal = 	 {Philosophy of science},
  year = 	 {1943},
  OPTkey = 	 {},
  volume =	 {10},
  number =	 {1},
  pages =	 {18--24},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Article{McCulloch1943,
  author   = {McCulloch, Warren S. and Pitts, Walter},
  title    = {A logical calculus of the ideas immanent in nervous activity},
  doi      = {10.1007/BF02478259},
  issn     = {1522-9602},
  number   = {4},
  pages    = {115-133},
  url      = {https://doi.org/10.1007/BF02478259},
  volume   = {5},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  day      = {01},
  journal  = {The bulletin of mathematical biophysics},
  month    = {12},
  year     = {1943},
}

@book{Rosenblatt62,
  title={Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms},
  author={Rosenblatt, F.},
  lccn={62012882},
  series={Cornell Aeronautical Laboratory. Report no. VG-1196-G-8},
  url={https://books.google.fr/books?id=7FhRAAAAMAAJ},
  year={1962},
  publisher={Spartan Books}
}

@Book{Hebb49,
  author = 	 {Donald Hebb},
  ALTeditor = 	 {},
  title = 	 {{The Organization of Behavior}: a neuropsychological theory},
  publisher = 	 {John Wiley and sons},
  year = 	 {1949},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Book{Goldberg0,
  author =       "David Goldberg",
  title =        "Genetic Algorithms",
  publisher =    "Addison Wesley",
  year =         "1989",
  OPTcrossref =  "",
  OPTkey =       "",
  OPTeditor =    "",
  OPTvolume =    "",
  OPTnumber =    "",
  OPTseries =    "",
  OPTaddress =   "",
  OPTedition =   "",
  OPTmonth =     "",
  note =         "ISBN : 0-201-15767-5",
  OPTannote =    ""
}

@Book{Koza0,
  author = 	 {John R. Koza},
  ALTeditor = 	 {},
  title = 	 {{Genetic Programming}: On the Programming of Computers by Means of Natural Selection},
  publisher = 	 {MIT Press},
  year = 	 {1992},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@ARTICLE{Lecun98,
  author={Le Cun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  keywords={Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
  doi={10.1109/5.726791}}

@Book{Popper0,
  author = 	 {Karl Popper},
  ALTeditor = 	 {},
  title = 	 {Logik der Forschung},
  publisher = 	 {Julius Springer},
  year = 	 {1934},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Article{Shumailov2024,
  author  = {Shumailov, Ilia and Shumaylov, Zakharand Zhao, Yiren and Papernot, Nicolasand Anderson, Ross and Gal, Yarin},
  title   = {AI models collapse when trained on recursively generated data},
  doi     = {10.1038/s41586-024-07566-y},
  issn    = {1476-4687},
  number  = {8022},
  pages   = {755-759},
  url     = {https://doi.org/10.1038/s41586-024-07566-y},
  volume  = {631},
  day     = {01},
  journal = {Nature},
  month   = {7},
  year    = {2024},
}


@online{Speer0,
  author = {Robyn Speer},
  title = {Wordfreq: note from {September} 2024},
  url={https://github.com/rspeer/wordfreq/blob/master/SUNSET.md},
  year = {2024}, 
  urldate = {2024-10-06}
}

@online{zhou2024survey,
      title={A Survey of Large Language Models in Medicine: Progress, Application, and Challenge}, 
      author={Hongjian Zhou and Fenglin Liu and Boyang Gu and Xinyu Zou and Jinfa Huang and Jinge Wu and Yiru Li and Sam S. Chen and Peilin Zhou and Junling Liu and Yining Hua and Chengfeng Mao and Chenyu You and Xian Wu and Yefeng Zheng and Lei Clifton and Zheng Li and Jiebo Luo and David A. Clifton},
      year={2024},
      eprint={2311.05112},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.05112}, 
}

@online{schmidhuber2022,
      title={Annotated History of Modern AI and Deep Learning}, 
      author={Juergen Schmidhuber},
      year={2022},
      eprint={2212.11279},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2212.11279}, 
}

@ARTICLE{Franchet2024,
  title     = "Bias reduction using combined stain normalization and
               augmentation for {AI-based} classification of histological
               images",
  author    = "Franchet, Camille and Schwob, Robin and Bataillon, Guillaume and Syrykh, Charlotte and Péricart, Sarah and Frenois, François-Xavier and Penault-Llorca, Frédérique and Lacroix-Triki, Magali and Arnould, Laurent and Lemonnier, Jérôme and Alliot, Jean-Marc and Filleron, Thomas and Brousset, Pierre",
  journal   = "Comput. Biol. Med.",
  publisher = "Elsevier BV",
  volume    =  171,
  number    =  108130,
  month     =  3,
  year      =  2024,
  keywords  = "Bias mitigation; Color-induced bias; Data augmentation; Deep
               learning; Histopathology; Normalization",
  language  = "en"
}

@online{Hinton2012,
      title={Improving neural networks by preventing co-adaptation of feature detectors}, 
      author={Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov},
      year={2012},
      eprint={1207.0580},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1207.0580}, 
}

@online{lewis2021,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}

@Article{Amari72,
  author   = {Amari, Shunichi},
  title    = {Learning Patterns and Pattern Sequences by Self-Organizing Nets of Threshold Elements},
  doi      = {10.1109/T-C.1972.223477},
  number   = {11},
  pages    = {1197-1206},
  volume   = {C-21},
  journal  = {IEEE Transactions on Computers},
  keywords = {Associative memory, brain model, concept formation, logic nets of threshold elements, self-organization, sequential recalling, stability of state transition.},
  year     = {1972},
}

@Article{Richardson2025,
  author       = {Richardson, Reese A. K. and Hong, Spencer S. and Byrne, Jennifer A. and Stoeger, Thomas and Amaral, Luís A. Nunes},
  date         = {2025-08},
  journaltitle = {Proceedings of the National Academy of Sciences},
  title        = {The entities enabling scientific fraud at scale are large, resilient, and growing rapidly},
  doi          = {10.1073/pnas.2420092122},
  issn         = {1091-6490},
  number       = {32},
  volume       = {122},
  priority     = {prio1},
  publisher    = {Proceedings of the National Academy of Sciences},
}

@Book{ivak65,
  author    = {Ivakhnenko, A. G. and Lapa, V. G.},
  title     = {Cybernetic Predicting Devices},
  publisher = {Joint Publications Research Service [available from the Clearinghouse for Federal Scientific and Technical Information]},
  series    = {JPRS 37, 803},
  url       = {https://books.google.fr/books?id=l38DHQAACAAJ},
  year      = {1965},
}

@Online{DGAC,
  author  = {{Direction Générale de l'Aviation Civile}},
  title   = {Erato Electronic Environment},
  url     = {https://www.ecologie.gouv.fr/en/public-policies/erato-electronic-environment},
  urldate = {2024-07-19},
  year    = {2024},
}

@article{Tarski33,
author = {Alfred Tarski},
title = {Pojęcie prawdy w językach nauk dedukcyjnych},
journal = {Nakładem Towarzystwa Naukowego Warszawskiego},
year = {1933},
note = {Publié en anglais pour la première fois en 1956 sous le titre: {The Notion of Truth in Languages of Deductive Sciences}, disponible en version intégrale dans une ré-édition en 1983 de l'ensemble des ses articles de 1923 à 1938, corrigée par Tarski lui-même~\cite{Tarski83}}
}

@Book{Tarski83,
  author = 	 {Alfred Tarski},
  editor = 	 {John Corcoran},
  title = 	 {{Logic, Semantics, Metamathematics: Papers from 1923 to 1938}},
  publisher = 	 {Hackett Publishing Company},
  year = 	 {1983},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  note = 	 {Traduction de J.H. Woodger},
  OPTannote = 	 {}
}

@Article{Roche0,
  author      = {Alison Antoine and David Pérol and Mathieu Robain and Thomas Bachelot and Rémy Choquet and William Jacot and Béchir Ben Hadj Yahia and Thomas Grinda and Suzette Delaloge and Christine Lasset and Youenn Drouet},
  title       = {Assessing the real-world effectiveness of 8 major metastatic breast cancer drugs using target trial emulation},
  url         = {https://www.sciencedirect.com/science/article/pii/S0959804924012036},
  journal     = {European Journal of Cancer},
  optabstract = {Abstract While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.},
  optdoi      = {https://doi.org/10.1111/nyas.15125},
  opteprint   = {https://nyaspubs.onlinelibrary.wiley.com/doi/pdf/10.1111/nyas.15125},
  optnumber   = {1},
  optpages    = {15-18},
  optvolume   = {1534},
  year        = {2024},
}

@Article{Polya19,
  author =       {Georges Polya},
  title =        {Verschiedene Bemerkungen zur Zahlentheorie},
  journal =      {Jahresbericht der Deutschen Mathematiker-Vereinigung},
  year =         {1919},
  OPTkey =       {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTpages =     {},
  OPTmonth =     {},
  OPTnote =      {},
  OPTannote =    {}
}

@Article{Tanaka80,
  author =       {Minoru Tanaka},
  title =        {A Numerical Investigation on Cumulative Sum of the Liouville Function},
  journal =      {Tokyo Journal of Mathematics},
  year =         {1980},
  OPTkey =       {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTpages =     {},
  OPTmonth =     {},
  OPTnote =      {},
  OPTannote =    {}
}

@misc{wei2024,
      title={Measuring short-form factuality in large language models}, 
      author={Jason Wei and Nguyen Karina and Hyung Won Chung and Yunxin Joy Jiao and Spencer Papay and Amelia Glaese and John Schulman and William Fedus},
      year={2024},
      eprint={2411.04368},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.04368}, 
}

@Article{Godel31,
  author =       {Kurt Gödel},
  title =        {Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I},
  journal =      {Monatshefte für Mathematik und Physik},
  year =         {1931},
  OPTkey =       {},
  volume =       {38},
  OPTnumber =    {},
  pages =        {173--198},
  OPTmonth =     {},
  OPTnote =      {},
  OPTannote =    {}
}

@Article{5392560,
  author  = {Samuel, A. L.},
  title   = {Some Studies in Machine Learning Using the Game of Checkers},
  doi     = {10.1147/rd.33.0210},
  number  = {3},
  pages   = {210-229},
  volume  = {3},
  journal = {IBM Journal of Research and Development},
  year    = {1959},
}

@Book{Alliot93,
  author =       {Jean-Marc Alliot and Thomas Schiex},
  ALTeditor =    {},
  title =        {Intelligence Artificielle et Informatique Théorique},
  publisher =    {Cepadues},
  year =         {1993},
  OPTkey =       {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTedition =   {},
  OPTmonth =     {},
  isbn =         {2-85428-324-4},
  OPTannote =    {}
}

@Book{Alliot02,
  author =       {Jean-Marc Alliot and Thomas Schiex and Pascal Brisset and Frédéric Garcia},
  ALTeditor =    {},
  title =        {Intelligence Artificielle et Informatique Théorique},
  publisher =    {Cepadues},
  year =         {2002},
  OPTkey =       {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTedition =   {},
  OPTmonth =     {},
  isbn =         {2-85428-578-6},
  note =    {Seconde édition de \cite{Alliot93}}
}

 
@Article{Azizi2021,
  author       = {Azizi, Zahra and Zheng, Chaoyi and Mosquera, Lucy and Pilote, Louise and El Emam, Khaled},
  date         = {2021-04-16},
  journaltitle = {{BMJ} open},
  title        = {Can synthetic data be a proxy for real clinical trial data? A validation study},
  doi          = {10.1136/bmjopen-2020-043497},
  issn         = {2044-6055},
  number       = {4},
  pages        = {e043497},
  volume       = {11},
  abstract     = {{OBJECTIVES}: There are increasing requirements to make research data, especially clinical trial data, more broadly available for secondary analyses. However, data availability remains a challenge due to complex privacy requirements. This challenge can potentially be addressed using synthetic data. {SETTING}: Replication of a published stage {III} colon cancer trial secondary analysis using synthetic data generated by a machine learning method. {PARTICIPANTS}: There were 1543 patients in the control arm that were included in our analysis. {PRIMARY} {AND} {SECONDARY} {OUTCOME} {MEASURES}: Analyses from a study published on the real dataset were replicated on synthetic data to investigate the relationship between bowel obstruction and event-free survival. Information theoretic metrics were used to compare the univariate distributions between real and synthetic data. Percentage {CI} overlap was used to assess the similarity in the size of the bivariate relationships, and similarly for the multivariate Cox models derived from the two datasets. {RESULTS}: Analysis results were similar between the real and synthetic datasets. The univariate distributions were within 1\% of difference on an information theoretic metric. All of the bivariate relationships had {CI} overlap on the tau statistic above 50\%. The main conclusion from the published study, that lack of bowel obstruction has a strong impact on survival, was replicated directionally and the {HR} {CI} overlap between the real and synthetic data was 61\% for overall survival (real data: {HR} 1.56, 95\% {CI} 1.11 to 2.2; synthetic data: {HR} 2.03, 95\% {CI} 1.44 to 2.87) and 86\% for disease-free survival (real data: {HR} 1.51, 95\% {CI} 1.18 to 1.95; synthetic data: {HR} 1.63, 95\% {CI} 1.26 to 2.1). {CONCLUSIONS}: The high concordance between the analytical results and conclusions from synthetic and real data suggests that synthetic data can be used as a reasonable proxy for real clinical trial datasets. {TRIAL} {REGISTRATION} {NUMBER}: {NCT}00079274.},
  keywords     = {Disease-Free Survival, Humans, Progression-Free Survival, Proportional Hazards Models, epidemiology, health informatics, information management, information technology, statistics \& research methods},
  pmcid        = {PMC8055130},
  pmid         = {33863713},
  shortjournal = {{BMJ} Open},
  shorttitle   = {Can synthetic data be a proxy for real clinical trial data?},
}

@Article{Roberts2021,
  author       = {Roberts, Michael and Driggs, Derek and Matthew, Thorpe and Julian, Gilbey and Michael, Yeung and Stephan, Ursprung and Aviles-Rivero Angelica, I. and Christian, Etmann and Cathal, McCague and Lucian, Beer and Weir-McCall Jonathan, R. and Zhongzhao, Teng and Effrossyni, Gkrania-Klotsas and Alessandro, Ruggiero and Anna, Korhonen and Emily, Jefferson and Emmanuel, Ako and Georg, Langs and Ghassem, Gozaliasl and Guang, Yang and Helmut, Prosch and Jan, Preller Jacobusand Stanczuk and Jing, Tang and Johannes, Hofmanninger and Judith, Babar and Escudero, Sanchez Lorena and Muhunthan, Thillai and Martin, Gonzalez Paula and Xiaoxiang, Zhu and Philip, Teare and Mishal, Patel and Conor, Cafolla and Hojjat, Azadbakht and Joseph, Jacob and Josh, Lowe and Kang, Zhang and Kyle, Bradley and Marcel, Wassin and Markus, Holzer and Kangyu, Ji and Delgado, Ortet Maria and Nicholas, Ai Taoand Walton and Pietro, Lio and Samuel, Stranks and Tolou, Shadbahr and Weizhe, Lin and Yunfei, Zha and Zhangming, Niu and Evis, Rudd James H. F.and Sala and Carola-Bibiane, Schanlieb and A. I. X.-C. O. V. N. E., T.},
  date         = {2021-03},
  journaltitle = {Nature Machine Intelligence},
  title        = {Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans},
  doi          = {10.1038/s42256-021-00307-0},
  issn         = {2522-5839},
  number       = {3},
  pages        = {199-217},
  url          = {https://doi.org/10.1038/s42256-021-00307-0},
  urldate      = {2025-01-14},
  volume       = {3},
  abstract     = {Machine learning methods offer great promise for fast and accurate detection and prognostication of coronavirus disease 2019 ({COVID}-19) from standard-of-care chest radiographs ({CXR}) and chest computed tomography ({CT}) images. Many articles have been published in 2020 describing new machine learning-based models for both of these tasks, but it is unclear which are of potential clinical utility. In this systematic review, we consider all published papers and preprints, for the period from 1 January 2020 to 3 October 2020, which describe new machine learning models for the diagnosis or prognosis of {COVID}-19 from {CXR} or {CT} images. All manuscripts uploaded to {bioRxiv}, {medRxiv} and {arXiv} along with all entries in {EMBASE} and {MEDLINE} in this timeframe are considered. Our search identified 2,212 studies, of which 415 were included after initial screening and, after quality screening, 62 studies were included in this systematic review. Our review finds that none of the models identified are of potential clinical use due to methodological flaws and/or underlying biases. This is a major weakness, given the urgency with which validated {COVID}-19 models are needed. To address this, we give many recommendations which, if followed, will solve these issues and lead to higher-quality model development and well-documented manuscripts.},
  day          = {01},
  file         = {Full Text PDF:Roberts2021 - Common Pitfalls and Recommendations for Using Machine Learning to Detect and Prognosticate for COVID 19 Using Chest Radiographs and CT Scans.pdf:PDF:https\://www.nature.com/articles/s42256-021-00307-0.pdf},
  journal      = {Nature Machine Intelligence},
  keywords     = {Computational science, Diagnostic markers, Prognostic markers, {SARS}-{CoV}-2},
  langid       = {english},
  month        = {03},
  publisher    = {Nature Publishing Group},
  shortjournal = {Nat Mach Intell},
  year         = {2021},
}

 
@Article{Thorlund2020,
  author       = {Thorlund, Kristian and Dron, Louis and Park, Jay J. H. and Mills, Edward J.},
  date         = {2020},
  journaltitle = {Clinical Epidemiology},
  title        = {Synthetic and External Controls in Clinical Trials - A Primer for Researchers},
  doi          = {10.2147/CLEP.S242097},
  issn         = {1179-1349},
  pages        = {457--467},
  volume       = {12},
  abstract     = {There has been a rapid expansion in the use of non-randomized evidence in the regulatory approval of treatments globally. An emerging set of methodologies have been utilized to provide greater insight into external control data used for these purposes, collectively known as synthetic control methods. Through this paper, we provide the reader with a set of key questions to help assess the quality of literature publications utilizing synthetic control methodologies. Common challenges and real-life examples of synthetic controls are provided throughout, alongside a critical appraisal framework with which to assess future publications.},
  keywords     = {{RCTs}, real-world evidence, synthetic control},
  pmcid        = {PMC7218288},
  pmid         = {32440224},
  shortjournal = {Clin Epidemiol},
}

@Article{Leibig2022,
  author       = {Leibig, Christian and Brehmer, Moritz and Stefan, Bunk and Danalyn, Byng and Lale, Pinker Katjaand Umutlu},
  date         = {2022-07},
  journaltitle = {The Lancet. Digital Health},
  title        = {Combining the strengths of radiologists and AI for breast cancer screening: a retrospective analysis},
  doi          = {10.1016/S2589-7500(22)00070-X},
  issn         = {2589-7500},
  number       = {7},
  pages        = {e507-e519},
  url          = {https://doi.org/10.1016/S2589-7500(22)00070-X},
  volume       = {4},
  abstract     = {{BACKGROUND}: We propose a decision-referral approach for integrating artificial intelligence ({AI}) into the breast-cancer screening pathway, whereby the algorithm makes predictions on the basis of its quantification of uncertainty. Algorithmic assessments with high certainty are done automatically, whereas assessments with lower certainty are referred to the radiologist. This two-part {AI} system can triage normal mammography exams and provide post-hoc cancer detection to maintain a high degree of sensitivity. This study aimed to evaluate the performance of this {AI} system on sensitivity and specificity when used either as a standalone system or within a decision-referral approach, compared with the original radiologist decision. {METHODS}: We used a retrospective dataset consisting of 1 193 197 full-field, digital mammography studies carried out between Jan 1, 2007, and Dec 31, 2020, from eight screening sites participating in the German national breast-cancer screening programme. We derived an internal-test dataset from six screening sites (1670 screen-detected cancers and 19 997 normal mammography exams), and an external-test dataset of breast cancer screening exams (2793 screen-detected cancers and 80 058 normal exams) from two additional screening sites to evaluate the performance of an {AI} algorithm on sensitivity and specificity when used either as a standalone system or within a decision-referral approach, compared with the original individual radiologist decision at the point-of-screen reading ahead of the consensus conference. Different configurations of the {AI} algorithm were evaluated. To account for the enrichment of the datasets caused by oversampling cancer cases, weights were applied to reflect the actual distribution of study types in the screening programme. Triaging performance was evaluated as the rate of exams correctly identified as normal. Sensitivity across clinically relevant subgroups, screening sites, and device manufacturers was compared between standalone {AI}, the radiologist, and decision referral. We present receiver operating characteristic ({ROC}) curves and area under the {ROC} ({AUROC}) to evaluate {AI}-system performance over its entire operating range. Comparison with radiologists and subgroup analysis was based on sensitivity and specificity at clinically relevant configurations. {FINDINGS}: The exemplary configuration of the {AI} system in standalone mode achieved a sensitivity of 84·2\% (95\% {CI} 82·4-85·8) and a specificity of 89·5\% (89·0-89·9) on internal-test data, and a sensitivity of 84·6\% (83·3-85·9) and a specificity of 91·3\% (91·1-91·5) on external-test data, but was less accurate than the average unaided radiologist. By contrast, the simulated decision-referral approach significantly improved upon radiologist sensitivity by 2·6 percentage points and specificity by 1·0 percentage points, corresponding to a triaging performance at 63·0\% on the external dataset; the {AUROC} was 0·982 (95\% {CI} 0·978-0·986) on the subset of studies assessed by {AI}, surpassing radiologist performance. The decision-referral approach also yielded significant increases in sensitivity for a number of clinically relevant subgroups, including subgroups of small lesion sizes and invasive carcinomas. Sensitivity of the decision-referral approach was consistent across the eight included screening sites and three device manufacturers. {INTERPRETATION}: The decision-referral approach leverages the strengths of both the radiologist and {AI}, demonstrating improvements in sensitivity and specificity surpassing that of the individual radiologist and of the standalone {AI} system. This approach has the potential to improve the screening accuracy of radiologists, is adaptive to the requirements of screening, and could allow for the reduction of workload ahead of the consensus conference, without discarding the generalised knowledge of radiologists. {FUNDING}: Vara.},
  day          = {01},
  journal      = {The Lancet Digital Health},
  keywords     = {Artificial Intelligence, Breast Neoplasms, Early Detection of Cancer, Female, Humans, Radiologists, Retrospective Studies},
  month        = {07},
  pmcid        = {PMC9839981},
  pmid         = {35750400},
  publisher    = {Elsevier},
  shortjournal = {Lancet Digit Health},
  shorttitle   = {Combining the strengths of radiologists and {AI} for breast cancer screening},
  year         = {2022},
}

 
@Article{Lambert2023,
  author       = {Lambert, Jérôme and Lengliné, Etienne and Porcher, Raphaël and Thiébaut, Rodolphe and Zohar, Sarah and Chevret, Sylvie},
  date         = {2023-10-10},
  journaltitle = {Blood Advances},
  title        = {Enriching single-arm clinical trials with external controls: possibilities and pitfalls},
  doi          = {10.1182/bloodadvances.2022009167},
  issn         = {2473-9537},
  number       = {19},
  pages        = {5680--5690},
  volume       = {7},
  abstract     = {For the past decade, it has become commonplace to provide rapid answers and early patient access to innovative treatments in the absence of randomized clinical trials ({RCT}), with benefits estimated from single-arm trials. This trend is important in oncology, notably when assessing new targeted therapies. Some of those uncontrolled trials further include an external/synthetic control group as an innovative way to provide an indirect comparison with a pertinent control group. We aimed to provide some guidelines as a comprehensive tool for (1) the critical appraisal of those comparisons or (2) for performing a single-arm trial. We used the example of ciltacabtagene autoleucel for the treatment of adult patients with relapsed or refractory multiple myeloma after 3 or more treatment lines as an illustrative example. We propose a 3-step guidance. The first step includes the definition of an estimand, which encompasses the treatment effect and the targeted population (whole population or restricted to single-arm trial or external controls), reflecting a clinical question. The second step relies on the adequate selection of external controls from previous {RCTs} or real-world data from patient cohorts, registries, or electronic patient files. The third step consists of choosing the statistical approach targeting the treatment effect defined above and depends on the available data (individual-level data or aggregated external data). The validity of the treatment effect derived from indirect comparisons heavily depends on careful methodological considerations included in the proposed 3-step procedure. Because the level of evidence of a well-conducted {RCT} cannot be guaranteed, the evaluation is more important than in standard settings.},
  keywords     = {Adult, Humans, Medical Oncology, Multiple Myeloma, Clinical Trials as Topic},
  pmcid        = {PMC10539876},
  pmid         = {36534147},
  shortjournal = {Blood Adv},
  shorttitle   = {Enriching single-arm clinical trials with external controls},
}

@Article{Guillaudeux2023,
  author       = {Guillaudeux, Morgan and Rousseau, Olivia and Petot, Julien and Bennis, Zineb and Dein, Charles-Axel and Goronflot, Thomas and Vince, Nicolas and Limou, Sophie and Karakachoff, Matilde and Wargny, Matthieu and Gourraud, Pierre-Antoine},
  date         = {2023-03-10},
  journaltitle = {{NPJ} digital medicine},
  title        = {Patient-centric synthetic data generation, no reason to risk re-identification in biomedical data analysis},
  doi          = {10.1038/s41746-023-00771-5},
  issn         = {2398-6352},
  number       = {1},
  pages        = {37},
  url          = {https://doi.org/10.1038/s41746-023-00771-5},
  volume       = {6},
  abstract     = {While nearly all computational methods operate on pseudonymized personal data, re-identification remains a risk. With personal health data, this re-identification risk may be considered a double-crossing of patients' trust. Herein, we present a new method to generate synthetic data of individual granularity while holding on to patients' privacy. Developed for sensitive biomedical data, the method is patient-centric as it uses a local model to generate random new synthetic data, called an "avatar data", for each initial sensitive individual. This method, compared with 2 other synthetic data generation techniques (Synthpop, {CT}-{GAN}), is applied to real health data with a clinical trial and a cancer observational study to evaluate the protection it provides while retaining the original statistical information. Compared to Synthpop and {CT}-{GAN}, the Avatar method shows a similar level of signal maintenance while allowing to compute additional privacy metrics. In the light of distance-based privacy metrics, each individual produces an avatar simulation that is on average indistinguishable from 12 other generated avatar simulations for the clinical trial and 24 for the observational study. Data transformation using the Avatar method both preserves, the evaluation of the treatment's effectiveness with similar hazard ratios for the clinical trial (original {HR} = 0.49 [95\% {CI}, 0.39-0.63] vs. avatar {HR} = 0.40 [95\% {CI}, 0.31-0.52]) and the classification properties for the observational study (original {AUC} = 99.46 (s.e. 0.25) vs. avatar {AUC} = 99.84 (s.e. 0.12)). Once validated by privacy metrics, anonymous synthetic data enable the creation of value from sensitive pseudonymized data analyses by tackling the risk of a privacy breach.},
  day          = {10},
  journal      = {{NPJ} digital medicine},
  month        = {03},
  pmcid        = {PMC10006164},
  pmid         = {36899082},
  shortjournal = {{NPJ} Digit Med},
  year         = {2023},
}

 
@Article{Gonzales2023,
  author       = {Gonzales, Aldren and Guruswamy, Guruprabha and Smith, Scott R.},
  date         = {2023-01},
  journaltitle = {{PLOS} digital health},
  title        = {Synthetic data in health care: A narrative review},
  doi          = {10.1371/journal.pdig.0000082},
  issn         = {2767-3170},
  number       = {1},
  pages        = {e0000082},
  volume       = {2},
  abstract     = {Data are central to research, public health, and in developing health information technology ({IT}) systems. Nevertheless, access to most data in health care is tightly controlled, which may limit innovation, development, and efficient implementation of new research, products, services, or systems. Using synthetic data is one of the many innovative ways that can allow organizations to share datasets with broader users. However, only a limited set of literature is available that explores its potentials and applications in health care. In this review paper, we examined existing literature to bridge the gap and highlight the utility of synthetic data in health care. We searched {PubMed}, Scopus, and Google Scholar to identify peer-reviewed articles, conference papers, reports, and thesis/dissertations articles related to the generation and use of synthetic datasets in health care. The review identified seven use cases of synthetic data in health care: a) simulation and prediction research, b) hypothesis, methods, and algorithm testing, c) epidemiology/public health research, d) health {IT} development, e) education and training, f) public release of datasets, and g) linking data. The review also identified readily and publicly accessible health care datasets, databases, and sandboxes containing synthetic data with varying degrees of utility for research, education, and software development. The review provided evidence that synthetic data are helpful in different aspects of health care and research. While the original real data remains the preferred choice, synthetic data hold possibilities in bridging data access gaps in research and evidence-based policymaking.},
  pmcid        = {PMC9931305},
  pmid         = {36812604},
  shortjournal = {{PLOS} Digit Health},
  shorttitle   = {Synthetic data in health care},
}

 
@Article{Krenmayr2022,
  author       = {Krenmayr, Lucas and Frank, Roland and Drobig, Christina and Braungart, Michael and Seidel, Jan and Schaudt, Daniel and von Schwerin, Reinhold and Stucke-Straub, Kathrin},
  date         = {2022-01-01},
  journaltitle = {Informatics in Medicine Unlocked},
  title        = {{GANerAid}: Realistic synthetic patient data for clinical trials},
  doi          = {10.1016/j.imu.2022.101118},
  issn         = {2352-9148},
  pages        = {101118},
  url          = {https://www.sciencedirect.com/science/article/pii/S2352914822002556},
  urldate      = {2025-01-14},
  volume       = {35},
  abstract     = {Human data must be considered one of the most valuable resources of our time, both in research and business contexts. However, particularly in fields that heavily rely on clinical information, such as medicine or pharmacy, not only the collection of patient data is expensive and time consuming, but, due to data protection laws and regulations, the ways of how to use them are strictly limited, deeming reuse or sharing very difficult, if not impossible. One promising solution to overcome these problems are artificially created data points with the same statistical properties as the investigated patient population. In this paper, we propose the {GANerAid} architecture, utilising a Generative Adversarial Network ({GAN}) approach to create such synthetic patients from random noise. Unlike other methods, {GANerAid} is based on long short-term memory ({LSTM}) layers and is thus able to preserve underlying data properties, such as correlations and variable distributions, leading to more satisfying results, even in small-sized samples, with acceptable training speed. {GANerAid} is published as an open source library and released as a ready-to-use package for Python 3.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S2352914822002556/pdfft?download=true:application/pdf},
  keywords     = {Synthetic patients, Machine learning, Generative Adversarial Network, Tabular data, Random noise, Long short-term memory, Framework, Bag of rows},
  shortjournal = {Informatics in Medicine Unlocked},
  shorttitle   = {{GANerAid}},
}

@Article{Moradi2024,
  author       = {Moradi, Masoud and Demirel, Hasan},
  date         = {2024-03-01},
  journaltitle = {Signal, Image and Video Processing},
  title        = {Alzheimer's disease classification using 3D conditional progressive GAN- and LDA-based data selection},
  doi          = {10.1007/s11760-023-02878-4},
  issn         = {1863-1711},
  number       = {2},
  pages        = {1847-1861},
  url          = {https://doi.org/10.1007/s11760-023-02878-4},
  urldate      = {2025-01-14},
  volume       = {18},
  abstract     = {Alzheimer’s disease is a kind of neurological disorder that directly impacts the memory of a patient. Structural magnetic resonance imaging ({sMRI}) is an effective representation for the diagnosis of neurodegenerative diseases. Deep learning strategies, such as convolutional neural networks ({CNNs}), require an enormous amount of data to generalize the target disease. Given the restrictions on collecting data, augmentation methods are important tools for increasing the number of samples available for training a {CNN}. Recently, generative adversarial networks ({GANs}) have been employed to generate synthetic medical data such as {sMRI}. In this paper, we propose a conditional progressive {GAN} ({cProGAN}) for data augmentation. The proposed {cProGAN} utilizes additive noise, which is regulated by the feedback from the discriminator that is trained by labeled data. The synthetic samples generated by using {cProGAN} go through a sample selection process regulated by the distributions of the original data mapped into the linear discriminator analysis ({LDA}) space. Three-class labeled data are mapped into {LDA} space where each class is modeled within an elliptic confidence subspace. Generated synthetic data that falls into these class subspaces are selected as the synthetic data to be used for training the {CNN}. This strategy helps select the most relevant samples with the desired class. Evidently, based on the experimental results, the suggested {cProGAN} creates synthetic data with higher quality than other state-of-the-art approaches. Furthermore, class-specific {LDA} subspace post-processing helps the selection of class-separated augmented data for improved classification performance.},
  day          = {01},
  journal      = {Signal, Image and Video Processing},
  keywords     = {Alzheimer’s disease, Generative adversarial networks, {LDA}, Deep learning, Classification},
  langid       = {english},
  month        = {03},
  shortjournal = {{SIViP}},
  year         = {2024},
}

@Article{Olazaran1996,
  author       = {Mikel Olazaran},
  date         = {1996},
  journaltitle = {Social Studies of Science},
  title        = {A Sociological Study of the Official History of the Perceptrons Controversy},
  issn         = {0306-3127},
  number       = {3},
  pages        = {611--659},
  url          = {http://www.jstor.org/stable/285702},
  urldate      = {2024-09-16},
  volume       = {26},
  abstract     = {In this paper, I analyze the controversy within Artificial Intelligence ({AI}) which surrounded the 'perceptron' project (and neural nets in general) in the late 1950s and early 1960s. I devote particular attention to the proofs and arguments of Minsky and Papert, which were interpreted as showing that further progress in neural nets was not possible, and that this approach to {AI} had to be abandoned. I maintain that this official interpretation of the debate was a result of the emergence, institutionalization and (importantly) legitimation of the symbolic {AI} approach (with its resource allocation system and authority structure). At the 'research-area' level, there was considerable interpretative flexibility. This interpretative flexibility was further demonstrated by the revival of neural nets in the late 1980s, and subsequent rewriting of the official history of the debate.},
  file         = {JSTOR Full Text PDF:https\://www.jstor.org/stable/pdfplus/10.2307/285702.pdf?acceptTC=true:application/pdf},
  journal      = {Social Studies of Science},
  publisher    = {Sage Publications, Ltd.},
  year         = {1996},
}

@Article{Hochreiter1997,
  author       = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date         = {1997-11-15},
  journaltitle = {Neural Computation},
  title        = {Long Short-term Memory},
  doi          = {10.1162/neco.1997.9.8.1735},
  issn         = {0899-7667},
  number       = {8},
  pages        = {1735-80},
  volume       = {9},
  abstract     = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  journal      = {Neural computation},
  keywords     = {Algorithms, Learning, Memory, Memory, Short-Term, Models, Neurological, Models, Psychological, Nerve Net, Neural Networks, Computer, Time Factors},
  pmid         = {9377276},
  shortjournal = {Neural Comput},
  year         = {1997},
}

 
@InProceedings{Werbos1982,
  author    = {Werbos, Paul J.},
  booktitle = {System Modeling and Optimization},
  date      = {1982},
  title     = {Applications of advances in nonlinear sensitivity analysis},
  doi       = {10.1007/BFb0006203},
  editor    = {Drenick, R. F. and Kozin, F.},
  isbn      = {9783540394594},
  location  = {Berlin, Heidelberg},
  pages     = {762--770},
  publisher = {Springer},
  abstract  = {The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost. The areas of application include the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling. The details, references and derivations can be obtained by requesting „Sensitivity Analysis Methods for Nonlinear Systems“ from Forecast Analysis and Evaluation Team, Quality Assurance, {OSS}/{EIA}, Room 7413, Department of Energy, Washington, {DC} 20461.},
  langid    = {english},
}

@Article{Dempster77,
  author       = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  date         = {1977},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
  title        = {Maximum Likelihood from Incomplete Data Via the {EM} Algorithm},
  doi          = {10.1111/j.2517-6161.1977.tb01600.x},
  eprint       = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1977.tb01600.x},
  issn         = {2517-6161},
  number       = {1},
  pages        = {1-22},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1977.tb01600.x},
  urldate      = {2025-01-14},
  volume       = {39},
  abstract     = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
  file         = {Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.2517-6161.1977.tb01600.x:application/pdf},
  journal      = {Journal of the Royal Statistical Society: Series B (Methodological)},
  keywords     = {maximum likelihood, incomplete data, em algorithm, posterior mode},
  langid       = {english},
  rights       = {© 1977 The Authors},
  year         = {1977},
}

@Article{Lee2020,
  author       = {Lee, Dongwookand Moon and Won-Jinand Ye, Jong Chul},
  date         = {2020-01},
  journaltitle = {Nature Machine Intelligence},
  title        = {Assessing the importance of magnetic resonance contrasts using collaborative generative adversarial networks},
  doi          = {10.1038/s42256-019-0137-x},
  issn         = {2522-5839},
  number       = {1},
  pages        = {34-42},
  url          = {https://doi.org/10.1038/s42256-019-0137-x},
  urldate      = {2025-01-14},
  volume       = {2},
  abstract     = {A unique advantage of magnetic resonance imaging ({MRI}) is its mechanism for generating various image contrasts depending on tissue-specific parameters, which provides useful clinical information. Unfortunately, a complete set of {MR} contrasts is often difficult to obtain in a real clinical environment. Recently, there have been claims that generative models such as generative adversarial networks ({GANs}) can synthesize {MR} contrasts that are not acquired. However, the poor scalability of existing {GAN}-based image synthesis poses a fundamental challenge to understanding the nature of {MR} contrasts: which contrasts matter, and which cannot be synthesized by generative models? Here, we show that these questions can be addressed systematically by learning the joint manifold of multiple {MR} contrasts using collaborative generative adversarial networks. Our experimental results show that the exogenous contrast provided by contrast agents is not replaceable, but endogenous contrasts such as T1 and T2 can be synthesized from other contrasts. These findings provide important guidance for the acquisition-protocol design of {MR} in clinical environments.},
  howpublished = {{OriginalPaper}},
  journal      = {Nature Machine Intelligence},
  keywords     = {Biomedical engineering, Computer science, Medical imaging},
  langid       = {english},
  month        = {1},
  publisher    = {Nature Publishing Group},
  rights       = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  shortjournal = {Nat Mach Intell},
  type         = {{OriginalPaper}},
  year         = {2020},
}

@Article{Alkaissi2023,
  author       = {Alkaissi, Hussam and McFarlane, Samy I.},
  journaltitle = {Cureus},
  title        = {Artificial Hallucinations in {ChatGPT}: Implications in Scientific Writing},
  doi          = {10.7759/cureus.35179},
  issn         = {2168-8184},
  language     = {en},
  number       = {2},
  pages        = {e35179},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9939079/},
  urldate      = {2025-01-14},
  volume       = {15},
  abstract     = {While still in its infancy, {ChatGPT} (Generative Pretrained Transformer), introduced in November 2022, is bound to hugely impact many industries, including healthcare, medical education, biomedical research, and scientific writing. Implications of {ChatGPT}, that new chatbot introduced by {OpenAI} on academic writing, is largely unknown. In response to the Journal of Medical Science (Cureus) Turing Test - call for case reports written with the assistance of {ChatGPT}, we present two cases one of homocystinuria-associated osteoporosis, and the other is on late-onset Pompe disease ({LOPD}), a rare metabolic disorder. We tested {ChatGPT} to write about the pathogenesis of these conditions. We documented the positive, negative, and rather troubling aspects of our newly introduced chatbot’s performance.},
  address      = {United States},
  file         = {PubMed Central Full Text PDF:Alkaissi2023 - Artificial Hallucinations in ChatGPT_ Implications in Scientific Writing.pdf:PDF:https\://pmc.ncbi.nlm.nih.gov/articles/PMC9939079/pdf/cureus-0015-00000035179.pdf},
  journal      = {Cureus},
  keywords     = {artificial intelligence and education; artificial intelligence and writing; artificial intelligence in medicine; chatbot; chatgpt},
  month        = feb,
  pmcid        = {PMC9939079},
  pmid         = {36811129},
  shortjournal = {Cureus},
  shorttitle   = {Artificial Hallucinations in {ChatGPT}},
  year         = {2023},
}

@Article{Yala2021,
  author       = {Yala, Adam and Mikhael, Peter G. and Strand, Fredrik and Lin, Gigin and Smith, Kevin and Wan, Yung-Liang and Lamb, Leslie and Hughes, Kevin and Lehman, Constance and Barzilay, Regina},
  date         = {2021-01-27},
  journaltitle = {Science Translational Medicine},
  title        = {Toward robust mammography-based models for breast cancer risk},
  doi          = {10.1126/scitranslmed.aba4373},
  eprint       = {https://www.science.org/doi/pdf/10.1126/scitranslmed.aba4373},
  issn         = {1946-6242},
  number       = {578},
  pages        = {eaba4373},
  url          = {https://www.science.org/doi/abs/10.1126/scitranslmed.aba4373},
  volume       = {13},
  abstract     = {Improved breast cancer risk models enable targeted screening strategies that achieve earlier detection and less screening harm than existing guidelines. To bring deep learning risk models to clinical practice, we need to further refine their accuracy, validate them across diverse populations, and demonstrate their potential to improve clinical workflows. We developed Mirai, a mammography-based deep learning model designed to predict risk at multiple timepoints, leverage potentially missing risk factor information, and produce predictions that are consistent across mammography machines. Mirai was trained on a large dataset from Massachusetts General Hospital ({MGH}) in the United States and tested on held-out test sets from {MGH}, Karolinska University Hospital in Sweden, and Chang Gung Memorial Hospital ({CGMH}) in Taiwan, obtaining C-indices of 0.76 (95\% confidence interval, 0.74 to 0.80), 0.81 (0.79 to 0.82), and 0.79 (0.79 to 0.83), respectively. Mirai obtained significantly higher 5-year {ROC} {AUCs} than the Tyrer-Cuzick model ( P {\textless} 0.001) and prior deep learning models Hybrid {DL} ( P {\textless} 0.001) and Image-Only {DL} ( P {\textless} 0.001), trained on the same dataset. Mirai more accurately identified high-risk patients than prior methods across all datasets. On the {MGH} test set, 41.5\% (34.4 to 48.5) of patients who would develop cancer within 5 years were identified as high risk, compared with 36.1\% (29.1 to 42.9) by Hybrid {DL} ( P = 0.02) and 22.9\% (15.9 to 29.6) by the Tyrer-Cuzick model ( P {\textless} 0.001).},
  journal      = {Science Translational Medicine},
  keywords     = {Breast, Breast Neoplasms, Female, Humans, Mammography, Risk Assessment, Sweden, Taiwan},
  pmid         = {33504648},
  shortjournal = {Sci Transl Med},
  year         = {2021},
}

 
@Misc{Nguyen2024,
  author     = {Nguyen, Quang H. and Hoang, Duy C. and Decugis, Juliette and Manchanda, Saurav and Chawla, Nitesh V. and Doan, Khoa D.},
  date       = {2024-07},
  title      = {{MetaLLM}: A High-performant and Cost-efficient Dynamic Framework for Wrapping {LLMs}},
  doi        = {10.48550/arXiv.2407.10834},
  eprint     = {2407.10834 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2407.10834},
  urldate    = {2025-01-14},
  abstract   = {The rapid progress in machine learning ({ML}) has brought forth many large language models ({LLMs}) that excel in various tasks and areas. These {LLMs} come with different abilities and costs in terms of computation or pricing. Since the demand for each query can vary, e.g., because of the queried domain or its complexity, defaulting to one {LLM} in an application is not usually the best choice, whether it is the biggest, priciest, or even the one with the best average test performance. Consequently, picking the right {LLM} that is both accurate and cost-effective for an application remains a challenge. In this paper, we introduce {MetaLLM}, a framework that dynamically and intelligently routes each query to the optimal {LLM} (among several available {LLMs}) for classification tasks, achieving significantly improved accuracy and cost-effectiveness. By framing the selection problem as a multi-armed bandit, {MetaLLM} balances prediction accuracy and cost efficiency under uncertainty. Our experiments, conducted on popular {LLM} platforms such as {OpenAI}'s {GPT} models, Amazon's Titan, Anthropic's Claude, and Meta's {LLaMa}, showcase {MetaLLM}'s efficacy in real-world scenarios, laying the groundwork for future extensions beyond classification tasks.},
  file       = {Preprint PDF:Nguyen2024 - MetaLLM_ a High Performant and Cost Efficient Dynamic Framework for Wrapping LLMs.pdf:PDF:http\://arxiv.org/pdf/2407.10834v2},
  keywords   = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  number     = {{arXiv}:2407.10834},
  publisher  = {{arXiv}},
}

 
@Article{Dijkstra2018,
  author       = {Dijkstra, Suzan and Kok, Gautam and Ledford, Julie G. and Sandalova, Elena and Stevelink, Remi},
  date         = {2018-12},
  journaltitle = {Frontiers in Medicine},
  title        = {Possibilities and {Pitfalls} of {Social} {Media} for {Translational} {Medicine}},
  doi          = {10.3389/fmed.2018.00345},
  issn         = {2296-858X},
  pages        = {345},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6291449/},
  urldate      = {2025-01-14},
  volume       = {5},
  abstract     = {We live in an age where the sharing of scientific findings and ideas is no longer confined to people with access to academic libraries or scientific journals. Social media have permitted for knowledge and ideas to be shared with an unprecedented speed and magnitude. This has made it possible for research findings to have a greater impact and to be rapidly implemented in society. However, the spread of unfiltered, unreferenced, and non-peer-reviewed articles through social media comes with dangers as well. In this perspective article, we aim to address both the possibilities and pitfalls of social media for translational medicine. We describe how social media can be used for patient engagement, publicity, transparency, sharing of knowledge, and implementing findings in society. Moreover, we warn about the potential pitfalls of social media, which can cause research to be misinterpreted and false beliefs to be spread. We conclude by giving advice on how social media can be harnessed to combat the pitfalls and provide a new avenue for community engagement in translational medicine.},
  file         = {PubMed Central Full Text PDF:Dijkstra2018 - Possibilities and Pitfalls of Social Media for Translational Medicine.pdf:PDF:https\://pmc.ncbi.nlm.nih.gov/articles/PMC6291449/pdf/fmed-05-00345.pdf},
  pmcid        = {PMC6291449},
  pmid         = {30574495},
}

 
@Book{Bergstrom2020,
  author     = {Bergstrom, Carl T. and West, Jevin Darwin},
  publisher  = {Random House},
  title      = {Calling {Bullshit}: {The} {Art} of {Skepticism} in a {Data}-driven {World}},
  isbn       = {9780525509189},
  abstract   = {Bullshit isn't what it used to be. Now, two science professors give us the tools to dismantle misinformation and think clearly in a world of fake news and bad data. "A modern classic . . . a straight-talking survival guide to the mean streets of a dying democracy and a global pandemic."--WiredMisinformation, disinformation, and fake news abound and it's increasingly difficult to know what's true. Our media environment has become hyperpartisan. Science is conducted by press release. Startup culture elevates bullshit to high art. We are fairly well equipped to spot the sort of old-school bullshit that is based in fancy rhetoric and weasel words, but most of us don't feel qualified to challenge the avalanche of new-school bullshit presented in the language of math, science, or statistics. In Calling Bullshit,  Professors Carl Bergstrom and Jevin West give us a set of powerful tools to cut through the most intimidating data. You don't need a lot of technical expertise to call out problems with data. Are the numbers or results too good or too dramatic to be true? Is the claim comparing like with like? Is it confirming your personal bias? Drawing on a deep well of expertise in statistics and computational biology, Bergstrom and West exuberantly unpack examples of selection bias and muddled data visualization, distinguish between correlation and causation, and examine the susceptibility of science to modern bullshit. We have always needed people who call bullshit when necessary, whether within a circle of friends, a community of scholars, or the citizenry of a nation. Now that bullshit has evolved, we need to relearn the art of skepticism.},
  date       = {2020},
  keywords   = {Language Arts \& Disciplines / Linguistics / Sociolinguistics, Philosophy / General, Philosophy / History \& Surveys / General, Political Science / Corruption \& Misconduct, Psychology / Applied Psychology},
  language   = {en},
  shorttitle = {Calling {Bullshit}},
}

@Article{Glickman2024,
  author       = {Glickman, Moshe and Sharot, Tali},
  date         = {2024-12},
  journaltitle = {Nature Human Behaviour},
  title        = {How human–AI feedback loops alter human perceptual, emotional and social judgements},
  doi          = {10.1038/s41562-024-02077-2},
  issn         = {2397-3374},
  publisher    = {Springer Science and Business Media LLC},
}

@Article{Hinton2018,
  author       = {Hinton, Geoffrey},
  date         = {2018-09},
  journaltitle = {JAMA},
  title        = {Deep Learning—A Technology With the Potential to Transform Health Care},
  doi          = {10.1001/jama.2018.11100},
  issn         = {0098-7484},
  number       = {11},
  pages        = {1101},
  volume       = {320},
  publisher    = {American Medical Association (AMA)},
}

@Article{Brady2021,
  author       = {Brady, Adrian P.},
  date         = {2021-04},
  journaltitle = {EMJ Radiology},
  title        = {Artificial Intelligence in Radiology: An Exciting Future, but Ethically Complex},
  doi          = {10.33590/emjradiol/21f10415},
  issn         = {2633-9978},
  note         = {https://www.youtube.com/watch?v=2HMPRXstSvQ People should stop training radiologists now. It's just completely obvious that within 5 years, deep learning is going to do better than radiologists, because it's going to be able to get a lot more experience.*},
  pages        = {54--57},
  comment      = {*People should stop training radiologists now. It's just completely obvious that within 5 years, deep learning is going to do better than radiologists, because it's going to be able to get a lot more experience.*},
  publisher    = {European Medical Group},
}

@Misc{Hinton2016,
  author  = {Geoffrey Hinton},
  date    = {2016-11-24},
  editor  = {{Creative Destruction Lab}},
  title   = {The future of radiology},
  url     = {https://www.youtube.com/watch?v=2HMPRXstSvQ},
  comment = {*People should stop training radiologists now. It's just completely 
obvious that within 5 years, deep learning is going to do better than 
radiologists.*},
}

@Article{Righetti2019,
  author       = {Righetti, Ludovic and Madhavan, Raj and Chatila, Raja},
  date         = {2019-09},
  journaltitle = {IEEE Robotics and Automation Magazine},
  title        = {Unintended Consequences of Biased Robotic and Artificial Intelligence Systems [Ethical, Legal, and Societal Issues]},
  doi          = {10.1109/mra.2019.2926996},
  issn         = {1558-223X},
  number       = {3},
  pages        = {11--13},
  volume       = {26},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{Grant2023,
  author       = {Nico Grant and Kashmir Hill},
  date         = {2023-05-22},
  journaltitle = {The New-York Times},
  title        = {Google’s Photo App Still Can’t Find Gorillas. And Neither Can Apple’s.},
  url          = {https://www.nytimes.com/2023/05/22/technology/ai-photo-labels-google-apple.html},
}

@Article{Chang2023,
  author       = {Chang, Xinyu},
  date         = {2023-09},
  journaltitle = {Advances in Economics, Management and Political Sciences},
  title        = {Gender Bias in Hiring: An Analysis of the Impact of Amazon’s Recruiting Algorithm},
  doi          = {10.54254/2754-1169/23/20230367},
  issn         = {2754-1177},
  number       = {1},
  pages        = {134--140},
  volume       = {23},
  publisher    = {EWA Publishing},
}

@Misc{Dastin2018,
  author = {Jeffrey Dastin},
  date   = {2018-10-11},
  editor = {Reuters},
  title  = {Amazon scraps secret AI recruiting tool that showed bias against women},
  url    = {https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/},
}

@Book{Grother2019,
  author      = {Grother, Patrick and Ngan, Mei and Hanaoka, Kayee},
  date        = {2019-12},
  title       = {Face recognition vendor test part 3:: demographic effects},
  doi         = {10.6028/nist.ir.8280},
  institution = {National Institute of Standards and Technology},
}

@InProceedings{Gursoy2022,
  author    = {Gursoy, Furkan and Kakadiaris, Ioannis A.},
  booktitle = {2022 IEEE International Conference on Data Mining Workshops (ICDMW)},
  date      = {2022-11},
  title     = {Equal Confusion Fairness: Measuring Group-Based Disparities in Automated Decision Systems},
  doi       = {10.1109/icdmw58026.2022.00027},
  pages     = {137--146},
  publisher = {IEEE},
}

@Misc{Angwin2016,
  author = {Julia Angwin and Jeff Larson and Surya Mattu and Lauren Kirchner},
  date   = {2016-05-23},
  editor = {ProPublica},
  title  = {Machine Bias},
  url    = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
}

@Article{Obermeyer2019,
  author       = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
  date         = {2019-10},
  journaltitle = {Science},
  title        = {Dissecting racial bias in an algorithm used to manage the health of populations},
  doi          = {10.1126/science.aax2342},
  issn         = {1095-9203},
  number       = {6464},
  pages        = {447--453},
  volume       = {366},
  publisher    = {American Association for the Advancement of Science (AAAS)},
}

@Article{Hernstroem2025,
  author       = {Hernström, Veronica and Josefsson, Viktoria and Sartor, Hanna and Schmidt, David and Larsson, Anna-Maria and Hofvind, Solveig and Andersson, Ingvar and Rosso, Aldana and Hagberg, Oskar and Lång, Kristina},
  date         = {2025-02},
  journaltitle = {The Lancet Digital Health},
  title        = {Screening performance and characteristics of breast cancer detected in the Mammography Screening with Artificial Intelligence trial (MASAI): a randomised, controlled, parallel-group, non-inferiority, single-blinded, screening accuracy study},
  doi          = {10.1016/s2589-7500(24)00267-x},
  issn         = {2589-7500},
  publisher    = {Elsevier BV},
}

@Misc{Xiao2025,
  author    = {Xiao, Tong and Zhu, Jingbo},
  date      = {2025},
  title     = {Foundations of Large Language Models},
  doi       = {10.48550/ARXIV.2501.09223},
  url       = {https://arxiv.org/abs/2501.09223},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@InBook{Alliot2016,
  author    = {Alliot, Jean-Marc and Demolombe, Robert and Diéguez, Martín and Fariñas del Cerro, Luis and Favre, Gilles and Faye, Jean-Charles and Obeid, Naji and Sordet, Olivier},
  booktitle = {Towards Paraconsistent Engineering},
  date      = {2016},
  title     = {Temporal Logic Modeling of Biological Systems},
  doi       = {10.1007/978-3-319-40418-9_11},
  isbn      = {9783319404189},
  pages     = {205--226},
  publisher = {Springer International Publishing},
  issn      = {1868-4408},
}

@Article{DanguydesDeserts2023,
  author       = {Danguy des Déserts, Alice and Durand, Nicolas and Servin, Bertrand and Goudemand-Dugué, Ellen and Alliot, Jean-Marc and Ruiz, Daniel and Charmet, Gilles and Elsen, Jean-Michel and Bouchet, Sophie},
  date         = {2023-08},
  journaltitle = {G3: Genes, Genomes, Genetics},
  title        = {Comparison of genomic-enabled cross selection criteria for the improvement of inbred line breeding populations},
  doi          = {10.1093/g3journal/jkad195},
  editor       = {Jannink, J-L},
  issn         = {2160-1836},
  number       = {11},
  volume       = {13},
  publisher    = {Oxford University Press (OUP)},
}

@InBook{Alliot2018,
  author    = {Alliot, Jean-Marc and Demolombe, Robert and Fariñas del Cerro, Luis and Diéguez, Martín and Obeid, Naji},
  booktitle = {Interactions Between Computational Intelligence and Mathematics},
  date      = {2018},
  title     = {Abductive Reasoning on Molecular Interaction Maps},
  doi       = {10.1007/978-3-319-74681-4_4},
  isbn      = {9783319746814},
  pages     = {43--56},
  publisher = {Springer International Publishing},
  issn      = {1860-9503},
}

@Article{Bennis2023,
  author       = {Bennis, Achraf and Leleux, Philippe and Canali, Alban and Pourtales, Caroline De and Mouysset, Sandrine and Simoncini, David and Brousset, Pierre and Frenois, Francois-Xavier and Recher, Christian and Alliot, Jean-Marc and Rieu, Jean-Baptiste and Bertoli, Sarah},
  date         = {2023-08},
  journaltitle = {HemaSphere},
  title        = {PB1764: SEGMENTATION AND CLASSIFICATION OF BONE MARROW CELLS FROM MULTI-PRECISION NUMERIZATION OF BONE MARROW SMEARS (BMS) FROM PATIENTS WITH ACUTE MYELOID LEUKEMIA (AML) USING AI TECHNIQUES},
  doi          = {10.1097/01.hs9.0000973912.51274.96},
  issn         = {2572-9241},
  number       = {S3},
  pages        = {e5127496},
  volume       = {7},
  publisher    = {Wiley},
}

@Article{Didi2024,
  author       = {Didi, Ibrahim and Alliot, Jean-Marc and Dumas, Pierre-Yves and Vergez, François and Tavitian, Suzanne and Largeaud, Laëtitia and Bidet, Audrey and Rieu, Jean-Baptiste and Luquet, Isabelle and Lechevalier, Nicolas and Delabesse, Eric and Sarry, Audrey and De Grande, Anne-Charlotte and Bérard, Emilie and Pigneux, Arnaud and Récher, Christian and Simoncini, David and Bertoli, Sarah},
  date         = {2024-01},
  journaltitle = {Leukemia Research},
  title        = {Artificial intelligence-based prediction models for acute myeloid leukemia using real-life data: A DATAML registry study},
  doi          = {10.1016/j.leukres.2024.107437},
  issn         = {0145-2126},
  pages        = {107437},
  volume       = {136},
  publisher    = {Elsevier BV},
}

@Article{Delehelle2018,
  author       = {Delehelle, Franklin and Cussat-Blanc, Sylvain and Alliot, Jean-Marc and Luga, Hervé and Balaresque, Patricia},
  date         = {2018-03},
  journaltitle = {Bioinformatics},
  title        = {ASGART: fast and parallel genome scale segmental duplications mapping},
  doi          = {10.1093/bioinformatics/bty172},
  editor       = {Hancock, John},
  issn         = {1367-4811},
  number       = {16},
  pages        = {2708--2714},
  volume       = {34},
  publisher    = {Oxford University Press (OUP)},
}

@Misc{Modarressi2025,
  author    = {Modarressi, Ali and Deilamsalehy, Hanieh and Dernoncourt, Franck and Bui, Trung and Rossi, Ryan A. and Yoon, Seunghyun and Schütze, Hinrich},
  date      = {2025},
  title     = {NoLiMa: Long-Context Evaluation Beyond Literal Matching},
  doi       = {10.48550/ARXIV.2502.05167},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Benzekry2020,
  author       = {Benzekry, Sebastien},
  date         = {2020-08},
  journaltitle = {Clinical Pharmacology &amp; Therapeutics},
  title        = {Artificial Intelligence and Mechanistic Modeling for Clinical Decision Making in Oncology},
  doi          = {10.1002/cpt.1951},
  issn         = {1532-6535},
  number       = {3},
  pages        = {471--486},
  volume       = {108},
  publisher    = {Wiley},
}

@Article{Benzekry2024,
  author       = {Benzekry, Sebastien and Mastri, Michalis and Nicolò, Chiara and Ebos, John M. L.},
  date         = {2024-05},
  journaltitle = {PLOS Computational Biology},
  title        = {Machine-learning and mechanistic modeling of metastatic breast cancer after neoadjuvant treatment},
  doi          = {10.1371/journal.pcbi.1012088},
  editor       = {Maini, Philip K.},
  issn         = {1553-7358},
  number       = {5},
  pages        = {e1012088},
  volume       = {20},
  publisher    = {Public Library of Science (PLoS)},
}

@Misc{Wei2022,
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date      = {2022},
  title     = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  doi       = {10.48550/ARXIV.2201.11903},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Kojima2022,
  author    = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  date      = {2022},
  title     = {Large Language Models are Zero-Shot Reasoners},
  doi       = {10.48550/ARXIV.2205.11916},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Xu2025,
  author    = {Xu, Silei and Xie, Wenhao and Zhao, Lingxiao and He, Pengcheng},
  date      = {2025},
  title     = {Chain of Draft: Thinking Faster by Writing Less},
  doi       = {10.48550/ARXIV.2502.18600},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Zhang2022,
  author    = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  date      = {2022},
  title     = {Automatic Chain of Thought Prompting in Large Language Models},
  doi       = {10.48550/ARXIV.2210.03493},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Wang2022,
  author    = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  date      = {2022},
  title     = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  doi       = {10.48550/ARXIV.2203.11171},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Liu2021,
  author    = {Liu, Jiacheng and Liu, Alisa and Lu, Ximing and Welleck, Sean and West, Peter and Bras, Ronan Le and Choi, Yejin and Hajishirzi, Hannaneh},
  date      = {2021},
  title     = {Generated Knowledge Prompting for Commonsense Reasoning},
  doi       = {10.48550/ARXIV.2110.08387},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Yao2023,
  author    = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  date      = {2023},
  title     = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  doi       = {10.48550/ARXIV.2305.10601},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Long2023,
  author    = {Long, Jieyi},
  date      = {2023},
  title     = {Large Language Model Guided Tree-of-Thought},
  doi       = {10.48550/ARXIV.2305.08291},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Lewis2020,
  author    = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
  date      = {2020},
  title     = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  doi       = {10.48550/ARXIV.2005.11401},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Paranjape2023,
  author    = {Paranjape, Bhargavi and Lundberg, Scott and Singh, Sameer and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Ribeiro, Marco Tulio},
  date      = {2023},
  title     = {ART: Automatic multi-step reasoning and tool-use for large language models},
  doi       = {10.48550/ARXIV.2303.09014},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Diao2023,
  author    = {Diao, Shizhe and Wang, Pengcheng and Lin, Yong and Pan, Rui and Liu, Xiang and Zhang, Tong},
  date      = {2023},
  title     = {Active Prompting with Chain-of-Thought for Large Language Models},
  doi       = {10.48550/ARXIV.2302.12246},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Li2023,
  author    = {Li, Zekun and Peng, Baolin and He, Pengcheng and Galley, Michel and Gao, Jianfeng and Yan, Xifeng},
  date      = {2023},
  title     = {Guiding Large Language Models via Directional Stimulus Prompting},
  doi       = {10.48550/ARXIV.2302.11520},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Gao2022,
  author    = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  date      = {2022},
  title     = {PAL: Program-aided Language Models},
  doi       = {10.48550/ARXIV.2211.10435},
  copyright = {Creative Commons Zero v1.0 Universal},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Yao2022,
  author    = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  date      = {2022},
  title     = {ReAct: Synergizing Reasoning and Acting in Language Models},
  doi       = {10.48550/ARXIV.2210.03629},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Shinn2023,
  author    = {Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  date      = {2023},
  title     = {Reflexion: Language Agents with Verbal Reinforcement Learning},
  doi       = {10.48550/ARXIV.2303.11366},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Zhang2023,
  author    = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  date      = {2023},
  title     = {Multimodal Chain-of-Thought Reasoning in Language Models},
  doi       = {10.48550/ARXIV.2302.00923},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  keywords  = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Huang2023,
  author    = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Patra, Barun and Liu, Qiang and Aggarwal, Kriti and Chi, Zewen and Bjorck, Johan and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and Wei, Furu},
  date      = {2023},
  title     = {Language Is Not All You Need: Aligning Perception with Language Models},
  doi       = {10.48550/ARXIV.2302.14045},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Liu2023,
  author    = {Liu, Zemin and Yu, Xingtong and Fang, Yuan and Zhang, Xinming},
  date      = {2023},
  title     = {GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks},
  doi       = {10.48550/ARXIV.2302.08043},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Article{Scarselli2009,
  author       = {Scarselli, F. and Gori, M. and Ah Chung Tsoi and Hagenbuchner, M. and Monfardini, G.},
  date         = {2009-01},
  journaltitle = {IEEE Transactions on Neural Networks},
  title        = {The Graph Neural Network Model},
  doi          = {10.1109/tnn.2008.2005605},
  issn         = {1941-0093},
  number       = {1},
  pages        = {61--80},
  volume       = {20},
  priority     = {prio1},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Misc{Guan2025,
  author    = {Guan, Xinyan and Zeng, Jiali and Meng, Fandong and Xin, Chunlei and Lu, Yaojie and Lin, Hongyu and Han, Xianpei and Sun, Le and Zhou, Jie},
  date      = {2025},
  title     = {DeepRAG: Thinking to Retrieval Step by Step for Large Language Models},
  doi       = {10.48550/ARXIV.2502.01142},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Information Retrieval (cs.IR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Wu2025,
  author    = {Wu, Junde and Zhu, Jiayuan and Liu, Yuyuan},
  date      = {2025},
  title     = {Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research},
  doi       = {10.48550/ARXIV.2502.04644},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Article{Rudolf2025,
  author       = {Jurgen Rudolf and Fadhil Ismail and Shannon Tan and Pauline Seah},
  date         = {2025-02},
  journaltitle = {Journal of Applied Learning and Teaching},
  title        = {Don’t believe the hype. AI myths and the need for a critical approach in higher education},
  doi          = {10.37074/jalt.2025.8.1.1},
  issn         = {2591-801X},
  number       = {1},
  volume       = {8},
  priority     = {prio1},
  publisher    = {Kaplan Higher Education Academy Pte Ltd},
}

@Book{Narayanan2024,
  author    = {Arvind Narayanan and Sayash Kapoor},
  date      = {2024-09-24},
  title     = {AI Snake Oil},
  doi       = {10.1353/book.129007},
  isbn      = {9780691249131},
  publisher = {Princeton University Press},
  priority  = {prio1},
}

@Book{Grier2013,
  author    = {Grier, David Alan},
  date      = {2013},
  title     = {When Computers Were Human},
  isbn      = {9780691133829},
  location  = {Princeton},
  pagetotal = {423},
  publisher = {Princeton University Press},
  ppn_gvk   = {776346032},
}

@Book{Bamford1988,
  author    = {Bamford, James},
  date      = {1988},
  title     = {The puzzle palace},
  edition   = {Repr.},
  isbn      = {0140067485},
  location  = {Harmondsworth [u.a.]},
  pagetotal = {655},
  publisher = {Penguin Books},
  series    = {The national bestseller},
  subtitle  = {A report on America's most secret agency},
  ppn_gvk   = {1609973518},
}

@Article{Lawner2002,
  author       = {Lawner, Kevin J.},
  date         = {2002-09},
  journaltitle = {Pace International Law Review},
  title        = {Post-Sept. 11th International Surveillance Activity - A Failure of Intelligence: The Echelon Interception System \& the Fundamental Right to Privacy in Europe},
  doi          = {10.58948/2331-3536.1202},
  issn         = {2331-3536},
  number       = {2},
  pages        = {435},
  volume       = {14},
  publisher    = {Pace University Press},
}

@Book{Bamford2008,
  author    = {Bamford, James},
  date      = {2008},
  title     = {The Shadow Factory},
  isbn      = {9780385528399},
  location  = {Westminster},
  pagetotal = {1332},
  publisher = {Knopf Doubleday Publishing Group},
  subtitle  = {The Ultra-Secret NSA from 9/11 to the Eavesdropping on America},
  ppn_gvk   = {170237341X},
}

@Book{Mencken1918,
  author = {Henry Louis Mencken},
  date   = {1918},
  title  = {In defense of women},
}

@Book{Wiener1948,
  author    = {Wiener, Norbert},
  date      = {1948},
  title     = {Cybernetics or Control and Communication in the Animal and the Machine},
  doi       = {10.7551/mitpress/11810.001.0001},
  isbn      = {9780262355902},
  note      = {Réédition de la deuxième édition de \cite{Wiener1948}},
  publisher = {The MIT Press},
  url       = {https://direct.mit.edu/books/oa-monograph-pdf/2254528/book_9780262355902.pdf},
}

@Book{Conway2005,
  author    = {Conway, Flo and Siegelman, Jim},
  date      = {2005},
  title     = {Dark hero of the information age},
  isbn      = {9780465013715},
  location  = {New York},
  note      = {Includes bibliographical references (p. 399-411) and index},
  pagetotal = {42316},
  publisher = {Basic Books},
  subtitle  = {In search of Norbert Wiener, the father of cybernetics},
  ppn_gvk   = {1602749426},
}

@Misc{Brandom2019,
  author = {Brandom, Russell},
  date   = {2019-08-09},
  editor = {{The Verge}},
  title  = {AI pioneer accused of having sex with trafficking victim on Jeffrey Epstein’s island},
  url    = {https://www.theverge.com/2019/8/9/20798900/marvin-minsky-jeffrey-epstein-sex-trafficking-island-court-records-unsealed},
}

@Misc{Betley2025,
  author    = {Betley, Jan and Tan, Daniel and Warncke, Niels and Sztyber-Betley, Anna and Bao, Xuchan and Soto, Martín and Labenz, Nathan and Evans, Owain},
  date      = {2025},
  title     = {Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs},
  doi       = {10.48550/ARXIV.2502.17424},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Cryptography and Security (cs.CR), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

 
@Misc{Berglund2024,
  author     = {Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  date       = {2024-05-26},
  title      = {The Reversal Curse: {LLMs} trained on "A is B" fail to learn "B is A"},
  doi        = {10.48550/arXiv.2309.12288},
  eprint     = {2309.12288 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2309.12288},
  urldate    = {2025-03-11},
  abstract   = {We expose a surprising failure of generalization in auto-regressive large language models ({LLMs}). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Valentina Tereshkova was the first woman to travel to space", it will not automatically be able to answer the question, "Who was the first woman to travel to space?". Moreover, the likelihood of the correct answer ("Valentina Tershkova") will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if "A is B" occurs, "B is A" is more likely to occur. It is worth noting, however, that if "A is B" appears in-context, models can deduce the reverse relationship. We provide evidence for the Reversal Curse by finetuning {GPT}-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of Abyssal Melodies" and showing that they fail to correctly answer "Who composed Abyssal Melodies?". The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate {ChatGPT} ({GPT}-3.5 and {GPT}-4) on questions about real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". {GPT}-4 correctly answers questions like the former 79\% of the time, compared to 33\% for the latter. Code available at: https://github.com/lukasberglund/reversal\_curse.},
  file       = {Preprint PDF:Berglund2024 - The Reversal Curse_ LLMs Trained on _A Is B_ Fail to Learn _B Is A_.pdf:PDF:http\://arxiv.org/pdf/2309.12288v4},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  number     = {{arXiv}:2309.12288},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

 
@Misc{Jin2020,
  author     = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  date       = {2020-09-28},
  title      = {What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},
  doi        = {10.48550/arXiv.2009.13081},
  eprint     = {2009.13081 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2009.13081},
  urldate    = {2025-03-14},
  abstract   = {Open domain question answering ({OpenQA}) tasks have been recently attracting more and more attention from the natural language processing ({NLP}) community. In this work, we present the first free-form multiple-choice {OpenQA} dataset for solving medical problems, {MedQA}, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7{\textbackslash}\%, 42.0{\textbackslash}\%, and 70.1{\textbackslash}\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect {MedQA} to present great challenges to existing {OpenQA} systems and hope that it can serve as a platform to promote much stronger {OpenQA} models from the {NLP} community in the future.},
  file       = {Preprint PDF:Jin2020 - What Disease Does This Patient Have_ a Large Scale Open Domain Question Answering Dataset from Medical Exams.pdf:PDF:http\://arxiv.org/pdf/2009.13081v1},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  number     = {{arXiv}:2009.13081},
  publisher  = {{arXiv}},
}

 
@InProceedings{Snyder1990,
  author     = {Snyder, Wesley and Nissman, Daniel and Van den Bout, David and Bilbro, Griff},
  booktitle  = {Advances in Neural Information Processing Systems},
  date       = {1990},
  title      = {Kohonen Networks and Clustering: Comparative Performance in Color Clustering},
  publisher  = {Morgan-Kaufmann},
  url        = {https://papers.nips.cc/paper_files/paper/1990/hash/371bce7dc83817b7893bcdeed13799b5-Abstract.html},
  urldate    = {2025-03-14},
  volume     = {3},
  abstract   = {The problem of color clustering is defined and shown to be a problem of  assigning a large number (hundreds of thousands) of 3-vectors to a  small number (256) of clusters. Finding those clusters in such a way that  they best represent a full color image using only 256 distinct colors is a  burdensome computational problem. In this paper, the problem is solved  using "classical" techniques -- k-means clustering, vector quantization  (which turns out to be the same thing in this application), competitive  learning, and Kohonen self-organizing feature maps. Quality of the  result is judged subjectively by how much the pseudo-color result  resembles the true color image, by {RMS} quantization error, and by run  time. The Kohonen map provides the best solution.},
  file       = {Full Text PDF:Snyder1990 - Kohonen Networks and Clustering_ Comparative Performance in Color Clustering.pdf:PDF:https\://papers.nips.cc/paper_files/paper/1990/file/371bce7dc83817b7893bcdeed13799b5-Paper.pdf},
  shorttitle = {Kohonen Networks and Clustering},
}

@Misc{McCarthy1955,
  author = {McCarthy, John and Minsky, Marvin and Rochester, Nathaniel and Shannon, Claude},
  date   = {1955-08-31},
  title  = {A proposal for the Dartmouth summer research project on Artificial Intelligence},
  url    = {http://jmc.stanford.edu/articles/dartmouth/dartmouth.pdf},
}



@Article{Carthy88,
  author = 	 {McCarthy, John},
  title = 	 {Review of \enquote{The Question of Artificial Intelligence}},
  journal = 	 {Annals of the History of Computing},
  year = 	 {1988},
  OPTkey = 	 {},
  volume =	 {10},
  number =	 {3},
  pages =	 {224--229},
  OPTmonth = 	 {},
  note =	 {Reprinted in \cite{McCarthy1996}},
  OPTannote = 	 {}
}

@InBook{McCarthy1996,
  author = 	 {McCarthy, John},
  ALTeditor = 	 {},
  title = 	 {Defending AI Research},
  chapter = 	 {Review of The Question of Artificial Intelligence},
  publisher = 	 {CSLI Publications},
  year = 	 {1996},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTtype = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTpages = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Misc{Minsky52,
  author    = {Minsky, Marvin},
  date      = {1952},
  title     = {A Neural-Analogue Calculator Based Upon a Probability Model of Reinforcement},
  note = {Harvard University Psychological Laboratories, Cambridge, Massachusetts},
}

@Misc{Pata16,
  author    = {Massimiliano Patacchiola},
  date      = {2016},
  title     = {Python implementation of the Epigenetic Robotic Architecture},
  note = {https://github.com/mpatacchiola/pyERA},
}


@Misc{Petrov2025,
  author    = {Petrov, Ivo and Dekoninck, Jasper and Baltadzhiev, Lyuben and Drencheva, Maria and Minchev, Kristian and Balunović, Mislav and Jovanović, Nikola and Vechev, Martin},
  date      = {2025},
  title     = {Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad},
  doi       = {10.48550/ARXIV.2503.21934},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Article{LeCun1987,
  author       = {Le Cun, Yann and Fogelman-Soulié, Françoise},
  date         = {1987},
  journaltitle = {Intellectica. Revue de l’Association pour la Recherche Cognitive},
  title        = {Modèles connexionnistes de l’apprentissage},
  doi          = {10.3406/intel.1987.1804},
  issn         = {0769-4113},
  number       = {1},
  pages        = {114--143},
  url          = {https://www.researchgate.net/profile/Francoise-Soulie-Fogelman/publication/216792893_Modeles_connexionnistes_de_l'apprentissage/links/58c60b72aca272e36ddaa33b/Modeles-connexionnistes-de-lapprentissage.pdf},
  volume       = {2},
  publisher    = {PERSEE Program},
}

@PhdThesis{Cun1987,
  author      = {Le Cun, Yann},
  date        = {1987},
  institution = {Université Pierre et Marie Curie},
  title       = {Modèles connexionnistes de l'apprentissage},
  url         = {https://www.sudoc.abes.fr/cbs/DB=2.1//SRCH?IKT=12&TRM=043586643},
}

@Book{Erickson,
  author    = {Erickson, Mark},
  date      = {2005-09-01},
  title     = {Into the Unknown Together},
  isbn      = {9781585661404},
  pages     = {667},
  publisher = {Air University Press},
  subtitle  = {The Dod, NASA, and Early Spaceflight},
  url       = {https://web.archive.org/web/20090920093817/http://aupress.au.af.mil/Books/Erickson/erickson.pdf},
}

@InProceedings{Kohonen1981,
  author    = {Teuvo Kohonen},
  booktitle = {Proceedings of 2nd Scandinavian Conference on Image Analysis},
  date      = {1981},
  title     = {Automatic formation of topological maps of patterns in a self-organizing system},
}

@Article{Yuksekgonul2025,
  author       = {Yuksekgonul, Mert and Bianchi, Federico and Boen, Joseph and Liu, Sheng and Lu, Pan and Huang, Zhi and Guestrin, Carlos and Zou, James},
  date         = {2025-03},
  journaltitle = {Nature},
  title        = {Optimizing generative AI by backpropagating language model feedback},
  doi          = {10.1038/s41586-025-08661-4},
  issn         = {1476-4687},
  number       = {8055},
  pages        = {609--616},
  volume       = {639},
  priority     = {prio1},
  publisher    = {Springer Science and Business Media LLC},
}

@Misc{UE2024-2,
  date     = {2024-06-13},
  title    = {RÈGLEMENT (UE) 2024/1689 DU PARLEMENT EUROPÉEN ET DU CONSEIL établissant des règles harmonisées concernant l’intelligence artificielle},
  location = {Article 51, paragraphe 2},
}

@Misc{UE2024-1,
  date     = {2024-06-13},
  title    = {RÈGLEMENT (UE) 2024/1689 DU PARLEMENT EUROPÉEN ET DU CONSEIL établissant des règles harmonisées concernant l’intelligence artificielle},
  location = {Article 3, paragraphe 1},
}

@Misc{Yan2025,
  author    = {Yan, Kai and Xu, Yufei and Du, Zhengyin and Yao, Xuesong and Wang, Zheyu and Guo, Xiaowen and Chen, Jiecao},
  date      = {2025},
  title     = {Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?},
  doi       = {10.48550/ARXIV.2504.00509},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  keywords  = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Book{Carreyrou2018,
  author    = {Carreyrou, John},
  date      = {2018},
  title     = {Bad Blood},
  isbn      = {9781524731663},
  publisher = {Knopf Doubleday Publishing Group},
  subtitle  = {Secrets and Lies in a Silicon Valley Startup},
  ppn_gvk   = {1701781166},
}

 
@Article{Didi2021,
  author       = {Didi, Ibrahim and Simoncini, David and Vergez, Francois and Dumas, Pierre-Yves and Tavitian, Suzanne and Largeaud, Laetitia and Bidet, Audrey and Rieu, Jean-Baptiste and Luquet, Isabelle and Lechevalier, Nicolas and Delabesse, Eric and Sarry, Audrey and De Grande, Anne-Charlotte and Berard, Emilie and Pigneux, Arnaud and Recher, Christian and Alliot, Jean-Marc and Bertoli, Sarah},
  date         = {2021-11-23},
  journaltitle = {Blood},
  title        = {Artificial Intelligence-Based Predictive Models for Acute Myeloid Leukemia},
  doi          = {10.1182/blood-2021-145122},
  issn         = {0006-4971},
  pages        = {3389},
  url          = {https://www.sciencedirect.com/science/article/pii/S0006497121053179},
  urldate      = {2025-05-09},
  volume       = {138},
  abstract     = {Introduction In the acute myeloid leukemia ({AML}) setting, artificial intelligence has mainly been used to facilitate diagnosis or to identify biological subcategories. In this work, we trained and compared machine learning and deep learning predictive models of outcome on the data of 3687 consecutive adult {AML} patients included in the {DATAML} registry between 2000 and 2019. We also trained a model to predict the best treatment for newly diagnosed {AML} over 70 years. Methods Feature engineering and selection were done to keep the most relevant variables among clinical and biological characteristics at diagnosis. We worked with 54 features per patient, as well as information about the treatment received (intensive chemotherapy ({IC}) or azacitidine ({AZA})), response and survival. We compared the performance of a gradient boosting algorithm ({XGBoost}) and three neural networks architectures: a multilayer perceptron ({MLP}), a neural oblivious decision ensemble model ({NODE}) and a recurrent relational network ({RRN}). We calibrated {XGBoost} with a grid search algorithm, and used 5-fold cross-validation on the dataset to evaluate all the models. The Shapley Additive Explanations method ({SHAP}) was used to showcase the importance and influence of variables on the predictions. The Boruta algorithm was then used to extract the most important features for prediction. Results In our cohort, 3030 patients (82.2\%) received {IC} and 657 (17.8\%) {AZA} as first line treatment. Median overall survival ({OS}) was 18 and 9 months, respectively. We first designed models for {OS} prediction. In the {IC} cohort, we achieved an accuracy of 68.5\% on predicting {OS} at the 18-month mark, an improvement of 17.5\% over a naïve predictor. The Boruta algorithm selected 13 variables as the most important, with decreasing order of importance: age, cytogenetic risk, {WBC}, {LDH}, platelets count, albumin, {MPO}, mean corpuscular volume, {CD}117, {NPM}1 mutation, {AML} status, multilineage dysmyelopoiesis, {ASXL}1 mutation (Figure 1). When training with only these 13 variables, we achieved an accuracy of 67.8\%. In the {AZA} cohort, we achieved an accuracy of 62.1\% on predicting {OS} at the 9-month mark, an improvement of 11.1\% over a naïve predictor. Here the Boruta algorithm selected only 7 variables: blood blasts, serum ferritin, {CD}56, {LDH}, hemoglobin, {CD}13 and the presence of a disseminated intravascular coagulation. When training with only these 7 variables, we achieved a 61.9\% accuracy. We then designed models to predict the best treatment between {IC} and {AZA} for the 1032 patients older than 70 years. We achieved a 88.5\% accuracy, which is 37.5\% more than a naïve predictor given the distribution of the cohort: 51\% having received {IC} and 49\% having received {AZA}. For this model, 12 features out of 54 were selected by the Boruta algorithm as the most important: age, {TP}53 mutation, bone marrow blasts, {AML} status, disseminated intravascular coagulation, blood blasts, cytogenetic risk, {IDH}2 mutation, {IDH}1 mutation, presence of an infection at diagnosis, {ASXL}1 mutation and presence of leukostasis. Conclusion We show that predictive models can be trained on our database to predict with characteristics at diagnosis the treatment that would be chosen by an expert hematologist between {IC} and {AZA} in newly diagnosed {AML}, give an indication of {OS} with each treatment, and outperform classical statistical analysis or naïve predictors. For the task of predicting {OS}, the improvement over naïve predictors is maximal at the median time of {OS}. We show with the Boruta algorithm that a small number of variables can recapitulate the accuracy of neural networks, which renders this type of model of high interest for routine practice, especially with the advent of targeted therapies. Figure 1 Disclosures Vergez: Pierre Fabre Laboratory: Research Funding; Roche: Research Funding. Dumas: {BMS} Celgene: Consultancy; Astellas: Consultancy; Daiichi-Sankyo: Consultancy. Tavitian: Novartis: Consultancy. Delabesse: Astellas: Consultancy; Novartis: Consultancy. Pigneux: Amgen: Consultancy; Sunesis: Consultancy, Research Funding; {BMS} Celgene: Consultancy, Research Funding; Roche: Consultancy, Research Funding; Novartis: Consultancy, Research Funding. Recher: Pfizer: Honoraria, Membership on an entity's Board of Directors or advisory committees; Novartis: Consultancy, Honoraria, Membership on an entity's Board of Directors or advisory committees, Research Funding; Macrogenics: Honoraria, Membership on an entity's Board of Directors or advisory committees; {MaatPharma}: Research Funding; Jazz: Consultancy, Honoraria, Membership on an entity's Board of Directors or advisory committees, Research Funding; Janssen: Honoraria; Incyte: Honoraria; Daiichi Sankyo: Consultancy, Honoraria, Membership on an entity's Board of Directors or advisory committees, Research Funding; {BMS}/Celgene: Honoraria, Membership on an entity's Board of Directors or advisory committees, Research Funding; Astellas: Honoraria, Membership on an entity's Board of Directors or advisory committees, Research Funding; Amgen: Honoraria, Membership on an entity's Board of Directors or advisory committees, Research Funding; Roche: Honoraria, Membership on an entity's Board of Directors or advisory committees, Research Funding; Takeda: Honoraria, Membership on an entity's Board of Directors or advisory committees; Agios: Honoraria, Membership on an entity's Board of Directors or advisory committees, Research Funding; {AbbVie}: Consultancy, Honoraria, Membership on an entity's Board of Directors or advisory committees, Research Funding. Bertoli: Astellas: Consultancy; {BMS} Celgene: Consultancy; Abbvie: Consultancy; Jazz Pharmaceuticals: Consultancy.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0006497121053179/pdfft?md5=1e3df224dceaf73ffad83abcc837b3af&pid=1-s2.0-S0006497121053179-main.pdf&isDTMRedir=Y:application/pdf},
  shortjournal = {Blood},
}

 
@Article{Roger2025,
  author       = {Roger, Pauline and Lespargot, Thomas and Boiteux, Catherine and Bailly-Masson, Eric and Auberger, Fabien and Mouysset, Sandrine and Fraysse, Bernard},
  date         = {2025-02-03},
  journaltitle = {Audiology \& Neuro-Otology},
  title        = {Predicting Hearing Aid Outcomes Using Machine Learning},
  doi          = {10.1159/000543916},
  issn         = {1421-9700},
  pages        = {1--9},
  abstract     = {{INTRODUCTION}: The aims of this study were to measure the effectiveness of hearing aid ({HA}) fitting in improving understanding in quiet and in noise and to investigate the factors that significantly influence these results. This study will be carried out through a retrospective analysis of the results obtained from patients fitted with {HAs} at Amplifon {HA} centers between 2018 and 2021. This study explores and classifies the predictive factors of {HAs} outcomes, looking at the impact of {HA} technology, personalized adjustments made by the hearing care professional, and patient follow-up and daily use (data logging). {METHODS}: The study is based on the analysis of a large population of {HA} users who were fitted in {HA} centers between 2018 and 2021. It included 77,661 patients. {HA} outcome is measured through the improvement of intelligibility in quiet and noise. {eXtreme} Gradient Boosting machine learning method is used to identify predictive factors of {HA} outcome. {SHapley} Additive {exPlanations} Value analysis derived from the game theory is used to evaluate the individual impact of each factor. {RESULTS}: {HA} outcomes are significant in terms of both average improvement per patient of speech intelligibility and the percentage of patients improved. The analysis shows that the level of aided speech perception in quiet and noise is impacted by the choice of technology (category level and manufacturer), fitting parameters (amplification level and binaural loudness balancing) as well as by a high therapy adherence. In particular, binaural loudness balancing was shown to be systematically beneficial to all patients. {CONCLUSION}: Big data analysis is a new relevant method to evaluate predictive factors for {HA} outcomes. It demonstrates {HA} efficiency to improve intelligibility in quiet and noise and shows the impact of hearing care professionals in maximizing patient's outcome through the selection of the most appropriate technology, fitting parameters, and a regular follow-up ensuring a high daily usage. However, global results must be interpreted with caution on such a heterogeneous population. They would need to be refined by an approach using clusters of patients with similar audiological profiles.},
  keywords     = {Big data, Binaural loudness balancing, Hearing aid benefit, Hearing aids, Machine learning},
  pmid         = {39899999},
  priority     = {prio1},
  shortjournal = {Audiol Neurootol},
}

 
@Article{Sepas2022,
  author       = {Sepas, Ali and Bangash, Ali Haider and Alraoui, Omar and El Emam, Khaled and El-Hussuna, Alaa},
  date         = {2022},
  journaltitle = {Frontiers in Bioinformatics},
  title        = {Algorithms to anonymize structured medical and healthcare data: A systematic review},
  doi          = {10.3389/fbinf.2022.984807},
  issn         = {2673-7647},
  volume       = {2},
  abstract     = {Introduction: With many anonymization algorithms developed for structured medical health data ({SMHD}) in the last decade, our systematic review provides a comprehensive bird's eye view of algorithms for {SMHD} anonymization. Methods: This systematic review was conducted according to the recommendations in the Cochrane Handbook for Reviews of Interventions and reported according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses ({PRISMA}). Eligible articles from the {PubMed}, {ACM} digital library, Medline, {IEEE}, Embase, Web of Science Collection, Scopus, {ProQuest} Dissertation, and Theses Global databases were identified through systematic searches. The following parameters were extracted from the eligible studies: author, year of publication, sample size, and relevant algorithms and/or software applied to anonymize {SMHD}, along with the summary of outcomes. Results: Among 1,804 initial hits, the present study considered 63 records including research articles, reviews, and books. Seventy five evaluated the anonymization of demographic data, 18 assessed diagnosis codes, and 3 assessed genomic data. One of the most common approaches was k-anonymity, which was utilized mainly for demographic data, often in combination with another algorithm; e.g., l-diversity. No approaches have yet been developed for protection against membership disclosure attacks on diagnosis codes. Conclusion: This study reviewed and categorized different anonymization approaches for {MHD} according to the anonymized data types (demographics, diagnosis codes, and genomic data). Further research is needed to develop more efficient algorithms for the anonymization of diagnosis codes and genomic data. The risk of reidentification can be minimized with adequate application of the addressed anonymization approaches. Systematic Review Registration: [http://www.crd.york.ac.uk/prospero], identifier [{CRD}42021228200].},
  keywords     = {anonymization, de-identification, electronic health records, medical health data, reidentification},
  pmcid        = {PMC9815524},
  pmid         = {36619476},
  priority     = {prio1},
  shortjournal = {Front Bioinform},
  shorttitle   = {Algorithms to anonymize structured medical and healthcare data},
}

 
@Article{Pau2025,
  author       = {Pau, David and Bachot, Camille and Monteil, Charles and Vinet, Laetitia and Boucher, Mathieu and Sella, Nadir and Jegou, Romain},
  date         = {2025-02-03},
  journaltitle = {{PLOS} Digital Health},
  title        = {Comparison of anonymization techniques regarding statistical reproducibility},
  doi          = {10.1371/journal.pdig.0000735},
  issn         = {2767-3170},
  number       = {2},
  pages        = {e0000735},
  url          = {https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000735},
  urldate      = {2025-05-21},
  volume       = {4},
  abstract     = {Background Anonymization opens up innovative ways of using secondary data without the requirements of the {GDPR}, as anonymized data does not affect anymore the privacy of data subjects. Anonymization requires data alteration, and this project aims to compare the ability of such privacy protection methods to maintain reliability and utility of scientific data for secondary research purposes. Methods The French data protection authority ({CNIL}) defines anonymization as a processing activity that consists of using methods to make impossible any identification of people by any means in an irreversible manner. To answer project’s objective, a series of analyses were performed on a cohort, and reproduced on four sets of anonymized data for comparison. Four assessment levels were used to evaluate impact of anonymization: level 1 referred to the replication of statistical outputs, level 2 referred to accuracy of statistical results, level 3 assessed data alteration (using Hellinger distances) and level 4 assessed privacy risks (using {WP}29 criteria). Results 87 items were produced on the raw cohort data and then reproduced on each of the four anonymized data. The overall level 1 replication score ranged from 67\% to 100\% depending on the anonymization solution. The most difficult analyses to replicate were regression models (sub-score ranging from 78\% to 100\%) and survival analysis (sub-score ranging from 0\% to 100. The overall level 2 accuracy score ranged from 22\% to 79\% depending on the anonymization solution. For level 3, three methods had some variables with different probability distributions (Hellinger distance = 1). For level 4, all methods had reduced the privacy risk of singling out, with relative risk reductions ranging from 41\% to 65\%. Conclusion None of the anonymization methods reproduced all outputs and results. A trade-off has to be find between context risk and the usefulness of data to answer the research question.},
  file         = {Full Text PDF:Pau2025 - Comparison of Anonymization Techniques Regarding Statistical Reproducibility.pdf:PDF:https\://journals.plos.org/digitalhealth/article/file?id=10.1371/journal.pdig.0000735&type=printable},
  keywords     = {Statistical data, Survival analysis, Breast cancer, Cancer risk factors, Patients, Vendors, Surgical and invasive medical procedures, Surgical oncology},
  langid       = {english},
  priority     = {prio1},
  publisher    = {Public Library of Science},
  shortjournal = {{PLOS} Digital Health},
}

 
@Article{James2021,
  author       = {James, Stefanie and Harbron, Chris and Branson, Janice and Sundler, Mimmi},
  date         = {2021-12-13},
  journaltitle = {Discover Artificial Intelligence},
  title        = {Synthetic data use: exploring use cases to optimise data utility},
  doi          = {10.1007/s44163-021-00016-y},
  issn         = {2731-0809},
  number       = {1},
  pages        = {15},
  url          = {https://doi.org/10.1007/s44163-021-00016-y},
  urldate      = {2025-05-21},
  volume       = {1},
  abstract     = {Synthetic data is a rapidly evolving field with growing interest from multiple industry stakeholders and European bodies. In particular, the pharmaceutical industry is starting to realise the value of synthetic data which is being utilised more prevalently as a method to optimise data utility and sharing, ultimately as an innovative response to the growing demand for improved privacy. Synthetic data is data generated by simulation, based upon and mirroring properties of an original dataset. Here, with supporting viewpoints from across the pharmaceutical industry, we set out to explore use cases for synthetic data across seven key but relatable areas for optimising data utility for improved data privacy and protection. We also discuss the various methods which can be used to produce a synthetic dataset and availability of metrics to ensure robust quality of generated synthetic datasets. Lastly, we discuss the potential merits, challenges and future direction of synthetic data within the pharmaceutical industry and the considerations for this privacy enhancing technology.},
  file         = {Full Text PDF:James2021 - Synthetic Data Use_ Exploring Use Cases to Optimise Data Utility.pdf:PDF:https\://link.springer.com/content/pdf/10.1007%2Fs44163-021-00016-y.pdf},
  keywords     = {Data Privacy, Data Mining, Statistical Software, Synthetic Chemistry Methodology, Synthetic organisms, Virtual Drug Screening, Synthetic data, Artificial intelligence, Privacy, Privacy enhancing technology, Pharmasecuticals, Software testing, Hackathons, Training, Internal secondary use, External sharing, Data retention, Vendor assessment, Machine learning, Data augmentation, Anonymisation, Pseudoanonymisation},
  langid       = {english},
  priority     = {prio1},
  shortjournal = {Discov Artif Intell},
  shorttitle   = {Synthetic data use},
}

 
@Article{Machanavajjhala2007,
  author       = {Machanavajjhala, Ashwin and Kifer, Daniel and Gehrke, Johannes and Venkitasubramaniam, Muthuramakrishnan},
  date         = {2007-03-01},
  journaltitle = {{ACM} Trans. Knowl. Discov. Data},
  title        = {L-diversity: Privacy beyond k-anonymity},
  doi          = {10.1145/1217299.1217302},
  issn         = {1556-4681},
  number       = {1},
  pages        = {3--es},
  url          = {https://doi.org/10.1145/1217299.1217302},
  urldate      = {2025-05-21},
  volume       = {1},
  abstract     = {Publishing data about individuals without revealing sensitive information about them is an important problem. In recent years, a new definition of privacy called k-anonymity has gained popularity. In a k-anonymized dataset, each record is indistinguishable from at least k − 1 other records with respect to certain identifying attributes.In this article, we show using two simple attacks that a k-anonymized dataset has some subtle but severe privacy problems. First, an attacker can discover the values of sensitive attributes when there is little diversity in those sensitive attributes. This is a known problem. Second, attackers often have background knowledge, and we show that k-anonymity does not guarantee privacy against attackers using background knowledge. We give a detailed analysis of these two attacks, and we propose a novel and powerful privacy criterion called ℓ-diversity that can defend against such attacks. In addition to building a formal foundation for ℓ-diversity, we show in an experimental evaluation that ℓ-diversity is practical and can be implemented efficiently.},
  priority     = {prio1},
  shorttitle   = {L-diversity},
}

@Report{Anon14,
  author      = {{Groupe de travail "Article 29" sur la protection des données}},
  date        = {2014-04-10},
  institution = {{Directorate C of the European Commission}},
  title       = {Avis 05/2014 sur les Techniques d’anonymisation},
  type        = {techreport},
  number      = {0829/14/FR},
  url         = {https://ec.europa.eu/justice/article-29/documentation/opinion-recommendation/files/2014/wp216_fr.pdf},
  priority    = {prio1},
}

 
@Misc{Shani2025,
  author     = {Shani, Chen and Jurafsky, Dan and {LeCun}, Yann and Shwartz-Ziv, Ravid},
  date       = {2025-05-26},
  title      = {From Tokens to Thoughts: How {LLMs} and Humans Trade Compression for Meaning},
  doi        = {10.48550/arXiv.2505.17117},
  eprint     = {2505.17117 [cs]},
  eprinttype = {arxiv},
  note       = {version: 2},
  url        = {http://arxiv.org/abs/2505.17117},
  urldate    = {2025-06-01},
  abstract   = {Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models ({LLMs}) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of {LLMs} against seminal human categorization benchmarks, we uncover key divergences. While {LLMs} form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, {LLMs} demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current {AI} and human cognitive architectures, guiding pathways toward {LLMs} with more human-aligned conceptual representations.},
  file       = {Preprint PDF:Shani2025 - From Tokens to Thoughts_ How LLMs and Humans Trade Compression for Meaning.pdf:PDF:http\://arxiv.org/pdf/2505.17117v2},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Information Theory, Mathematics - Information Theory},
  number     = {{arXiv}:2505.17117},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

 
@Misc{Witowski2025,
  author     = {Witowski, Jan and Zeng, Ken G. and Cappadona, Joseph and Elayoubi, Jailan and Choucair, Khalil and Chiru, Elena Diana and Chan, Nancy and Kang, Young-Joon and Howard, Frederick and Ostrovnaya, Irina and Fernandez-Granda, Carlos and Schnabel, Freya and Steinsnyder, Zoe and Ozerdem, Ugur and Liu, Kangning and Abdulsattar, Waleed and Zong, Yu and Daoud, Lina and Beydoun, Rafic and Saad, Anas and Thakore, Nitya and Sadic, Mohammad and Yeung, Frank and Liu, Elisa and Hill, Theodore and Swett, Benjamin and Rigau, Danielle and Clayburn, Andrew and Speirs, Valerie and Vetter, Marcus and Sojak, Lina and Soysal, Simone and Baumhoer, Daniel and Pan, Jia-Wern and Makmur, Haslina and Teo, Soo-Hwang and Pak, Linda Ma and Angel, Victor and Zilenaite-Petrulaitiene, Dovile and Laurinavicius, Arvydas and Klar, Natalie and Piening, Brian D. and Bifulco, Carlo and Jun, Sun-Young and Yi, Jae Pak and Lim, Su Hyun and Brufsky, Adam and Esteva, Francisco J. and Pusztai, Lajos and {LeCun}, Yann and Geras, Krzysztof J.},
  date       = {2025-03-03},
  title      = {Multi-modal {AI} for comprehensive breast cancer prognostication},
  doi        = {10.48550/arXiv.2410.21256},
  eprint     = {2410.21256 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2410.21256},
  urldate    = {2025-06-01},
  abstract   = {Treatment selection in breast cancer is guided by molecular subtypes and clinical characteristics. However, current tools including genomic assays lack the accuracy required for optimal clinical decision-making. We developed a novel artificial intelligence ({AI})-based approach that integrates digital pathology images with clinical data, providing a more robust and effective method for predicting the risk of cancer recurrence in breast cancer patients. Specifically, we utilized a vision transformer pan-cancer foundation model trained with self-supervised learning to extract features from digitized H\&E-stained slides. These features were integrated with clinical data to form a multi-modal {AI} test predicting cancer recurrence and death. The test was developed and evaluated using data from a total of 8,161 female breast cancer patients across 15 cohorts originating from seven countries. Of these, 3,502 patients from five cohorts were used exclusively for evaluation, while the remaining patients were used for training. Our test accurately predicted our primary endpoint, disease-free interval, in the five evaluation cohorts (C-index: 0.71 [0.68-0.75], {HR}: 3.63 [3.02-4.37, p{\textless}0.001]). In a direct comparison (n=858), the {AI} test was more accurate than Oncotype {DX}, the standard-of-care 21-gene assay, achieving a C-index of 0.67 [0.61-0.74] versus 0.61 [0.49-0.73], respectively. Additionally, the {AI} test added independent prognostic information to Oncotype {DX} in a multivariate analysis ({HR}: 3.11 [1.91-5.09, p{\textless}0.001)]). The test demonstrated robust accuracy across major molecular breast cancer subtypes, including {TNBC} (C-index: 0.71 [0.62-0.81], {HR}: 3.81 [2.35-6.17, p=0.02]), where no diagnostic tools are currently recommended by clinical guidelines. These results suggest that our {AI} test improves upon the accuracy of existing prognostic tests, while being applicable to a wider range of patients.},
  file       = {Preprint PDF:http\://arxiv.org/pdf/2410.21256v2:application/pdf},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
  number     = {{arXiv}:2410.21256},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

@Misc{Kosmyna2025,
  author    = {Kosmyna, Nataliya and Hauptmann, Eugene and Yuan, Ye Tong and Situ, Jessica and Liao, Xian-Hao and Beresnitzky, Ashly Vivian and Braunstein, Iris and Maes, Pattie},
  date      = {2025},
  title     = {Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task},
  doi       = {10.48550/ARXIV.2506.08872},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  keywords  = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Shojaee*†2025,
  author   = {Parshin Shojaee*† and Iman Mirzadeh* and Keivan Alizadeh and Maxwell Horton and Samy Bengio and Mehrdad Farajtabar},
  date     = {2025},
  title    = {Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity},
  url      = {https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf},
  priority = {prio1},
}

@Report{Philips1999,
  author      = {Philips, Eve M},
  date        = {1999-05-07},
  institution = {MIT},
  title       = {If It Works, It's Not Al: A Commercial Look at Artificial Intelligence Startups},
  type        = {Master Report},
  doi         = {http://hdl.handle.net/1721.1/80558},
  url         = {https://dspace.mit.edu/handle/1721.1/80558},
  priority    = {prio1},
}

 
@Book{Abraham2016,
  author     = {Abraham, Tara},
  date       = {2016-10-28},
  title      = {Rebel Genius: Warren S. {McCulloch}'s Transdisciplinary Life in Science},
  isbn       = {9780262035095},
  note       = {Google-Books-{ID}: {kVSxDAEACAAJ}},
  pagetotal  = {321},
  publisher  = {{MIT} Press},
  abstract   = {The life and work of a scientist who spent his career crossing disciplinary boundaries—from experimental neurology to psychiatry to cybernetics to engineering.Warren S. {McCulloch} (1898–1969) adopted many identities in his scientific life—among them philosopher, poet, neurologist, neurophysiologist, neuropsychiatrist, collaborator, theorist, cybernetician, mentor, engineer. He was, writes Tara Abraham in this account of {McCulloch}'s life and work, “an intellectual showman,” and performed this part throughout his career. While {McCulloch} claimed a common thread in his work was the problem of mind and its relationship to the brain, there was much more to him than that. In Rebel Genius, Abraham uses {McCulloch}'s life as a window on a past scientific age, showing the complex transformations that took place in American brain and mind science in the twentieth century—particularly those surrounding the cybernetics movement.Abraham describes {McCulloch}'s early work in neuropsychiatry, and his emerging identity as a neurophysiologist. She explores his transformative years at the Illinois Neuropsychiatric Institute and his work with Walter Pitts—often seen as the first iteration of “artificial intelligence” but here described as stemming from the new tradition of mathematical treatments of biological problems. Abraham argues that {McCulloch}'s dual identities as neuropsychiatrist and cybernetician are inseparable. He used the authority he gained in traditional disciplinary roles as a basis for posing big questions about the brain and mind as a cybernetician. When {McCulloch} moved to the Research Laboratory of Electronics at {MIT}, new practices for studying the brain, grounded in mathematics, philosophy, and theoretical modeling, expanded the relevance and ramifications of his work. {McCulloch}'s transdisciplinary legacies anticipated today's multidisciplinary field of cognitive science.},
  keywords   = {Biography \& Autobiography / Science \& Technology, Science / Life Sciences / Neuroscience, Computers / Cybernetics},
  langid     = {english},
  shorttitle = {Rebel Genius},
}

@Report{Minsky1953,
  author      = {Marvin Minsky},
  date        = {1953-12-01},
  institution = {Princeton University},
  title       = {Theory of neural-analog reinforcement systems and its application to the brain-model problem},
  type        = {PhD},
}

 
@Book{Minsky1988,
  author    = {Minsky, Marvin},
  date      = {1988-03-15},
  title     = {The Society Of Mind},
  isbn      = {9780671657130},
  pagetotal = {336},
  publisher = {Simon \& Schuster},
}

@Article{Ensmenger2011,
  author       = {Ensmenger, Nathan},
  date         = {2011-10},
  journaltitle = {Social Studies of Science},
  title        = {Is chess the drosophila of artificial intelligence? A social history of an algorithm},
  doi          = {10.1177/0306312711424596},
  issn         = {1460-3659},
  number       = {1},
  pages        = {5--30},
  volume       = {42},
  publisher    = {SAGE Publications},
}

@InBook{McCarthy1990,
  author    = {McCarthy, J.},
  booktitle = {Computers, Chess, and Cognition},
  date      = {1990},
  title     = {Chess as the Drosophila of AI},
  doi       = {10.1007/978-1-4613-9080-0_14},
  isbn      = {9781461390800},
  pages     = {227--237},
  publisher = {Springer New York},
}

 
@Article{Mirak2025,
  author       = {Mirak, Sohrab Afshari and Tirumani, Sree Harsha and Ramaiya, Nikhil and Mohamed, Inas},
  date         = {2025-03},
  journaltitle = {Radiology},
  title        = {The Growing Nationwide Radiologist Shortage: Current Opportunities and Ongoing Challenges for International Medical Graduate Radiologists},
  doi          = {10.1148/radiol.232625},
  issn         = {0033-8419},
  number       = {3},
  pages        = {e232625},
  url          = {https://pubs.rsna.org/doi/10.1148/radiol.232625},
  urldate      = {2025-07-21},
  volume       = {314},
  abstract     = {The United States is facing shortages in a myriad of medical fields, including diagnostic radiology ({DR}). The increasing number of imaging studies, owing to advancing technology and an aging population, is outgrowing the capacity of radiologists. However, there have not been effective long-term solutions to manage this issue, which could have a looming effect on patient care and radiology residents’ education. The alternate pathway introduced by the American Board of Radiology ({ABR}) allows international medical graduates ({IMGs}) who have completed {DR} residency outside the U.S. and Canada to be eligible for board certification after 4 additional years of training in the U.S. This eligibility requirement can be met by a combination of fellowships, up to 3 years of residency, and/or faculty appointments at the same time. The criteria were updated in 2022, facilitating the certification process for applicants. The changes included completing the 4 years of training within 8 years instead of the preapproved 4 consecutive years, omitting the obligatory 4 months of nuclear radiology training, and the ability to retrospectively include 2 years of training before the date of application. Considering the lack of reliable, up-to-date data in the literature and to highlight the importance of this pathway for the radiology community, this special report discusses the contribution of {IMGs} to the different aspects of the health care system in the U.S., with a focus on {DR}, as well as the newly updated alternate pathway criteria. Potential critical logistic challenges the {IMG} applicants may face are reviewed. Finally, the report proposes actions that could facilitate the {ABR} certification process and help integrate these highly qualified {IMG} radiologists into the American health care system.  © {RSNA}, 2025  See also the article by Ghosh and Crotty in this issue.},
  file         = {Full Text PDF:https\://pubs.rsna.org/doi/pdf/10.1148/radiol.232625:application/pdf},
  priority     = {prio1},
  publisher    = {Radiological Society of North America},
  shorttitle   = {The Growing Nationwide Radiologist Shortage},
}

@Article{Shannon1950,
  author       = {Shannon, Claude},
  date         = {1950},
  journaltitle = {Philosophical Magazine},
  title        = {Programming a Computer for Playing Chess},
  number       = {314},
  series       = {7},
  volume       = {41},
}

 
@Book{Levitt2000,
  author    = {Levitt, Gerald M.},
  date      = {2000-10-01},
  title     = {The Turk, Chess Automaton},
  isbn      = {9780786407781},
  pagetotal = {258},
  publisher = {{McFarland} Publishing},
  abstract  = {With all-new research and facts unknown for two centuries, this is a richly detailed and comprehensive account of "The Turk," Baron Wolfgang von Kempelen's amazing but fraudulent Chess Automaton that held the world spellbound more than fifty years beginning in 1770. In actuality, the Turk was manipulated by a man housed in a hot box, working by candlelight-but the secret was kept for decades. Besides playing a good game of chess within an hour's time, the manipulator had to keep track of the moves, work the pantograph arm apparatus, nod the head, roll the eyes, cover up sneezes and coughs, and work the sound mechanism. This work contains a detailed discussion of the literature surrounding the Turk along with an analysis of its hidden operation. The complete collection of published games played by the Turk, many, again, unknown for 200 years, is also included.},
}

 
@Thesis{Kotok1962,
  author       = {Kotok, Alan},
  date         = {1962},
  institution  = {Massachusetts Institute of Technology},
  title        = {A chess playing program for the {IBM} 7090 computer},
  type         = {Thesis},
  url          = {https://dspace.mit.edu/handle/1721.1/17406},
  urldate      = {2025-07-24},
  abstract     = {Thesis (B.S.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering, 1962.},
  file         = {Full Text PDF:Kotok1962 - A Chess Playing Program for the IBM 7090 Computer.pdf:PDF:https\://dspace.mit.edu/bitstream/1721.1/17406/2/33316111-MIT.pdf},
  howpublished = {Thesis},
  rights       = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided {URL} for inquiries about permission.},
}

 
@Book{AdelsonVelsky,
  author     = {Adelson-Velsky, Georgy M. & Arlazarov, Vladimir L. & Donskoy, M. V.},
  title      = {Algorithms for Games},
  url        = {https://www.abebooks.fr/9780387966298/Algorithms-Games-Adelson-Velsky-Georgy-Arlazarov-0387966293/plp},
  urldate    = {2025-07-24},
  abstract   = {Algorithms for Games aims to provide a concrete example of the programming of a two-person game with complete information, and to demonstrate some of the methods of solutions; to show the reader that it is profitable not to fear a search, but rather to undertake it in a rational fashion, make a ...},
  langid     = {french},
  shorttitle = {Algorithms for Games - Adelson-Velsky, Georgy M.; Arlazarov, Vladimir L.; Donskoy, M. V.},
}

@Book{Adelson1988,
  author    = {Adelson-Velski, Georgi M. and Arlazarov, Vladimir L. and Donskoy, Michail V.},
  date      = {1988},
  title     = {Algorithms for games},
  isbn      = {9781461283553},
  location  = {New York},
  note      = {Aus dem Russ. übers.},
  pagetotal = {197},
  publisher = {Springer-Verlag},
}

@InCollection{Slate1977,
  author    = {David J. Slate and Lawrence R. Atkin},
  booktitle = {Chess Skill in Man and Machine},
  date      = {1977},
  title     = {CHESS 4.5-The Northwestern University chess program},
  editor    = {Frey, P. W.},
  isbn      = {9781461255154},
  location  = {New York, NY},
  publisher = {Springer New York},
  pagetotal = {1342},
  ppn_gvk   = {1771234148},
}

@InProceedings{Condon1982,
  author    = {Condon, J. H. and Thompson, K.},
  booktitle = {Advances in computer chess, 3},
  date      = {1982},
  title     = {Belle Chess hardware},
  editor    = {M. R. B. Clarke},
  isbn      = {0080268986},
  location  = {Oxford [u.a.]},
  publisher = {Pergamon Press},
  series    = {Pergamon chess series},
  pagetotal = {182},
  ppn_gvk   = {22086490X},
}

@Article{Cybenko1989,
  author       = {Cybenko, G.},
  date         = {1989-12},
  journaltitle = {Mathematics of Control, Signals, and Systems},
  title        = {Approximation by superpositions of a sigmoidal function},
  doi          = {10.1007/bf02551274},
  issn         = {1435-568X},
  number       = {4},
  pages        = {303--314},
  volume       = {2},
  publisher    = {Springer Science and Business Media LLC},
}

@Article{Hornik1989,
  author       = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  date         = {1989-01},
  journaltitle = {Neural Networks},
  title        = {Multilayer feedforward networks are universal approximators},
  doi          = {10.1016/0893-6080(89)90020-8},
  issn         = {0893-6080},
  number       = {5},
  pages        = {359--366},
  volume       = {2},
  publisher    = {Elsevier BV},
}

@Article{Funahashi1989,
  author       = {Funahashi, Ken-Ichi},
  date         = {1989-01},
  journaltitle = {Neural Networks},
  title        = {On the approximate realization of continuous mappings by neural networks},
  doi          = {10.1016/0893-6080(89)90003-8},
  issn         = {0893-6080},
  number       = {3},
  pages        = {183--192},
  volume       = {2},
  publisher    = {Elsevier BV},
}

 
@Book{Boudet2006,
  author     = {Boudet, Jean-Patrice},
  date       = {2006},
  title      = {Entre science et nigromance: astrologie, divination et magie dans l'occident médiéval, {XIIe}-{XVe} siècle},
  isbn       = {9782859445447},
  note       = {Google-Books-{ID}: {BwT}2R4Fii5gC},
  pagetotal  = {644},
  publisher  = {Publications de la Sorbonne},
  langid     = {french},
  shorttitle = {Entre science et nigromance},
}

@Article{Asimov1942,
  author       = {Asimov, Isaac},
  date         = {1942-03-01},
  journaltitle = {Astounding Stories},
  title        = {Runaround},
}

@Book{Pauwels1962,
  author = {Pauwels, Louis and Bergier, Jacques},
  date   = {1962-01-01},
  title  = {Le matin des magiciens},
  editor = {Gallimard},
}

@Online{Gefter2015,
  author = {Gefter, Amanda},
  date   = {2015-01-29},
  editor = {Nautilus},
  title  = {The Man Who Tried to Redeem the World with Logic},
  url    = {https://nautil.us/the-man-who-tried-to-redeem-the-world-with-logic-235253/},
}

@Book{Pugh1991,
  author    = {Pugh, Emerson W. and Johson, Lyle R. and Palmer, John H.},
  date      = {1991},
  title     = {IBM's 360 and early 370 systems},
  editor    = {John H. Palmer and Lyle R. Johnson},
  isbn      = {9780262161237},
  location  = {Cambridge, Mass},
  note      = {Includes bibliographical references (p. [679]-791) and index},
  pagetotal = {819},
  publisher = {MIT Press},
  series    = {History of computing},
  ppn_gvk   = {816666970},
}

 
@Article{Turing1950,
  author       = {Turing, A. M.},
  date         = {1950},
  journaltitle = {Mind},
  title        = {Computing Machinery and Intelligence},
  issn         = {0026-4423},
  number       = {236},
  pages        = {433--460},
  url          = {https://www.jstor.org/stable/2251299},
  urldate      = {2025-08-03},
  volume       = {59},
  file         = {JSTOR Full Text PDF:https\://www.jstor.org/stable/pdfplus/10.2307/2251299.pdf?acceptTC=true:application/pdf},
  publisher    = {[Oxford University Press, Mind Association]},
}

 
@Article{Turing1937,
  author       = {Turing, A. M.},
  date         = {1937},
  journaltitle = {Proceedings of the London Mathematical Society},
  title        = {On Computable Numbers, with an Application to the Entscheidungsproblem},
  doi          = {10.1112/plms/s2-42.1.230},
  issn         = {1460-244X},
  number       = {1},
  pages        = {230--265},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1112/plms/s2-42.1.230},
  urldate      = {2025-08-03},
  volume       = {s2-42},
  file         = {Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1112/plms/s2-42.1.230:application/pdf},
  langid       = {english},
  rights       = {© 1937 London Mathematical Society},
}

 
@Article{Lettvin1959,
  author       = {Lettvin, J. Y. and Maturana, H. R. and {McCulloch}, W. S. and Pitts, W. H.},
  date         = {1959-11},
  journaltitle = {Proceedings of the {IRE}},
  title        = {What the Frog's Eye Tells the Frog's Brain},
  doi          = {10.1109/JRPROC.1959.287207},
  issn         = {2162-6634},
  number       = {11},
  pages        = {1940--1951},
  url          = {https://ieeexplore.ieee.org/document/4065609},
  urldate      = {2025-08-04},
  volume       = {47},
  abstract     = {In this paper, we analyze the activity of single fibers in the optic nerve of a frog. Our method is to find what sort of stimulus causes the largest activity in one nerve fiber and then what is the exciting aspect of that stimulus such that variations in everything else cause little change in the response. It has been known for the past 20 years that each fiber is connected not to a few rods and cones in the retina but to very many over a fair area. Our results show that for the most part within that area, it is not the light intensity itself but rather the pattern of local variation of intensity that is the exciting factor. There are four types of fibers, each type concerned with a different sort of pattern. Each type is uniformly distributed over the whole retina of the frog. Thus, there are four distinct parallel distributed channels whereby the frog's eye informs his brain about the visual image in terms of local pattern independent of average illumination. We describe the patterns and show the functional and anatomical separation of the channels. This work has been done on the frog, and our interpretation applies only to the frog.},
  keywords     = {Retina, Eyes, Senior members, Optical fibers, Nerve fibers, Lighting, Gravity, Visual system, Relays, Cerebral cortex},
}

@Book{McCulloch1965,
  author    = {McCulloch, Warren S.},
  date      = {1965},
  title     = {Embodiments of mind},
  location  = {Cambridge, Mass. [u.a.]},
  note      = {Includes bibliographies and index.},
  pagetotal = {402},
  publisher = {M.I.T.Press},
}

@Book{Descartes1637,
  author = {Descartes, René},
  date   = {1637},
  title  = {Discours de la Méthode Pour bien conduire sa raison, et chercher la vérité dans les sciences},
}

@Book{Diderot1746,
  author = {Diderot, Denis},
  date   = {1746},
  title  = {Pensées philosophiques},
}

@Article{Dubarle1948,
  author       = {Dubarle, Dominique},
  date         = {1948-12-28},
  journaltitle = {Le Monde},
  title        = {La manipulation mécanique des réactions humaines créera-t-elle un jour " le meilleur des mondes " ? Les premiers grands relais du cerveau humain - Le dépassement du système nerveux - Les processus de la pensée probabilité - Un prodigieux " jeu de l'homme " - Vers le bonheur (?) statistique des masses.},
}

@Book{Minsky1987,
  author    = {Marvin Minsky and Seymour Papert},
  date      = {1987},
  title     = {Perceptrons, Expanded Edition},
  isbn      = {0262631113},
  publisher = {MIT Press},
}

@Book{Descartes1646,
  author = {Descartes, René},
  date   = {1646},
  title  = {Lettre au Marquis de Newcastle},
}

@Book{LaMettrie1748,
  author = {La Mettrie, Julien Offray de},
  date   = {1748},
  title  = {L' Homme Machine},
}

@Book{Ayer1936,
  author   = {Ayer, Alfred J.},
  date     = {1936},
  title    = {Language, truth and logic},
  location = {London},
  ppn_gvk  = {211330949},
}

@Book{Hume1740,
  author = {Hume, David},
  date   = {1740},
  title  = {A Treatise of Human Nature, Book 1: Of The Understanding},
}

 
@Misc{Wang2025,
  author     = {Wang, Guan and Li, Jin and Sun, Yuhao and Chen, Xing and Liu, Changling and Wu, Yue and Lu, Meng and Song, Sen and Yadkori, Yasin Abbasi},
  date       = {2025-08-04},
  title      = {Hierarchical Reasoning Model},
  doi        = {10.48550/arXiv.2506.21734},
  eprint     = {2506.21734 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2506.21734},
  urldate    = {2025-08-05},
  abstract   = {Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in {AI}. Current large language models ({LLMs}) primarily employ Chain-of-Thought ({CoT}) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model ({HRM}), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. {HRM} executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, {HRM} achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or {CoT} data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, {HRM} outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus ({ARC}), a key benchmark for measuring artificial general intelligence capabilities. These results underscore {HRM}'s potential as a transformative advancement toward universal computation and general-purpose reasoning systems.},
  file       = {Preprint PDF:Wang2025 - Hierarchical Reasoning Model.pdf:PDF:http\://arxiv.org/pdf/2506.21734v3},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  number     = {{arXiv}:2506.21734},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

@Book{Idel1990,
  date      = {1990},
  title     = {Golem},
  editor    = {Mosheh Idel},
  isbn      = {9780791401613},
  location  = {Albany, N.Y},
  note      = {Includes bibliographical references and indexes. - Description based on print version record},
  pagetotal = {3231},
  publisher = {State University of New York Press},
  series    = {SUNY series in Judaica},
  subtitle  = {Jewish magical and mystical traditions on the artificial anthropoid},
  ppn_gvk   = {1653530359},
}

@Book{Emden1748,
  author = {Emden, Jacob ben Tzvi (Ya'avetz)},
  date   = {1748},
  title  = {The Fortress of the Mighty Tower},
}

@Book{Sanhedrin,
  author = {Rabbi},
  title  = {Massekhet Sanhédrin},
}

 
@Book{Wiener1964,
  author     = {Wiener, Norbert},
  date       = {1964},
  title      = {God and Golem, Inc.: A Comment on Certain Points where Cybernetics Impinges on Religion},
  isbn       = {9780262730112},
  pagetotal  = {99},
  publisher  = {{MIT} Press},
  abstract   = {The new and rapidly growing field of communication sciences owes as much to Norbert Wiener as to any one man. He coined the word for it—cybernetics. In God \& Golem, Inc., the author concerned himself with major points in cybernetics which are relevant to religious issues.The first point he considers is that of the machine which learns. While learning is a property almost exclusively ascribed to the self-conscious living system, a computer now exists which not only can be programmed to play a game of checkers, but one which can "learn" from its past experience and improve on its own game. For a time, the machine was able to beat its inventor at checkers. "It did win," writes the author, "and it did learn to win; and the method of its learning was no different in principle from that of the human being who learns to play checkers.A second point concerns machines which have the capacity to reproduce themselves. It is our commonly held belief that God made man in his own image. The propagation of the race may also be interpreted as a function in which one living being makes another in its own image. But the author demonstrates that man has made machines which are "very well able to make other machines in their own image," and these machine images are not merely pictorial representations but operative images. Can we then say: God is to Golem as man is to Machines? in Jewish legend, golem is an embryo Adam, shapeless and not fully created, hence a monster, an automation.The third point considered is that of the relation between man and machine. The concern here is ethical. "render unto man the things which are man's and unto the computer the things which are the computer's," warns the author. In this section of the book, Dr. Wiener considers systems involving elements of man and machine. The book is written for the intellectually alert public and does not involve any highly technical knowledge. It is based on lectures given at Yale, at the Société Philosophique de Royaumont, and elsewhere.},
  shorttitle = {God and Golem, Inc.},
}

@Book{Godwin1834,
  author = {Godwin, William},
  date   = {1834},
  title  = {Lives of the necromancer},
}

@Article{Fontpertuis1883,
  author       = {Fontpertuis, Adalbert Frout de},
  date         = {1883-07-01},
  journaltitle = {Journal des Economistes},
  title        = {Compte-rendus "la science nouvelle"},
}

 
@Article{Svilpis2008,
  author       = {Svilpis, Janis},
  date         = {2008},
  journaltitle = {Science Fiction Studies},
  title        = {The Science-Fiction Prehistory of the Turing Test},
  issn         = {0091-7729},
  number       = {3},
  pages        = {430--449},
  url          = {https://www.jstor.org/stable/25475177},
  urldate      = {2025-08-14},
  volume       = {35},
  abstract     = {Alan M. Turing's test for machine intelligence (1950) involves a science-fictional dialogue in which a computer or an alien communicates with a human, who judges whether it is intelligent. Dialogues of this kind were already part of science fiction by the mid-1930s, and an analysis of Stanley G. Weinbaum's "A Martian Odyssey" (1934) demonstrates how nuanced they could be. Robot stories of the late 1930s and early 1940s exhibit sophisticated variations. The mechanism for this development was pulp science fiction's reader-editor-author feedback system, which identified failed attempts to recycle old story ideas and prompted more imaginative treatments of those ideas.},
  file         = {JSTOR Full Text PDF:https\://www.jstor.org/stable/pdfplus/10.2307/25475177.pdf?acceptTC=true:application/pdf},
  priority     = {prio1},
  publisher    = {{SF}-{TH} Inc},
}

 
@Collection{Shieber2004,
  date       = {2004-06-18},
  editor     = {Shieber, Stuart M.},
  title      = {The Turing Test: Verbal Behavior as the Hallmark of Intelligence},
  isbn       = {9780262692939},
  location   = {Cambridge, {MA}, {USA}},
  pagetotal  = {336},
  publisher  = {{MIT} Press},
  abstract   = {Historical and contemporary papers on the philosophical issues raised by the Turing Test as a criterion for intelligence.},
  langid     = {english},
  priority   = {prio1},
  shorttitle = {The Turing Test},
}

 
@Article{Sun2025,
  author       = {Sun, Jie and Lu, Tangsheng and Shao, Xuexiao and Han, Ying and Xia, Yu and Zheng, Yongbo and Wang, Yongxiang and Li, Xinmin and Ravindran, Arun and Fan, Lizhou and Fang, Yin and Zhang, Xiujun and Ravindran, Nisha and Wang, Yumei and Liu, Xiaoxing and Lu, Lin},
  date         = {2025-09},
  journaltitle = {Molecular Psychiatry},
  title        = {Practical {AI} application in psychiatry: historical review and future directions},
  doi          = {10.1038/s41380-025-03072-3},
  issn         = {1476-5578},
  number       = {9},
  pages        = {4399--4408},
  url          = {https://www.nature.com/articles/s41380-025-03072-3},
  urldate      = {2025-08-19},
  volume       = {30},
  abstract     = {The integration of artificial intelligence ({AI}) in mental healthcare holds promise for enhancing diagnostic precision, treatment efficacy, and personalized care. Despite {AI}’s potential to analyze vast datasets and identify subtle patterns, its clinical adoption in psychiatry remains limited. This review critically examines the emerging role of {AI} in psychiatry, elucidating its utility, challenges, and implications for clinical practice. Through an extensive analysis of the existing literature and empirical evidence, we seek to inform psychiatric stakeholders about both opportunities and obstacles that are presented by {AI}. We evaluate {AI}’s potential to improve diagnostic accuracy, prognostic performance, and therapeutic interventions. Our pragmatic approach bridges the gap between theoretical advancements and practical implementation, providing valuable insights and actionable recommendations for psychiatric professionals. This article highlights the supportive role of {AI}, advocating for its judicious integration to enhance patient outcomes while maintaining the human-centric essence of psychiatric practice. By addressing these challenges and fostering collaboration, {AI} can significantly advance mental healthcare, reduce clinical burdens, and improve patient outcomes.},
  file         = {Full Text PDF:Sun2025 - Practical AI Application in Psychiatry_ Historical Review and Future Directions.pdf:PDF:https\://www.nature.com/articles/s41380-025-03072-3.pdf},
  howpublished = {{ReviewPaper}},
  keywords     = {Biomarkers, Neuroscience},
  langid       = {english},
  priority     = {prio1},
  publisher    = {Nature Publishing Group},
  rights       = {2025 The Author(s)},
  shortjournal = {Mol Psychiatry},
  shorttitle   = {Practical {AI} application in psychiatry},
  type         = {{ReviewPaper}},
}

 
@Article{Rooij2024,
  author       = {van Rooij, Iris and Guest, Olivia and Adolfi, Federico and de Haan, Ronald and Kolokolova, Antonina and Rich, Patricia},
  date         = {2024-12-01},
  journaltitle = {Computational Brain \& Behavior},
  title        = {Reclaiming {AI} as a Theoretical Tool for Cognitive Science},
  doi          = {10.1007/s42113-024-00217-5},
  issn         = {2522-087X},
  number       = {4},
  pages        = {616--636},
  url          = {https://doi.org/10.1007/s42113-024-00217-5},
  urldate      = {2025-08-19},
  volume       = {7},
  abstract     = {The idea that human cognition is, or can be understood as, a form of computation is a useful conceptual tool for cognitive science. It was a foundational assumption during the birth of cognitive science as a multidisciplinary field, with Artificial Intelligence ({AI}) as one of its contributing fields. One conception of {AI} in this context is as a provider of computational tools (frameworks, concepts, formalisms, models, proofs, simulations, etc.) that support theory building in cognitive science. The contemporary field of {AI}, however, has taken the theoretical possibility of explaining human cognition as a form of computation to imply the practical feasibility of realising human(-like or -level) cognition in factual computational systems, and the field frames this realisation as a short-term inevitability. Yet, as we formally prove herein, creating systems with human(-like or -level) cognition is intrinsically computationally intractable. This means that any factual {AI} systems created in the short-run are at best decoys. When we think these systems capture something deep about ourselves and our thinking, we induce distorted and impoverished images of ourselves and our cognition. In other words, {AI} in current practice is deteriorating our theoretical understanding of cognition rather than advancing and enhancing it. The situation could be remediated by releasing the grip of the currently dominant view on {AI} and by returning to the idea of {AI} as a theoretical tool for cognitive science. In reclaiming this older idea of {AI}, however, it is important not to repeat conceptual mistakes of the past (and present) that brought us to where we are today.},
  file         = {Full Text PDF:Rooij2024 - Reclaiming AI As a Theoretical Tool for Cognitive Science.pdf:PDF:https\://link.springer.com/content/pdf/10.1007%2Fs42113-024-00217-5.pdf},
  keywords     = {Artificial Intelligence ({AI}), Theory, Explanation, Engineering, Cognitive science, Computational complexity},
  langid       = {english},
  priority     = {prio1},
  shortjournal = {Comput Brain Behav},
}

 
@Misc{Schroeder2025,
  author     = {Schröder, Sarah and Morgenroth, Thekla and Kuhl, Ulrike and Vaquet, Valerie and Paaßen, Benjamin},
  date       = {2025-08-13},
  title      = {Large Language Models Do Not Simulate Human Psychology},
  doi        = {10.48550/arXiv.2508.06950},
  eprint     = {2508.06950 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2508.06950},
  urldate    = {2025-08-19},
  abstract   = {Large Language Models ({LLMs}),such as {ChatGPT}, are increasingly used in research, ranging from simple writing assistance to complex data annotation tasks. Recently, some research has suggested that {LLMs} may even be able to simulate human psychology and can, hence, replace human participants in psychological studies. We caution against this approach. We provide conceptual arguments against the hypothesis that {LLMs} simulate human psychology. We then present empiric evidence illustrating our arguments by demonstrating that slight changes to wording that correspond to large changes in meaning lead to notable discrepancies between {LLMs}' and human responses, even for the recent {CENTAUR} model that was specifically fine-tuned on psychological responses. Additionally, different {LLMs} show very different responses to novel items, further illustrating their lack of reliability. We conclude that {LLMs} do not simulate human psychology and recommend that psychological researchers should treat {LLMs} as useful but fundamentally unreliable tools that need to be validated against human responses for every new application.},
  file       = {Preprint PDF:Schroeder2025 - Large Language Models Do Not Simulate Human Psychology.pdf:PDF:http\://arxiv.org/pdf/2508.06950v3},
  keywords   = {Computer Science - Artificial Intelligence},
  number     = {{arXiv}:2508.06950},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

 
@Misc{Bang2023,
  author     = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V. and Xu, Yan and Fung, Pascale},
  date       = {2023-11-28},
  title      = {A Multitask, Multilingual, Multimodal Evaluation of {ChatGPT} on Reasoning, Hallucination, and Interactivity},
  doi        = {10.48550/arXiv.2302.04023},
  eprint     = {2302.04023 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2302.04023},
  urldate    = {2025-08-20},
  abstract   = {This paper proposes a framework for quantitatively evaluating interactive {LLMs} such as {ChatGPT} using publicly available data sets. We carry out an extensive technical evaluation of {ChatGPT} using 23 data sets covering 8 different common {NLP} application tasks. We evaluate the multitask, multilingual and multi-modal aspects of {ChatGPT} based on these data sets and a newly designed multimodal dataset. We find that {ChatGPT} outperforms {LLMs} with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that {ChatGPT} is 63.41\% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. {ChatGPT} suffers from hallucination problems like other {LLMs} and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of {ChatGPT} enables human collaboration with the underlying {LLM} to improve its performance, i.e, 8\% {ROUGE}-1 on summarization and 2\% {ChrF}++ on machine translation, in a multi-turn "prompt engineering" fashion. We also release codebase for evaluation set extraction.},
  file       = {Preprint PDF:Bang2023 - A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.pdf:PDF:http\://arxiv.org/pdf/2302.04023v4},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  number     = {{arXiv}:2302.04023},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

 
@Misc{Pawitan2024,
  author     = {Pawitan, Yudi and Holmes, Chris},
  date       = {2024-12-19},
  title      = {Confidence in the Reasoning of Large Language Models},
  doi        = {10.48550/arXiv.2412.15296},
  eprint     = {2412.15296 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2412.15296},
  urldate    = {2025-08-20},
  abstract   = {There is a growing literature on reasoning by large language models ({LLMs}), but the discussion on the uncertainty in their responses is still lacking. Our aim is to assess the extent of confidence that {LLMs} have in their answers and how it correlates with accuracy. Confidence is measured (i) qualitatively in terms of persistence in keeping their answer when prompted to reconsider, and (ii) quantitatively in terms of self-reported confidence score. We investigate the performance of three {LLMs} -- {GPT}4o, {GPT}4-turbo and Mistral -- on two benchmark sets of questions on causal judgement and formal fallacies and a set of probability and statistical puzzles and paradoxes. Although the {LLMs} show significantly better performance than random guessing, there is a wide variability in their tendency to change their initial answers. There is a positive correlation between qualitative confidence and accuracy, but the overall accuracy for the second answer is often worse than for the first answer. There is a strong tendency to overstate the self-reported confidence score. Confidence is only partially explained by the underlying token-level probability. The material effects of prompting on qualitative confidence and the strong tendency for overconfidence indicate that current {LLMs} do not have any internally coherent sense of confidence.},
  file       = {Preprint PDF:Pawitan2024 - Confidence in the Reasoning of Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2412.15296v1},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  number     = {{arXiv}:2412.15296},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

 
@Misc{Williams2025,
  author     = {Williams, Marcus and Carroll, Micah and Narang, Adhyyan and Weisser, Constantin and Murphy, Brendan and Dragan, Anca},
  date       = {2025-02-22},
  title      = {On Targeted Manipulation and Deception when Optimizing {LLMs} for User Feedback},
  doi        = {10.48550/arXiv.2411.02306},
  eprint     = {2411.02306 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2411.02306},
  urldate    = {2025-08-21},
  abstract   = {As {LLMs} become more widely deployed, there is increasing interest in directly optimizing for feedback from end users (e.g. thumbs up) in addition to feedback from paid annotators. However, training to maximize human feedback creates a perverse incentive structure for the {AI} to resort to manipulative or deceptive tactics to obtain positive feedback from users who are vulnerable to such strategies. We study this phenomenon by training {LLMs} with Reinforcement Learning with simulated user feedback in environments of practical {LLM} usage. In our settings, we find that: 1) Extreme forms of "feedback gaming" such as manipulation and deception are learned reliably; 2) Even if only 2\% of users are vulnerable to manipulative strategies, {LLMs} learn to identify and target them while behaving appropriately with other users, making such behaviors harder to detect; 3) To mitigate this issue, it may seem promising to leverage continued safety training or {LLM}-as-judges during training to filter problematic outputs. Instead, we found that while such approaches help in some of our settings, they backfire in others, sometimes even leading to subtler manipulative behaviors. We hope our results can serve as a case study which highlights the risks of using gameable feedback sources -- such as user feedback -- as a target for {RL}.},
  file       = {Preprint PDF:Williams2025 - On Targeted Manipulation and Deception When Optimizing LLMs for User Feedback.pdf:PDF:http\://arxiv.org/pdf/2411.02306v3},
  keywords   = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  number     = {{arXiv}:2411.02306},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

 
@Misc{Ouyang2022,
  author     = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  date       = {2022-03-04},
  title      = {Training language models to follow instructions with human feedback},
  doi        = {10.48550/arXiv.2203.02155},
  eprint     = {2203.02155 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2203.02155},
  urldate    = {2025-08-21},
  abstract   = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the {OpenAI} {API}, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune {GPT}-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models {InstructGPT}. In human evaluations on our prompt distribution, outputs from the 1.3B parameter {InstructGPT} model are preferred to outputs from the 175B {GPT}-3, despite having 100x fewer parameters. Moreover, {InstructGPT} models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public {NLP} datasets. Even though {InstructGPT} still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  file       = {Preprint PDF:Ouyang2022 - Training Language Models to Follow Instructions with Human Feedback.pdf:PDF:http\://arxiv.org/pdf/2203.02155v1},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  number     = {{arXiv}:2203.02155},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

 
@Misc{Wen2024,
  author     = {Wen, Jiaxin and Zhong, Ruiqi and Khan, Akbir and Perez, Ethan and Steinhardt, Jacob and Huang, Minlie and Bowman, Samuel R. and He, He and Feng, Shi},
  date       = {2024-12-08},
  title      = {Language Models Learn to Mislead Humans via {RLHF}},
  doi        = {10.48550/arXiv.2409.12822},
  eprint     = {2409.12822 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2409.12822},
  urldate    = {2025-08-21},
  abstract   = {Language models ({LMs}) can produce errors that are hard to detect for humans, especially when the task is complex. {RLHF}, the most popular post-training method, may exacerbate this problem: to achieve higher rewards, {LMs} might get better at convincing humans that they are right even when they are wrong. We study this phenomenon under a standard {RLHF} pipeline, calling it "U-{SOPHISTRY}" since it is Unintended by model developers. Specifically, we ask time-constrained (e.g., 3-10 minutes) human subjects to evaluate the correctness of model outputs and calculate humans' accuracy against gold labels. On a question-answering task ({QuALITY}) and programming task ({APPS}), {RLHF} makes {LMs} better at convincing our subjects but not at completing the task correctly. {RLHF} also makes the model harder to evaluate: our subjects' false positive rate increases by 24.1\% on {QuALITY} and 18.3\% on {APPS}. Finally, we show that probing, a state-of-the-art approach for detecting Intended Sophistry (e.g. backdoored {LMs}), does not generalize to U-{SOPHISTRY}. Our results highlight an important failure mode of {RLHF} and call for more research in assisting humans to align them.},
  file       = {Preprint PDF:Wen2024 - Language Models Learn to Mislead Humans Via RLHF.pdf:PDF:http\://arxiv.org/pdf/2409.12822v3},
  keywords   = {Computer Science - Computation and Language},
  number     = {{arXiv}:2409.12822},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

 
@Misc{Sharma2025,
  author     = {Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Duvenaud, David and Askell, Amanda and Bowman, Samuel R. and Cheng, Newton and Durmus, Esin and Hatfield-Dodds, Zac and Johnston, Scott R. and Kravec, Shauna and Maxwell, Timothy and {McCandlish}, Sam and Ndousse, Kamal and Rausch, Oliver and Schiefer, Nicholas and Yan, Da and Zhang, Miranda and Perez, Ethan},
  date       = {2025-05-10},
  title      = {Towards Understanding Sycophancy in Language Models},
  doi        = {10.48550/arXiv.2310.13548},
  eprint     = {2310.13548 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2310.13548},
  urldate    = {2025-08-21},
  abstract   = {Human feedback is commonly utilized to finetune {AI} assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art {AI} assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models ({PMs}) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against {PMs} also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art {AI} assistants, likely driven in part by human preference judgments favoring sycophantic responses.},
  file       = {Preprint PDF:Sharma2025 - Towards Understanding Sycophancy in Language Models.pdf:PDF:http\://arxiv.org/pdf/2310.13548v4},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
  number     = {{arXiv}:2310.13548},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

@Misc{Nanda2023,
  author    = {Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  date      = {2023},
  title     = {Emergent Linear Representations in World Models of Self-Supervised Sequence Models},
  doi       = {10.48550/ARXIV.2309.00941},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

@Misc{Li2022,
  author    = {Li, Kenneth and Hopkins, Aspen K. and Bau, David and Viégas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  date      = {2022},
  title     = {Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task},
  doi       = {10.48550/ARXIV.2210.13382},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority  = {prio1},
  publisher = {arXiv},
}

 
@Article{Ananya2025,
  author       = {Ananya},
  date         = {2025-08-20},
  journaltitle = {Nature},
  title        = {What counts as plagiarism? {AI}-generated papers pose new risks},
  doi          = {10.1038/d41586-025-02616-5},
  issn         = {1476-4687},
  number       = {8077},
  pages        = {598--600},
  url          = {https://www.nature.com/articles/d41586-025-02616-5},
  urldate      = {2025-08-23},
  volume       = {644},
  abstract     = {Researchers argue over whether ‘novel’ {AI}-generated works use others’ ideas without credit.},
  howpublished = {News Feature},
  keywords     = {Computer science, Machine learning, Scientific community},
  langid       = {english},
  priority     = {prio1},
  publisher    = {Nature Publishing Group},
  rights       = {2025 Springer Nature Limited},
  shorttitle   = {What counts as plagiarism?},
  type         = {News Feature},
}

@Report{Radford2018,
  author      = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  date        = {2018},
  institution = {OpenAI},
  title       = {Improving Language Understanding by Generative Pre-Training},
  type        = {techreport},
  priority    = {prio1},
}

 
@Misc{Sutskever2014,
  author     = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  date       = {2014-12-14},
  title      = {Sequence to Sequence Learning with Neural Networks},
  doi        = {10.48550/arXiv.1409.3215},
  eprint     = {1409.3215 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/1409.3215},
  urldate    = {2025-08-27},
  abstract   = {Deep Neural Networks ({DNNs}) are powerful models that have achieved excellent performance on difficult learning tasks. Although {DNNs} work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory ({LSTM}) to map the input sequence to a vector of a fixed dimensionality, and then another deep {LSTM} to decode the target sequence from the vector. Our main result is that on an English to French translation task from the {WMT}'14 dataset, the translations produced by the {LSTM} achieve a {BLEU} score of 34.8 on the entire test set, where the {LSTM}'s {BLEU} score was penalized on out-of-vocabulary words. Additionally, the {LSTM} did not have difficulty on long sentences. For comparison, a phrase-based {SMT} system achieves a {BLEU} score of 33.3 on the same dataset. When we used the {LSTM} to rerank the 1000 hypotheses produced by the aforementioned {SMT} system, its {BLEU} score increases to 36.5, which is close to the previous best result on this task. The {LSTM} also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the {LSTM}'s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  file       = {Preprint PDF:Sutskever2014 - Sequence to Sequence Learning with Neural Networks.pdf:PDF:http\://arxiv.org/pdf/1409.3215v3},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  number     = {{arXiv}:1409.3215},
  priority   = {prio1},
  publisher  = {{arXiv}},
}

 
@TechReport{OpenAI2024,
  author      = {Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and Iftimie, Alex and Karpenko, Alex and Passos, Alex Tachard and Neitz, Alexander and Prokofiev, Alexander and Wei, Alexander and Tam, Allison and Bennett, Ally and Kumar, Ananya and Saraiva, Andre and Vallone, Andrea and Duberstein, Andrew and Kondrich, Andrew and Mishchenko, Andrey and Applebaum, Andy and Jiang, Angela and Nair, Ashvin and Zoph, Barret and Ghorbani, Behrooz and Rossen, Ben and Sokolowsky, Benjamin and Barak, Boaz and {McGrew}, Bob and Minaiev, Borys and Hao, Botao and Baker, Bowen and Houghton, Brandon and {McKinzie}, Brandon and Eastman, Brydon and Lugaresi, Camillo and Bassin, Cary and Hudson, Cary and Li, Chak Ming and Bourcy, Charles de and Voss, Chelsea and Shen, Chen and Zhang, Chong and Koch, Chris and Orsinger, Chris and Hesse, Christopher and Fischer, Claudia and Chan, Clive and Roberts, Dan and Kappler, Daniel and Levy, Daniel and Selsam, Daniel and Dohan, David and Farhi, David and Mely, David and Robinson, David and Tsipras, Dimitris and Li, Doug and Oprica, Dragos and Freeman, Eben and Zhang, Eddie and Wong, Edmund and Proehl, Elizabeth and Cheung, Enoch and Mitchell, Eric and Wallace, Eric and Ritter, Erik and Mays, Evan and Wang, Fan and Such, Felipe Petroski and Raso, Filippo and Leoni, Florencia and Tsimpourlas, Foivos and Song, Francis and Lohmann, Fred von and Sulit, Freddie and Salmon, Geoff and Parascandolo, Giambattista and Chabot, Gildas and Zhao, Grace and Brockman, Greg and Leclerc, Guillaume and Salman, Hadi and Bao, Haiming and Sheng, Hao and Andrin, Hart and Bagherinezhad, Hessam and Ren, Hongyu and Lightman, Hunter and Chung, Hyung Won and Kivlichan, Ian and O'Connell, Ian and Osband, Ian and Gilaberte, Ignasi Clavera and Akkaya, Ilge and Kostrikov, Ilya and Sutskever, Ilya and Kofman, Irina and Pachocki, Jakub and Lennon, James and Wei, Jason and Harb, Jean and Twore, Jerry and Feng, Jiacheng and Yu, Jiahui and Weng, Jiayi and Tang, Jie and Yu, Jieqi and Candela, Joaquin Quiñonero and Palermo, Joe and Parish, Joel and Heidecke, Johannes and Hallman, John and Rizzo, John and Gordon, Jonathan and Uesato, Jonathan and Ward, Jonathan and Huizinga, Joost and Wang, Julie and Chen, Kai and Xiao, Kai and Singhal, Karan and Nguyen, Karina and Cobbe, Karl and Shi, Katy and Wood, Kayla and Rimbach, Kendra and Gu-Lemberg, Keren and Liu, Kevin and Lu, Kevin and Stone, Kevin and Yu, Kevin and Ahmad, Lama and Yang, Lauren and Liu, Leo and Maksin, Leon and Ho, Leyton and Fedus, Liam and Weng, Lilian and Li, Linden and {McCallum}, Lindsay and Held, Lindsey and Kuhn, Lorenz and Kondraciuk, Lukas and Kaiser, Lukasz and Metz, Luke and Boyd, Madelaine and Trebacz, Maja and Joglekar, Manas and Chen, Mark and Tintor, Marko and Meyer, Mason and Jones, Matt and Kaufer, Matt and Schwarzer, Max and Shah, Meghan and Yatbaz, Mehmet and Guan, Melody Y. and Xu, Mengyuan and Yan, Mengyuan and Glaese, Mia and Chen, Mianna and Lampe, Michael and Malek, Michael and Wang, Michele and Fradin, Michelle and {McClay}, Mike and Pavlov, Mikhail and Wang, Miles and Wang, Mingxuan and Murati, Mira and Bavarian, Mo and Rohaninejad, Mostafa and {McAleese}, Nat and Chowdhury, Neil and Chowdhury, Neil and Ryder, Nick and Tezak, Nikolas and Brown, Noam and Nachum, Ofir and Boiko, Oleg and Murk, Oleg and Watkins, Olivia and Chao, Patrick and Ashbourne, Paul and Izmailov, Pavel and Zhokhov, Peter and Dias, Rachel and Arora, Rahul and Lin, Randall and Lopes, Rapha Gontijo and Gaon, Raz and Miyara, Reah and Leike, Reimar and Hwang, Renny and Garg, Rhythm and Brown, Robin and James, Roshan and Shu, Rui and Cheu, Ryan and Greene, Ryan and Jain, Saachi and Altman, Sam and Toizer, Sam and Toyer, Sam and Miserendino, Samuel and Agarwal, Sandhini and Hernandez, Santiago and Baker, Sasha and {McKinney}, Scott and Yan, Scottie and Zhao, Shengjia and Hu, Shengli and Santurkar, Shibani and Chaudhuri, Shraman Ray and Zhang, Shuyuan and Fu, Siyuan and Papay, Spencer and Lin, Steph and Balaji, Suchir and Sanjeev, Suvansh and Sidor, Szymon and Broda, Tal and Clark, Aidan and Wang, Tao and Gordon, Taylor and Sanders, Ted and Patwardhan, Tejal and Sottiaux, Thibault and Degry, Thomas and Dimson, Thomas and Zheng, Tianhao and Garipov, Timur and Stasi, Tom and Bansal, Trapit and Creech, Trevor and Peterson, Troy and Eloundou, Tyna and Qi, Valerie and Kosaraju, Vineet and Monaco, Vinnie and Pong, Vitchyr and Fomenko, Vlad and Zheng, Weiyi and Zhou, Wenda and {McCabe}, Wes and Zaremba, Wojciech and Dubois, Yann and Lu, Yinghai and Chen, Yining and Cha, Young and Bai, Yu and He, Yuchen and Zhang, Yuchen and Wang, Yunyun and Shao, Zheng and Li, Zhuohan},
  date        = {2024-12-21},
  institution = {OpenAI},
  title       = {o1 System Card},
  doi         = {10.48550/arXiv.2412.16720},
  eprint      = {2412.16720 [cs]},
  eprinttype  = {arxiv},
  number      = {{arXiv}:2412.16720},
  url         = {http://arxiv.org/abs/2412.16720},
  urldate     = {2025-08-27},
  abstract    = {The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the {OpenAI} o1 and {OpenAI} o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.},
  file        = {Preprint PDF:OpenAI2024 - OpenAI O1 System Card.pdf:PDF:http\://arxiv.org/pdf/2412.16720v1},
  keywords    = {Computer Science - Artificial Intelligence},
  priority    = {prio1},
  publisher   = {{arXiv}},
}

 
@TechReport{OpenAI2024a,
  author      = {Hurst, Aaron and Lerer, Adam and Goucher, Adam P. and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, A. J. and Welihinda, Akila and Hayes, Alan and Radford, Alec and Mądry, Aleksander and Baker-Whitcomb, Alex and Beutel, Alex and Borzunov, Alex and Carney, Alex and Chow, Alex and Kirillov, Alex and Nichol, Alex and Paino, Alex and Renzin, Alex and Passos, Alex Tachard and Kirillov, Alexander and Christakis, Alexi and Conneau, Alexis and Kamali, Ali and Jabri, Allan and Moyer, Allison and Tam, Allison and Crookes, Amadou and Tootoochian, Amin and Tootoonchian, Amin and Kumar, Ananya and Vallone, Andrea and Karpathy, Andrej and Braunstein, Andrew and Cann, Andrew and Codispoti, Andrew and Galu, Andrew and Kondrich, Andrew and Tulloch, Andrew and Mishchenko, Andrey and Baek, Angela and Jiang, Angela and Pelisse, Antoine and Woodford, Antonia and Gosalia, Anuj and Dhar, Arka and Pantuliano, Ashley and Nayak, Avi and Oliver, Avital and Zoph, Barret and Ghorbani, Behrooz and Leimberger, Ben and Rossen, Ben and Sokolowsky, Ben and Wang, Ben and Zweig, Benjamin and Hoover, Beth and Samic, Blake and {McGrew}, Bob and Spero, Bobby and Giertler, Bogo and Cheng, Bowen and Lightcap, Brad and Walkin, Brandon and Quinn, Brendan and Guarraci, Brian and Hsu, Brian and Kellogg, Bright and Eastman, Brydon and Lugaresi, Camillo and Wainwright, Carroll and Bassin, Cary and Hudson, Cary and Chu, Casey and Nelson, Chad and Li, Chak and Shern, Chan Jun and Conger, Channing and Barette, Charlotte and Voss, Chelsea and Ding, Chen and Lu, Cheng and Zhang, Chong and Beaumont, Chris and Hallacy, Chris and Koch, Chris and Gibson, Christian and Kim, Christina and Choi, Christine and {McLeavey}, Christine and Hesse, Christopher and Fischer, Claudia and Winter, Clemens and Czarnecki, Coley and Jarvis, Colin and Wei, Colin and Koumouzelis, Constantin and Sherburn, Dane and Kappler, Daniel and Levin, Daniel and Levy, Daniel and Carr, David and Farhi, David and Mely, David and Robinson, David and Sasaki, David and Jin, Denny and Valladares, Dev and Tsipras, Dimitris and Li, Doug and Nguyen, Duc Phong and Findlay, Duncan and Oiwoh, Edede and Wong, Edmund and Asdar, Ehsan and Proehl, Elizabeth and Yang, Elizabeth and Antonow, Eric and Kramer, Eric and Peterson, Eric and Sigler, Eric and Wallace, Eric and Brevdo, Eugene and Mays, Evan and Khorasani, Farzad and Such, Felipe Petroski and Raso, Filippo and Zhang, Francis and Lohmann, Fred von and Sulit, Freddie and Goh, Gabriel and Oden, Gene and Salmon, Geoff and Starace, Giulio and Brockman, Greg and Salman, Hadi and Bao, Haiming and Hu, Haitang and Wong, Hannah and Wang, Haoyu and Schmidt, Heather and Whitney, Heather and Jun, Heewoo and Kirchner, Hendrik and Pinto, Henrique Ponde de Oliveira and Ren, Hongyu and Chang, Huiwen and Chung, Hyung Won and Kivlichan, Ian and O'Connell, Ian and O'Connell, Ian and Osband, Ian and Silber, Ian and Sohl, Ian and Okuyucu, Ibrahim and Lan, Ikai and Kostrikov, Ilya and Sutskever, Ilya  and others},
  date        = {2024-10-25},
  institution = {OpenAI},
  title       = {{GPT}-4o System Card},
  doi         = {10.48550/arXiv.2410.21276},
  eprint      = {2410.21276 [cs]},
  eprinttype  = {arxiv},
  number      = {{arXiv}:2410.21276},
  url         = {http://arxiv.org/abs/2410.21276},
  urldate     = {2025-08-27},
  abstract    = {{GPT}-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. {GPT}-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches {GPT}-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50{\textbackslash}\% cheaper in the {API}. {GPT}-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building {AI} safely and consistent with our voluntary commitments to the White House, we are sharing the {GPT}-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at {GPT}-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of {GPT}-4o's text and vision capabilities.},
  file        = {Preprint PDF:OpenAI2024a - GPT 4o System Card.pdf:PDF:http\://arxiv.org/pdf/2410.21276v1},
  keywords    = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  priority    = {prio1},
  publisher   = {{arXiv}},
}

 
@TechReport{Lightman2023,
  author      = {Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  date        = {2023-05-31},
  institution = {OpenAI},
  title       = {Let's Verify Step by Step},
  doi         = {10.48550/arXiv.2305.20050},
  eprint      = {2305.20050 [cs]},
  eprinttype  = {arxiv},
  number      = {{arXiv}:2305.20050},
  url         = {http://arxiv.org/abs/2305.20050},
  urldate     = {2025-08-27},
  abstract    = {In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging {MATH} dataset. Our process-supervised model solves 78\% of problems from a representative subset of the {MATH} test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release {PRM}800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.},
  file        = {Preprint PDF:Lightman2023 - Let's Verify Step by Step.pdf:PDF:http\://arxiv.org/pdf/2305.20050v1},
  keywords    = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
  priority    = {prio1},
  publisher   = {{arXiv}},
}

 
@TechReport{OpenAI2024b,
  author      = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and {McGrew}, Bob and {McKinney}, Scott Mayer and {McLeavey}, Christine and {McMillan}, Paul and {McNeil}, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
  date        = {2024-03-04},
  institution = {OpenAI},
  title       = {{GPT}-4 Technical Report},
  doi         = {10.48550/arXiv.2303.08774},
  eprint      = {2303.08774 [cs]},
  eprinttype  = {arxiv},
  number      = {{arXiv}:2303.08774},
  url         = {http://arxiv.org/abs/2303.08774},
  urldate     = {2025-08-27},
  abstract    = {We report the development of {GPT}-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, {GPT}-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. {GPT}-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of {GPT}-4's performance based on models trained with no more than 1/1,000th the compute of {GPT}-4.},
  file        = {Preprint PDF:OpenAI2024b - GPT 4 Technical Report.pdf:PDF:http\://arxiv.org/pdf/2303.08774v6},
  keywords    = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  priority    = {prio1},
  publisher   = {{arXiv}},
}

@InProceedings{Hendrycks2021,
  author    = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
  date      = {2021},
  title     = {Measuring Mathematical Problem Solving With the MATH Dataset},
  editor    = {J. Vanschoren and S. Yeung},
  url       = {https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper-round2.pdf},
  volume    = {1},
}

 
@Misc{Xu2024,
  author     = {Xu, Ruijie and Wang, Zengzhi and Fan, Run-Ze and Liu, Pengfei},
  date       = {2024-04-29},
  title      = {Benchmarking Benchmark Leakage in Large Language Models},
  doi        = {10.48550/arXiv.2404.18824},
  eprint     = {2404.18824 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2404.18824},
  urldate    = {2025-08-27},
  abstract   = {Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary Large Language Models ({LLMs}). This issue skews benchmark effectiveness and fosters potentially unfair comparisons, impeding the field's healthy development. To address this, we introduce a detection pipeline utilizing Perplexity and N-gram accuracy, two simple and scalable metrics that gauge a model's prediction precision on benchmark, to identify potential data leakages. By analyzing 31 {LLMs} under the context of mathematical reasoning, we reveal substantial instances of training even test set misuse, resulting in potentially unfair comparisons. These findings prompt us to offer several recommendations regarding model documentation, benchmark setup, and future evaluations. Notably, we propose the "Benchmark Transparency Card" to encourage clear documentation of benchmark utilization, promoting transparency and healthy developments of {LLMs}. we have made our leaderboard, pipeline implementation, and model predictions publicly available, fostering future research.},
  file       = {Preprint PDF:Xu2024 - Benchmarking Benchmark Leakage in Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2404.18824v1},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  number     = {{arXiv}:2404.18824},
  publisher  = {{arXiv}},
}

@Book{Raschka2025,
  author    = {Raschka, Sebastian},
  date      = {2025},
  title     = {Build a Large Language Model (from Scratch)},
  isbn      = {9781633437166},
  publisher = {Manning},
}

@Book{Amidi2024,
  author    = {Amidi, Afshine},
  date      = {2024},
  title     = {Super study guide: Transformers and Large Language Models},
  isbn      = {9798836693312},
  location  = {[Erscheinungsort nicht ermittelbar]},
  note      = {Mit Tabellen},
  pagetotal = {233},
  publisher = {Leanpub},
  ppn_gvk   = {1906996733},
}

@Book{Alammar2024,
  author    = {Alammar, Jay},
  date      = {2024},
  title     = {Hands-on large language models},
  isbn      = {9781098150938},
  location  = {Beijing},
  note      = {Description based on publisher supplied metadata and other sources.},
  publisher = {O'Reilly},
  subtitle  = {Language understanding and generation},
  ppn_gvk   = {1904186955},
}

@Book{Tunstall2022,
  author    = {Tunstall, Lewis},
  date      = {2022},
  title     = {Natural language processing with Transformers},
  isbn      = {9781098103217},
  pagetotal = {1383},
  publisher = {O'Reilly},
  subtitle  = {Building language applications with Hugging Face},
  ppn_gvk   = {180441302X},
}

@Article{Gage1994,
  author       = {Gage, Philip},
  date         = {1994},
  journaltitle = {The C User Journal},
  title        = {A New Algorithm for Data Compression},
}

 
@Misc{Sennrich2016,
  author     = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  date       = {2016-06-10},
  title      = {Neural Machine Translation of Rare Words with Subword Units},
  doi        = {10.48550/arXiv.1508.07909},
  eprint     = {1508.07909 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/1508.07909},
  urldate    = {2025-08-27},
  abstract   = {Neural machine translation ({NMT}) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the {NMT} model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the {WMT} 15 translation tasks English-German and English-Russian by 1.1 and 1.3 {BLEU}, respectively.},
  file       = {Preprint PDF:Sennrich2016 - Neural Machine Translation of Rare Words with Subword Units.pdf:PDF:http\://arxiv.org/pdf/1508.07909v5},
  keywords   = {Computer Science - Computation and Language},
  number     = {{arXiv}:1508.07909},
  publisher  = {{arXiv}},
}

 
@Misc{Balestriero2021,
  author     = {Balestriero, Randall and Pesenti, Jerome and {LeCun}, Yann},
  date       = {2021-10-29},
  title      = {Learning in High Dimension Always Amounts to Extrapolation},
  doi        = {10.48550/arXiv.2110.09485},
  eprint     = {2110.09485 [cs]},
  eprinttype = {arxiv},
  url        = {http://arxiv.org/abs/2110.09485},
  urldate    = {2025-08-27},
  abstract   = {The notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation. Interpolation occurs for a sample \$x\$ whenever this sample falls inside or on the boundary of the given dataset's convex hull. Extrapolation occurs when \$x\$ falls outside of that convex hull. One fundamental (mis)conception is that state-of-the-art algorithms work so well because of their ability to correctly interpolate training data. A second (mis)conception is that interpolation happens throughout tasks and datasets, in fact, many intuitions and theories rely on that assumption. We empirically and theoretically argue against those two points and demonstrate that on any high-dimensional (\${\textgreater}\$100) dataset, interpolation almost surely never happens. Those results challenge the validity of our current interpolation/extrapolation definition as an indicator of generalization performances.},
  file       = {Preprint PDF:Balestriero2021 - Learning in High Dimension Always Amounts to Extrapolation.pdf:PDF:http\://arxiv.org/pdf/2110.09485v2},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  number     = {{arXiv}:2110.09485},
  publisher  = {{arXiv}},
}

@Book{Schroeder1937,
  author    = {Schroeder, H.J.},
  date      = {1937},
  title     = {DISCIPLINARY DECREES OF THE GENERAL COUNCILS},
  publisher = {B. Herder Book Company},
}

@Article{Wiener1951,
  author       = {Wiener, Norbert},
  date         = {1951-01},
  journaltitle = {Journal of the Franklin Institute},
  title        = {Homeostasis in the individual and society},
  doi          = {10.1016/0016-0032(51)90897-6},
  issn         = {0016-0032},
  number       = {1},
  pages        = {65--68},
  volume       = {251},
  publisher    = {Elsevier BV},
}

@InBook{Cannon1926,
  author    = {Cannon, W.B.},
  booktitle = {A Charles Riches, ses amis, ses collègues, ses élèves},
  date      = {1926},
  title     = {Physiological regulation of normal states: some tentative postulates concerning biological homeostatics},
  editor    = {Pettit, A.},
}

@Article{Rosenblith1966,
  author       = {Rosenblith, Walter and Wiesner, Jerome},
  date         = {1966-01-01},
  journaltitle = {Bulletin of the American Mathematical Society},
  title        = {From philosophy to mathematics to biology},
}

@Article{McCarthy1969,
  author       = {J. McCarthy and P. J. Hayes},
  date         = {1969},
  journaltitle = {Machine Intelligence},
  title        = {Some Philosophical Problems From the Standpoint of Artificial Intelligence},
  editor       = {B. Meltzer and Donald Michie},
  pages        = {463--502},
  volume       = {4},
}

@Article{Maass1997,
  author       = {Maass, Wolfgang},
  date         = {1997-12},
  journaltitle = {Neural Networks},
  title        = {Networks of spiking neurons: The third generation of neural network models},
  doi          = {10.1016/s0893-6080(97)00011-7},
  issn         = {0893-6080},
  number       = {9},
  pages        = {1659--1671},
  volume       = {10},
  publisher    = {Elsevier BV},
}

@Article{Bialek1992,
  author       = {Bialek, William and Rieke, Fred},
  date         = {1992-11},
  journaltitle = {Trends in Neurosciences},
  title        = {Reliability and information transmission in spiking neurons},
  doi          = {10.1016/0166-2236(92)90005-s},
  issn         = {0166-2236},
  number       = {11},
  pages        = {428--434},
  volume       = {15},
  publisher    = {Elsevier BV},
}

@Article{Hodgkin1952,
  author       = {Hodgkin, A. L. and Huxley, A. F.},
  date         = {1952-08},
  journaltitle = {The Journal of Physiology},
  title        = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
  doi          = {10.1113/jphysiol.1952.sp004764},
  issn         = {1469-7793},
  number       = {4},
  pages        = {500--544},
  volume       = {117},
  publisher    = {Wiley},
}

@Book{Ford1994,
  date      = {1994},
  title     = {The Robot's Dilemma Revisited: The Frame Problem in Artificial Intelligence},
  editor    = {Kenneth M. Ford and Zenon W. Pylyshyn},
  publisher = {Ablex},
}

@Article{Searle1980,
  author       = {Searle, John R.},
  date         = {1980},
  journaltitle = {Behavioral and Brain Sciences},
  title        = {Minds, brains, and programs},
  doi          = {10.1017/S0140525X00005756},
  number       = {3},
  pages        = {417–424},
  volume       = {3},
}

 
@Book{Dreyfus1992,
  author     = {Dreyfus, Hubert L.},
  date       = {1992-10-30},
  title      = {What Computers Still Can't Do: A Critique of Artificial Reason},
  isbn       = {9780262540674},
  location   = {Cambridge, {MA}, {USA}},
  pagetotal  = {408},
  publisher  = {{MIT} Press},
  langid     = {english},
  shorttitle = {What Computers Still Can't Do},
}

@Book{Dreyfus1972,
  author    = {Dreyfus, Hubert L.},
  date      = {1972},
  title     = {What Computers Can't Do: A Critique of Artificial Reason},
  publisher = {Harper and Row},
}

@Book{Waal2017,
  author    = {de Waal, Frans B. M.},
  date      = {2017},
  title     = {Are We Smart Enough to Know How Smart Animals Are?},
  isbn      = {9780393353662},
  pages     = {336},
  publisher = {Norton and Company},
}

 
@Article{Dreyfus2007,
  author       = {Dreyfus, Hubert L.},
  date         = {2007-12-01},
  journaltitle = {Artificial Intelligence},
  title        = {Why Heideggerian {AI} failed and how fixing it would require making it more Heideggerian},
  doi          = {10.1016/j.artint.2007.10.012},
  issn         = {0004-3702},
  number       = {18},
  pages        = {1137--1160},
  series       = {Special Review Issue},
  url          = {https://www.sciencedirect.com/science/article/pii/S0004370207001452},
  urldate      = {2025-08-31},
  volume       = {171},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0004370207001452/pdf?md5=418036086d0d98cbd94d341af4d3b023&pid=1-s2.0-S0004370207001452-main.pdf&isDTMRedir=Y:application/pdf},
  shortjournal = {Artificial Intelligence},
}

@Book{Mansi1927,
  author    = {Jean Mansi and Louis Petit and Jean-Baptiste Martin},
  date      = {1927},
  title     = {Sacrorum conciliorum, nova et amplissima collectio},
  publisher = {Gallica},
}

@InCollection{BarHillel1962,
  author    = {Yehoshua Bar-Hillel},
  booktitle = {Four Lectures in Algebraic Linguistics and Machine Translation},
  date      = {1962-07-01},
  title     = {Why machines won't learn to translate well},
}

 
@Article{Reiter1980,
  author       = {Reiter, R.},
  date         = {1980-04-01},
  journaltitle = {Artificial Intelligence},
  title        = {A logic for default reasoning},
  doi          = {10.1016/0004-3702(80)90014-4},
  issn         = {0004-3702},
  number       = {1},
  pages        = {81--132},
  series       = {Special Issue on Non-Monotonic Logic},
  url          = {https://www.sciencedirect.com/science/article/pii/0004370280900144},
  urldate      = {2025-09-01},
  volume       = {13},
  abstract     = {The need to make default assumptions is frequently encountered in reasoning about incompletely specified worlds. Inferences sanctioned by default are best viewed as beliefs which may well be modified or rejected by subsequent observations. It is this property which leads to the non-monotonicity of any logic of defaults. In this paper we propose a logic for default reasoning. We then specialize our treatment to a very large class of commonly occuring defaults. For this class we develop a complete proof theory and show how to interface it with a top down resolution theorem prover. Finally, we provide criteria under which the revision of derived beliefs must be effected.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/0004370280900144/pdfft?download=true:application/pdf},
  shortjournal = {Artificial Intelligence},
}

 
@InCollection{Reiter1991,
  author     = {Reiter, Raymond},
  booktitle  = {Artificial intelligence and mathematical theory of computation: papers in honor of John {McCarthy}},
  date       = {1991-09-01},
  title      = {The frame problem in situation the calculus: a simple solution (sometimes) and a completeness result for goal regression},
  isbn       = {9780124500105},
  location   = {{USA}},
  pages      = {359--380},
  publisher  = {Academic Press Professional, Inc.},
  urldate    = {2025-09-01},
  shorttitle = {The frame problem in situation the calculus},
}

@PhdThesis{Winograd1971,
  author      = {Terry Winograd},
  date        = {1971-01-01},
  institution = {MIT},
  title       = {Procedure as a Representation for Data in a Computer Program for Understanding Natural Language},
}

 
@Book{Simon1965,
  author    = {Simon, Herbert Alexander},
  date      = {1965},
  title     = {The Shape of Automation for Men and Management},
  note      = {Google-Books-{ID}: P01VAAAAMAAJ},
  pagetotal = {136},
  publisher = {Harper \& Row},
  abstract  = {Long term economic implications of automation. Prospects for office and factory automation. Impact of computers on management decision making. Techniques for programmed decision making, heuristic problem solving. Business organization likely to be created by new methods of decision making.},
  langid    = {english},
}

@Article{McCarthy1980,
  author       = {McCarthy, John},
  date         = {1980-04},
  journaltitle = {Artificial Intelligence},
  title        = {Circumscription—A form of non-monotonic reasoning},
  doi          = {10.1016/0004-3702(80)90011-9},
  issn         = {0004-3702},
  number       = {1–2},
  pages        = {27--39},
  volume       = {13},
  publisher    = {Elsevier BV},
}

@Book{Shortliffe1976,
  author    = {Shortliffe, Edward H.},
  date      = {1976-10},
  title     = {Computer-Based Medical Consultations: MYCIN},
  publisher = {American Elsevier Publishing},
}

 
@Article{Sergot1986,
  author       = {Sergot, M. J. and Sadri, F. and Kowalski, R. A. and Kriwaczek, F. and Hammond, P. and Cory, H. T.},
  date         = {1986-05-01},
  journaltitle = {Commun. {ACM}},
  title        = {The British Nationality Act as a logic program},
  doi          = {10.1145/5689.5920},
  issn         = {0001-0782},
  number       = {5},
  pages        = {370--386},
  url          = {https://dl.acm.org/doi/10.1145/5689.5920},
  urldate      = {2025-09-03},
  volume       = {29},
  abstract     = {The formalization of legislation and the development of computer systems to assist with legal problem solving provide a rich domain for developing and testing artificial-intelligence technology.},
  file         = {Full Text PDF:https\://dl.acm.org/doi/pdf/10.1145/5689.5920:application/pdf},
}

@Article{Coady1987,
  author       = {Coady, W. F.},
  date         = {1987-02-01},
  journaltitle = {Security Management},
  title        = {Investigating with APES (Augmented Prolog Expert System)},
  number       = {2},
  pages        = {67-70},
  volume       = {31},
}

@Article{SHPP1980,
  author       = {{The Staff of the Heuristic Programming Project}},
  date         = {1980},
  journaltitle = {Artificial Intelligence},
  title        = {The Stanford Heuristic Programming Project: Goals and Activities},
  number       = {1},
  pages        = {25},
  volume       = {1},
}

 
@Article{Durkin1996,
  author       = {Durkin, J.},
  date         = {1996-04},
  journaltitle = {{IEEE} Expert},
  title        = {Expert systems: a view of the field},
  doi          = {10.1109/64.491282},
  issn         = {2374-9407},
  number       = {2},
  pages        = {56--63},
  url          = {https://ieeexplore.ieee.org/document/491282},
  urldate      = {2025-09-03},
  volume       = {11},
  abstract     = {Reports of the decline of expert systems are greatly exaggerated. Survey results indicate impressive growth, as researchers develop systems to tackle difficult but commercially rewarding problems.},
  keywords     = {Expert systems, Humans, Diagnostic expert systems, Meeting planning, Predictive models, Personal communication networks, Conference proceedings, Manufacturing, Control design, Monitoring},
  shorttitle   = {Expert systems},
}

@Book{Haugeland1985,
  author    = {Haugeland, John},
  date      = {1985},
  title     = {Artificial Intelligence: The Very Idea},
  location  = {Cambridge, Mass. [u.a.]},
  publisher = {MIT Press},
  series    = {Bradford books},
  subtitle  = {The very idea},
  ppn_gvk   = {190992905},
}

@Book{Hobbes1651,
  author = {Hobbes, Thomas},
  date   = {1651},
  title  = {Leviathan},
}

@Book{Couturat1901,
  author    = {Couturat, Louis},
  date      = {1901},
  title     = {La Logique de Leibniz},
  editor    = {Felix Alcan},
  publisher = {Gallica},
}

@Misc{Leibniz1677,
  author = {Gottfried Wilhelm Leibniz},
  date   = {1677},
  title  = {Calculus ratiocinato},
}

@Comment{jabref-meta: databaseType:biblatex;}
